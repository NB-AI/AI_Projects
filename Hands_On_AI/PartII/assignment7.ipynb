{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(0,120,170)\">Hands-on AI II</h1>\n",
    "<h2 style=\"color:rgb(0,120,170)\">Unit 7 -- Introduction to Natural Language Processing II </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 0\n",
    "\n",
    "- Import the same modules as discussed in the lecture notebook.\n",
    "- Check if your model versions are correct.\n",
    "- Use your GPU if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import u7_utils as u7\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed Python version: 3.7 (✓)\n",
      "Installed numpy version: 1.18.1 (✓)\n",
      "Installed matplotlib version: 3.1.3 (✓)\n",
      "Installed PyTorch version: 1.5.0 (✓)\n"
     ]
    }
   ],
   "source": [
    "u7.check_module_versions()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(208,90,80)\">ABOUT THIS NOTEBOOK</h1>\n",
    "<span style=\"color:rgb(208,90,80)\">In this notebook you should solve a small task on your one. <br><br> The goal is to train an LSTM network with a different number of hidden cells on the Penn Treebank dataset. You should decide on the validation dataset which model works best and then try it on the test dataset. This is a first example of a hyperparameter search. <br> We only evaluate how you build this hyperparameter search.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">Defining hyper-parameters</h3>\n",
    "In contrast to the lecture notebook we do not set the parameter <i> nhid </i>. This is the hyperparameter which we will later use for the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'resources/penn/'\n",
    "emsize = 200 # size of word embeddings\n",
    "lr = 20 # initial learning rate\n",
    "clipping = 0.25 # gradient clipping\n",
    "epochs = 3 # upper epoch limit\n",
    "train_batch_size = 10 # batch size for training\n",
    "eval_batch_size = 5 # batch size for elidation/test\n",
    "max_seq_len = 35 # sequence length\n",
    "seed = 1111 # random seed to facilitate reproducability\n",
    "print_interval = 1000 # report interval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc7c39a56b0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">Data & dictionary</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens in dictionary 10001\n",
      "Train data: number of tokens 929589\n",
      "Validation data: number of tokens 73760\n",
      "Test data: number of tokens 82430\n"
     ]
    }
   ],
   "source": [
    "train_corpus = u7.Corpus(os.path.join(data_path, 'train.txt'))\n",
    "valid_corpus = u7.Corpus(os.path.join(data_path, 'valid.txt'))\n",
    "test_corpus = u7.Corpus(os.path.join(data_path, 'test.txt'))\n",
    "\n",
    "dictionary = u7.Dictionary()\n",
    "train_corpus.fill_dictionary(dictionary)\n",
    "ntokens = len(dictionary)\n",
    "print (f'Number of tokens in dictionary {ntokens}')\n",
    "\n",
    "train_data = train_corpus.words_to_ids(dictionary)\n",
    "print (f'Train data: number of tokens {len(train_data)}')\n",
    "\n",
    "valid_data = valid_corpus.words_to_ids(dictionary)\n",
    "print (f'Validation data: number of tokens {len(valid_data)}')\n",
    "\n",
    "test_data = test_corpus.words_to_ids(dictionary)\n",
    "print (f'Test data: number of tokens {len(test_data)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batchified data shape: torch.Size([92958, 10])\n",
      "Validation batchified data shape: torch.Size([14752, 5])\n",
      "Test batchified data shape: torch.Size([16486, 5])\n"
     ]
    }
   ],
   "source": [
    "train_data_batches = u7.batchify(train_data, train_batch_size, device)\n",
    "print (f'Train batchified data shape: {train_data_batches.shape}')\n",
    "\n",
    "val_data_batches = u7.batchify(valid_data, eval_batch_size, device)\n",
    "print (f'Validation batchified data shape: {val_data_batches.shape}')\n",
    "\n",
    "test_data_batches = u7.batchify(test_data, eval_batch_size, device)\n",
    "print (f'Test batchified data shape: {test_data_batches.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">Training</h3>\n",
    "Nothing to do here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: torch.nn.Module, dictionary: u7.Dictionary,\n",
    "          max_seq_len: int, train_batch_size: int, \n",
    "          train_data_batches, optimizer: torch.optim.Optimizer,\n",
    "          criterion: torch.nn, clipping: int, learning_rate: int,\n",
    "          print_interval: int, epoch: int):\n",
    "    \"\"\"\n",
    "    Function to train the model. \n",
    "    :return:\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    ntokens = len(dictionary)\n",
    "    start_hidden = model.init_hidden(train_batch_size)\n",
    "    for batch, i in enumerate(range(0, train_data_batches.size(0) - 1, max_seq_len)):\n",
    "        data, targets = u7.get_batch(train_data_batches, i, max_seq_len)\n",
    "\n",
    "        # forward pass\n",
    "        model.zero_grad()\n",
    "        start_hidden = u7.repackage_hidden(start_hidden)\n",
    "        output, last_hidden = model(data, start_hidden)\n",
    "\n",
    "        # loss computation & backward pass\n",
    "        output = output.view(-1, ntokens)\n",
    "        loss = criterion(output, targets.view(-1))\n",
    "        loss.backward()\n",
    "\n",
    "        start_hidden = last_hidden\n",
    "        # clipping gradient\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clipping)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch % print_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / print_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f'| epoch {epoch :3d} | {batch :5d}/{int(len(train_data_batches)/max_seq_len) :5d} batches' \n",
    "                  f'| lr {learning_rate :02.2f} | ms/batch {elapsed * 1000 / print_interval :5.2f} |'\n",
    "                  f'loss {cur_loss :5.2f} | perplexity {math.exp(cur_loss) :8.2f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LM_LSTMModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhid):\n",
    "        super(LM_LSTMModel, self).__init__()\n",
    "        self.ntoken = ntoken\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.rnn = nn.LSTM(ninp, nhid)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "        self.nhid = nhid\n",
    "        \n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters())\n",
    "        return (weight.new_zeros(1, bsz, self.nhid),\n",
    "                weight.new_zeros(1, bsz, self.nhid))\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        emb = self.encoder(input)\n",
    "        hiddens, last_hidden = self.rnn(emb, hidden)\n",
    "        \n",
    "        decoded = self.decoder(hiddens)\n",
    "        return F.log_softmax(decoded, dim=-1), last_hidden\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "\n",
    "- Train the model for three epochs and validate after each epoch. Repeat this procedure with different number of LSTM cells (the <i> nhid </i> parameter in the lecture notebook). Save the best models for the different runs.\n",
    "- What is the best model? You can use the suggested parameter values but you can try different values too if wanted. Please note that for larger number of LSTM cells the training might be pretty time-consuming.\n",
    "- Load the best model and evaluate it on the test dataset.\n",
    "- NOTA BENE: use the Adam optimizer to get better performance <code> optimizer = optim.Adam(model.parameters(), lr=1e-2, weight_decay=1e-5)</code>, instead of SGD as done in the lecture (you can check for it in earlier notebooks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nhid= [8, 16, 32, 64, 128]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################################################################################################\n",
      "Number of LSTM cells:  8\n",
      "\n",
      "\n",
      "0. epoch:\n",
      "| epoch   0 |  1000/ 2655 batches| lr 20.00 | ms/batch 114.48 |loss  6.36 | perplexity   575.52\n",
      "| epoch   0 |  2000/ 2655 batches| lr 20.00 | ms/batch 102.30 |loss  5.94 | perplexity   378.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   0 | time: 283.80s| valid loss  5.85 | valid perplexity   345.87\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nina/anaconda3/lib/python3.7/site-packages/torch/serialization.py:402: UserWarning: Couldn't retrieve source code for container of type LM_LSTMModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved on path model8.pt\n",
      "\n",
      "\n",
      "1. epoch:\n",
      "| epoch   1 |  1000/ 2655 batches| lr 20.00 | ms/batch 95.00 |loss  5.78 | perplexity   325.27\n",
      "| epoch   1 |  2000/ 2655 batches| lr 20.00 | ms/batch 100.03 |loss  5.73 | perplexity   308.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 265.62s| valid loss  5.73 | valid perplexity   308.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "saved on path model8.pt\n",
      "\n",
      "\n",
      "2. epoch:\n",
      "| epoch   2 |  1000/ 2655 batches| lr 20.00 | ms/batch 96.36 |loss  5.69 | perplexity   296.49\n",
      "| epoch   2 |  2000/ 2655 batches| lr 20.00 | ms/batch 98.99 |loss  5.67 | perplexity   289.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 265.67s| valid loss  5.68 | valid perplexity   294.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "saved on path model8.pt\n",
      "#################################################################################################\n",
      "Number of LSTM cells:  16\n",
      "\n",
      "\n",
      "0. epoch:\n",
      "| epoch   0 |  1000/ 2655 batches| lr 20.00 | ms/batch 93.73 |loss  6.09 | perplexity   442.12\n",
      "| epoch   0 |  2000/ 2655 batches| lr 20.00 | ms/batch 97.65 |loss  5.72 | perplexity   303.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   0 | time: 261.17s| valid loss  5.66 | valid perplexity   286.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "saved on path model16.pt\n",
      "\n",
      "\n",
      "1. epoch:\n",
      "| epoch   1 |  1000/ 2655 batches| lr 20.00 | ms/batch 99.64 |loss  5.57 | perplexity   263.47\n",
      "| epoch   1 |  2000/ 2655 batches| lr 20.00 | ms/batch 99.63 |loss  5.52 | perplexity   249.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 270.17s| valid loss  5.55 | valid perplexity   256.46\n",
      "-----------------------------------------------------------------------------------------\n",
      "saved on path model16.pt\n",
      "\n",
      "\n",
      "2. epoch:\n",
      "| epoch   2 |  1000/ 2655 batches| lr 20.00 | ms/batch 98.11 |loss  5.48 | perplexity   238.65\n",
      "| epoch   2 |  2000/ 2655 batches| lr 20.00 | ms/batch 93.17 |loss  5.45 | perplexity   232.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 260.68s| valid loss  5.50 | valid perplexity   244.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "saved on path model16.pt\n",
      "#################################################################################################\n",
      "Number of LSTM cells:  32\n",
      "\n",
      "\n",
      "0. epoch:\n",
      "| epoch   0 |  1000/ 2655 batches| lr 20.00 | ms/batch 100.58 |loss  5.87 | perplexity   354.53\n",
      "| epoch   0 |  2000/ 2655 batches| lr 20.00 | ms/batch 109.75 |loss  5.47 | perplexity   237.37\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   0 | time: 284.73s| valid loss  5.45 | valid perplexity   233.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "saved on path model32.pt\n",
      "\n",
      "\n",
      "1. epoch:\n",
      "| epoch   1 |  1000/ 2655 batches| lr 20.00 | ms/batch 103.18 |loss  5.34 | perplexity   209.04\n",
      "| epoch   1 |  2000/ 2655 batches| lr 20.00 | ms/batch 101.78 |loss  5.30 | perplexity   200.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 275.88s| valid loss  5.36 | valid perplexity   211.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "saved on path model32.pt\n",
      "\n",
      "\n",
      "2. epoch:\n",
      "| epoch   2 |  1000/ 2655 batches| lr 20.00 | ms/batch 117.75 |loss  5.25 | perplexity   190.73\n",
      "| epoch   2 |  2000/ 2655 batches| lr 20.00 | ms/batch 119.90 |loss  5.23 | perplexity   187.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 320.14s| valid loss  5.30 | valid perplexity   201.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "saved on path model32.pt\n",
      "#################################################################################################\n",
      "Number of LSTM cells:  64\n",
      "\n",
      "\n",
      "0. epoch:\n",
      "| epoch   0 |  1000/ 2655 batches| lr 20.00 | ms/batch 122.48 |loss  5.75 | perplexity   314.28\n",
      "| epoch   0 |  2000/ 2655 batches| lr 20.00 | ms/batch 136.76 |loss  5.34 | perplexity   208.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   0 | time: 351.30s| valid loss  5.32 | valid perplexity   204.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "saved on path model64.pt\n",
      "\n",
      "\n",
      "1. epoch:\n",
      "| epoch   1 |  1000/ 2655 batches| lr 20.00 | ms/batch 122.89 |loss  5.20 | perplexity   180.87\n",
      "| epoch   1 |  2000/ 2655 batches| lr 20.00 | ms/batch 119.66 |loss  5.16 | perplexity   173.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 325.40s| valid loss  5.23 | valid perplexity   186.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "saved on path model64.pt\n",
      "\n",
      "\n",
      "2. epoch:\n",
      "| epoch   2 |  1000/ 2655 batches| lr 20.00 | ms/batch 122.75 |loss  5.10 | perplexity   163.82\n",
      "| epoch   2 |  2000/ 2655 batches| lr 20.00 | ms/batch 119.47 |loss  5.09 | perplexity   162.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 325.17s| valid loss  5.17 | valid perplexity   176.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "saved on path model64.pt\n",
      "#################################################################################################\n",
      "Number of LSTM cells:  128\n",
      "\n",
      "\n",
      "0. epoch:\n",
      "| epoch   0 |  1000/ 2655 batches| lr 20.00 | ms/batch 170.40 |loss  5.68 | perplexity   292.48\n",
      "| epoch   0 |  2000/ 2655 batches| lr 20.00 | ms/batch 176.48 |loss  5.27 | perplexity   194.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   0 | time: 468.98s| valid loss  5.22 | valid perplexity   185.54\n",
      "-----------------------------------------------------------------------------------------\n",
      "saved on path model128.pt\n",
      "\n",
      "\n",
      "1. epoch:\n",
      "| epoch   1 |  1000/ 2655 batches| lr 20.00 | ms/batch 174.95 |loss  5.10 | perplexity   163.77\n",
      "| epoch   1 |  2000/ 2655 batches| lr 20.00 | ms/batch 179.57 |loss  5.06 | perplexity   156.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 472.19s| valid loss  5.10 | valid perplexity   163.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "saved on path model128.pt\n",
      "\n",
      "\n",
      "2. epoch:\n",
      "| epoch   2 |  1000/ 2655 batches| lr 20.00 | ms/batch 160.92 |loss  4.98 | perplexity   145.80\n",
      "| epoch   2 |  2000/ 2655 batches| lr 20.00 | ms/batch 168.68 |loss  4.97 | perplexity   144.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 452.87s| valid loss  5.05 | valid perplexity   155.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "saved on path model128.pt\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = None\n",
    "criterion = nn.NLLLoss()\n",
    "list_path = []\n",
    "for cells in nhid:\n",
    "    # This part is need to get the best model of each number of LSTM cells:\n",
    "    save_path = f'model{cells}.pt'\n",
    "    list_path.append(save_path)\n",
    "    c = None\n",
    "\n",
    "    print('#################################################################################################')\n",
    "    print('Number of LSTM cells: ', cells)\n",
    "    model = LM_LSTMModel(ntokens, emsize, cells).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, weight_decay=1e-5)\n",
    "    # your code goes here\n",
    "    for epoch in range(3):\n",
    "        print('\\n')\n",
    "        print(f'{epoch}. epoch:')\n",
    "        epoch_start_time = time.time()\n",
    "        train(model, dictionary, max_seq_len, train_batch_size, train_data_batches, optimizer, criterion, clipping, lr, print_interval, epoch)\n",
    "                \n",
    "        \n",
    "        val_loss = u7.evaluate(model, dictionary, max_seq_len, eval_batch_size, val_data_batches, criterion) # validation loss\n",
    "        print('-' * 89)\n",
    "        print(f'| end of epoch {epoch :3d} | time: {time.time() - epoch_start_time :5.2f}s' \n",
    "              f'| valid loss {val_loss :5.2f} | valid perplexity {math.exp(val_loss):8.2f}')\n",
    "        print('-' * 89)\n",
    "\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "        if c == None or val_loss < c:\n",
    "            with open(save_path, 'wb') as f:\n",
    "                torch.save(model, f)\n",
    "            c = f'best_val_loss{cells}'\n",
    "            c = val_loss # save model when model is improving\n",
    "            print('saved on path', save_path)\n",
    "        else:\n",
    "            # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "            lr /= 4.0\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best model: Model with 128 cells and 2 epochs running trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best saved model.\n",
    "with open('model128.pt', 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss  5.00 | test perplexity 149.05\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "test_loss = u7.evaluate(model, dictionary, max_seq_len, \n",
    "                           eval_batch_size, test_data_batches, criterion)\n",
    "print('=' * 89)\n",
    "#print('| End of training | test loss {:5.2f} | test perplexity {:8.2f}'.format(\n",
    "#    test_loss, math.exp(test_loss)))\n",
    "print(f'| End of training | test loss {test_loss :5.2f} | test perplexity {math.exp(test_loss) :5.2f}')\n",
    "print('=' * 89)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "- Count the parameters of the best model. How many parameters does it have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model has 7 parameters.\n"
     ]
    }
   ],
   "source": [
    "# your code goes here\n",
    "count_para = 0\n",
    "for i in model.parameters():\n",
    "    count_para += 1\n",
    "    \n",
    "print(f'The best model has {count_para} parameters.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
