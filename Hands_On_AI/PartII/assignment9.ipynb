{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(0,120,170)\">Hands-on AI II</h1>\n",
    "<h2 style=\"color:rgb(0,120,170)\">Unit 9 (Assignment) -- Introduction to Reinforcement Learning -- Part II </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Exercise 0</h2>\n",
    "\n",
    "- Import the same modules as discussed in the lecture notebook.\n",
    "- Check if your model versions are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your imports go here\n",
    "import u9_utils as u9\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from typing import Any, Dict, Tuple\n",
    "from gym.envs.toy_text import TaxiEnv\n",
    "\n",
    "# Set Seaborn plotting style.\n",
    "sns.set()\n",
    "\n",
    "import gym\n",
    "\n",
    "from gym.envs.toy_text import FrozenLakeEnv \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed Python version: 3.7 (✓)\n",
      "Installed matplotlib version: 3.1.3 (✓)\n",
      "Installed Pandas version: 1.0.3 (✓)\n",
      "Installed Seaborn version: 0.10.1 (✓)\n",
      "Installed OpenAI Gym version: 0.17.2 (✓)\n"
     ]
    }
   ],
   "source": [
    "# your modul version check goes here\n",
    "u9.check_module_versions()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All exercises in this assignment are referring to the <i>FrozenLake-v0</i> environment of <a href=\"https://gym.openai.com\"><i>OpenAI Gym</i></a>. This environment is descibed according to its official <a href=\"https://gym.openai.com/envs/FrozenLake-v0/\">OpenAI Gym website</a> as follows:<br>\n",
    "<cite>Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend.</cite>\n",
    "\n",
    "\n",
    "There are <i>four</i> types of surfaces described in this environment:\n",
    "<ul>\n",
    "    <li><code>S</code> $\\rightarrow$ starting point (<span style=\"color:rgb(0,255,0)\"><i>safe</i></span>)</li>\n",
    "    <li><code>F</code> $\\rightarrow$ frozen surface (<span style=\"color:rgb(0,255,0)\"><i>safe</i></span>)</li>\n",
    "    <li><code>H</code> $\\rightarrow$ hole (<span style=\"color:rgb(255,0,0)\"><i>fall to your doom</i></span>)</li>\n",
    "    <li><code>G</code> $\\rightarrow$ goal (<span style=\"color:rgb(255,0,255)\"><i>frisbee location</i></span>)</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "If not already done, more information on how to <i>install</i> and <i>import</i> the <code>gym</code> module is available in the lecture's notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(0,120,170)\">States and actions</h3>\n",
    "Experiment with the <i>FrozenLake-v0</i> environment as discussed during the lecture and explained in the accompanying notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "lake_environment = FrozenLakeEnv()\n",
    "u9.set_seed(environment=lake_environment, seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "Current state ID: 0\n"
     ]
    }
   ],
   "source": [
    "lake_environment.render(mode=r'human')\n",
    "current_state_id = lake_environment.s\n",
    "print(f'\\nCurrent state ID: {current_state_id}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current position of the <i>disc retrieving</i> entity is displayed as a filled <span style=\"color:rgb(255,0,0)\"><i>red</i></span> rectangle.\n",
    "\n",
    "As we want to tackle this problem using our renowned <i>random search</i> approach, we have to analyse its applicability beforehand. Hence, the number of possible <i>actions</i> and <i>states</i> is of utter importance, as we don't want to get lost in the depth of combinatorial explosion.\n",
    "<ul>\n",
    "    <li>Query the amount of <i>actions</i> using the appropriate peoperty of the lake environment.</li>\n",
    "    <li>Query the amount of <i>states</i> using the appropriate property of the lake environment.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The FrozenLake-v0 environment comprises <4> actions and <16> states.\n"
     ]
    }
   ],
   "source": [
    "num_actions = lake_environment.action_space.n\n",
    "num_states = lake_environment.observation_space.n\n",
    "print(f'The FrozenLake-v0 environment comprises <{num_actions}> actions and <{num_states}> states.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Exercise 1</h2>\n",
    "\n",
    "- Create a q_table for the frozen lake environment.\n",
    "- Apply $Q$-learning as it was done in the lecture to solve the environment.\n",
    "- Test the learned policy and animate one (or more) exemplary episode.\n",
    "- What do you observe? Does the agent learn anything useful? Discuss if something strange happens. Hint: print the q_table during training to better understand what is going on during learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# q_table\n",
    "q_table = np.zeros([lake_environment.observation_space.n, lake_environment.action_space.n])\n",
    "q_table\n",
    "# lines: states\n",
    "# columns: actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 4)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of q_table\n",
    "q_table.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem: ordered action is not always done action. --> Next state can't be expected that easily. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_q_learning(environment: lake_environment, alpha: float = 0.1):\n",
    "    \"\"\"\n",
    "    Solve lake_environment by applying Q learning.\n",
    "    \"\"\"\n",
    "    for i in range(1, 10001):\n",
    "        # your code goes here\n",
    "        state = environment.reset()\n",
    "        done = False\n",
    "    \n",
    "        while not done:\n",
    "            action = np.argmax(q_table[state]) \n",
    "            next_state, reward, done, info = environment.step(action) \n",
    "            old_value = q_table[state, action] # old q value\n",
    "            next_max = np.max(q_table[next_state])\n",
    "            new_value = (1 - alpha) * old_value + alpha * (reward + next_max)\n",
    "            q_table[state, action] = new_value # update Q table\n",
    "\n",
    "            state = next_state\n",
    "        \n",
    "        # only for printing:\n",
    "        if i % 100 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"Episode: {i}\")\n",
    "            print(q_table)\n",
    "\n",
    "\n",
    "        # until here\n",
    "    print(\"Training finished.\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10000\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "Training finished.\n",
      "\n",
      "CPU times: user 9.59 s, sys: 1.12 s, total: 10.7 s\n",
      "Wall time: 8.88 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from IPython.display import clear_output\n",
    "# train your agent\n",
    "alpha = 0.1\n",
    "apply_q_learning(lake_environment, alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after 100 episodes:\n",
      "Average timesteps per episode: 17.19\n",
      "Average dives per episode: 1.0\n"
     ]
    }
   ],
   "source": [
    "total_epochs, total_dives = 0, 0\n",
    "episodes = 100\n",
    "\n",
    "captured_frames = [[] for _ in range(episodes)]\n",
    "\n",
    "for episode in range(episodes):\n",
    "    # test your method\n",
    "    state = lake_environment.reset()\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])\n",
    "        state, reward, done, info = lake_environment.step(action)\n",
    "\n",
    "        if done and (reward == 0.0):\n",
    "            penalties += 1 # when false drop off\n",
    "\n",
    "        epochs += 1\n",
    "\n",
    "        captured_frames[episode].append({\n",
    "            r'frame': lake_environment.render(mode=r'ansi'),\n",
    "            r'state': state,\n",
    "            r'action': action,\n",
    "            r'reward': reward\n",
    "        })\n",
    "\n",
    "    total_dives += penalties\n",
    "    total_epochs += epochs\n",
    "\n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
    "print(f\"Average dives per episode: {total_dives / episodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "\u001b[41mH\u001b[0mFFG\n",
      "\n",
      "Step No.: 5\n",
      "State ID: 12\n",
      "Action ID: 0\n",
      "Reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "u9.animate_environment_search(frames=captured_frames[12], verbose=True, delay=0.4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "\u001b[41mH\u001b[0mFFG\n",
      "\n",
      "Step No.: 29\n",
      "State ID: 12\n",
      "Action ID: 0\n",
      "Reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "u9.animate_environment_search(frames=captured_frames[8], verbose=True, delay=0.4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "Step No.: 295\n",
      "State ID: 5\n",
      "Action ID: 0\n",
      "Reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "u9.animate_environment_search(frames=captured_frames[50], verbose=True, delay=0.4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe? Does the agent learn anything useful? Discuss if something strange happens. Hint: print the q_table during training to better understand what is going on during learning. <br>\n",
    "The q_table stays as it is only filled up with 0s. It follows that the model doesn't seem to learn anything. \n",
    "The problem is that the agent always acts with the same probabilities without changing them. It learns nothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Exercise 2</h2>\n",
    "Very likely your training in Exercise 1 was not successful. Try to add exploration to your algorithm (you might have to write a new function):\n",
    "<li><code>I</code> $\\rightarrow$ Throw a random uniform number between 0 and 1. \n",
    "<li><code>II</code> $\\rightarrow$ If the number is smaller than 0.1, sample a random action.\n",
    "<li><code>III</code> $\\rightarrow$ Choose your action as usual.   \n",
    "    \n",
    "- Apply the modified $Q$-learning again to solve the environment.\n",
    "- Test the learned policy and animate one (or more) exemplary episode.\n",
    "- What do you observe? Does the agent learn now?."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# q_table\n",
    "del q_table\n",
    "q_table = np.zeros([lake_environment.observation_space.n, lake_environment.action_space.n])\n",
    "q_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def apply_q_learning_exploration(environment: lake_environment, alpha: float = 0.1):\n",
    "    \"\"\"\n",
    "    Solve lake_environment by applying Q learning and exploration.\n",
    "    \"\"\"\n",
    "    for i in range(1, 10001):\n",
    "        # your code goes here\n",
    "\n",
    "        \n",
    "        state = environment.reset()\n",
    "        done = False\n",
    "    \n",
    "        while not done:\n",
    "            \n",
    "            # I: get a random number between 0 and 1\n",
    "            rand = random.randint(0, 10000)\n",
    "            number = rand/10000\n",
    "            \n",
    "            if number < 0.1:# II: make a random action when number smaller than 0.1\n",
    "                action = environment.action_space.sample()\n",
    "                \n",
    "            else:# III: choose action as usual\n",
    "                action = np.argmax(q_table[state]) \n",
    "                \n",
    "            next_state, reward, done, info = environment.step(action) \n",
    "            old_value = q_table[state][action] # old q value\n",
    "            next_max = np.max(q_table[next_state])\n",
    "            new_value = (1 - alpha) * old_value + alpha * (reward + next_max)\n",
    "\n",
    "            q_table[state, action] = new_value # update Q table\n",
    "\n",
    "            state = next_state\n",
    "            \n",
    "        \n",
    "        # only for printing:\n",
    "        if i % 100 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"Episode: {i}\")\n",
    "            print(q_table)  \n",
    "   \n",
    "    print(\"Training finished.\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10000\n",
      "[[0.87874789 0.87759927 0.87607821 0.88044035]\n",
      " [0.50881911 0.58336085 0.62660669 0.88044035]\n",
      " [0.84435986 0.70403614 0.79603599 0.88044035]\n",
      " [0.54321723 0.69570303 0.60090239 0.88044035]\n",
      " [0.87196583 0.62799478 0.60722176 0.63168139]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.5922006  0.33148232 0.42363847 0.11253993]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.67027754 0.55595798 0.61912579 0.85930305]\n",
      " [0.56046815 0.84475158 0.6079083  0.52366454]\n",
      " [0.78855707 0.64004165 0.57295127 0.53995953]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.56787153 0.80096881 0.89906889 0.43769242]\n",
      " [0.86196977 0.94129664 0.91389607 0.90193405]\n",
      " [0.         0.         0.         0.        ]]\n",
      "Training finished.\n",
      "\n",
      "CPU times: user 36.9 s, sys: 6.72 s, total: 43.7 s\n",
      "Wall time: 32.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from IPython.display import clear_output\n",
    "# train your agent\n",
    "alpha = 0.1\n",
    "apply_q_learning_exploration(lake_environment, alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results after 100 episodes:\n",
      "Average timesteps per episode: 91.15\n",
      "Average dives per episode: 0.13\n"
     ]
    }
   ],
   "source": [
    "total_epochs, total_dives = 0, 0\n",
    "episodes = 100\n",
    "\n",
    "captured_frames = [[] for _ in range(episodes)]\n",
    "\n",
    "for episode in range(episodes):\n",
    "    # test your method\n",
    "    state = lake_environment.reset()\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])\n",
    "        state, reward, done, info = lake_environment.step(action)\n",
    "\n",
    "        if done and (reward == 0.0):\n",
    "            penalties += 1 # when false drop off\n",
    "\n",
    "        epochs += 1\n",
    "\n",
    "        # Save rendering of current state.\n",
    "        captured_frames[episode].append({\n",
    "            r'frame': lake_environment.render(mode=r'ansi'),\n",
    "            r'state': state,\n",
    "            r'action': action,\n",
    "            r'reward': reward\n",
    "        })\n",
    "\n",
    "    total_dives += penalties\n",
    "    total_epochs += epochs\n",
    "    \n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
    "print(f\"Average dives per episode: {total_dives / episodes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "\n",
      "Step No.: 32\n",
      "State ID: 15\n",
      "Action ID: 1\n",
      "Reward: 1.0\n"
     ]
    }
   ],
   "source": [
    "u9.animate_environment_search(frames=captured_frames[4], verbose=True, delay=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "Step No.: 20\n",
      "State ID: 5\n",
      "Action ID: 0\n",
      "Reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "u9.animate_environment_search(frames=captured_frames[40], verbose=True, delay=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "\n",
      "Step No.: 126\n",
      "State ID: 15\n",
      "Action ID: 1\n",
      "Reward: 1.0\n"
     ]
    }
   ],
   "source": [
    "u9.animate_environment_search(frames=captured_frames[8], verbose=True, delay=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe? Does the agent learn now?<br>\n",
    "When you have a look at the q_table now you can discover an interesting fact. On all states which are holes or the goal the probability for all actions there is 0. That's because the game is over then. However, there is no probability of 0 for the remaining fields. <br>\n",
    "The average of dives per episode is only 13 % now (not 100 % like before). Because of slippery ice it can happen that you fall in the water although you have acted in a right way. Therefore, the average dive probability is even smaller when the ice wouldn't be slippery.<br>\n",
    "Furthermore, the agent learnt to connect special states with individual actions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
