{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Collaborative Filtering & Evaluation\n",
    "\n",
    "*Learning from User-generated Data, CP MMS, JKU Linz 2021*\n",
    "\n",
    "This exercise is comprised of two parts: **Collaborative filtering** and **Recommender Systems Evaluation I**.\\\n",
    "It concerns material of Lecture 2 \"**Memory-based Collaborative Filtering (CF)**\" and partially Lecture 3 \"**Evaluating Recommender Systems**\".\n",
    "\n",
    "The deadline for submitting the whole Exercise 1 is **30.03.2021**.\n",
    "\n",
    "## Requirements\n",
    "Below you will find four tasks. Each task is graded on a scale from 0 to 3 points. 12 points is the maximal score possible for this exercise.\n",
    "\n",
    "Solving each task will require you to edit this Jupyter Notebook. It is to be uploaded to **Moodle** before the deadline. Make sure to enter your name and matriculation number into the file name!\\\n",
    "Example file name: \"**Ex-1_Bond-James_k0000007.ipynb**\"\n",
    "\n",
    "\n",
    "If a task requires you to write a function, follow the template provided. Do not change the template and make sure that your function acts in accordance with the specifications.\n",
    "\n",
    "It should be possible to run all cells of your submitted notebook from the first to the last without errors in a reasonable time (under 2 minutes). In the opposite case you may loose points or even receive zero for the whole exercise, so **be careful with what you submit**.\n",
    "\n",
    "For this exercise only add new cells and import additional libraries if you really-really need to (you most probably won't need to). **Feel free to experiment with the notebook, but clean it up before submitting.**\n",
    "\n",
    "---\n",
    "\n",
    "## Recap: Collaborative Filtering (CF)\n",
    "### Interaction Matrix\n",
    "* Every element is a result of interaction between a certain user and an item. In this case - explicit rating on a scale from 1 to 5 (stars ⭐️);\n",
    "* In a real world sceario such matrix can be recreated from log files of your database (attached to an online shop, for example), where each interaction is represented as a single record in a very long list. This is quite efficient, because usually interaction matrices are very sparce. Why is it so?\n",
    "* How do we treat missing values? Should we replace them with zeros?\n",
    "* What interactions can we consider while creating an interaction matrix?\n",
    "\n",
    "\n",
    "|        | item_1 | item_2 | item_3 | item_4 | item_5 |\n",
    "| ---    |   ---  |   ---  |   ---  |   ---  |   ---  |\n",
    "| user_1 | 3 |   | 2 | 3 | 3 |\n",
    "| user_2 | 4 | 3 | 4 | 3 |   |\n",
    "| user_3 | 3 | 2 | 1 | 4 | 4 |\n",
    "| user_4 |   | 5 | 4 | 3 | 1 |\n",
    "| user_5 | 5 |   | 3 | 4 | **(?)** |\n",
    "\n",
    "### Recommendation scenario\n",
    "* We consider basic scenario of recommendation -- predicting missing rating for an item given a user;\n",
    "* This scenario can be extended. For example if we predict ratings for all items unseen by the user, we'll be able to propose top 10 items to purchase/listen to/watch next (Amazon, Spotify, YouTube);\n",
    "* The predicted rating is usually hidden from the user and only used internally by the recommender system;\n",
    "\n",
    "\n",
    "* Input: **User_id**, **Item_id**;\n",
    "* Output: **Rating_Estimate** (usually continuous, can be truncated to conform to our scale, but we don't bother);\n",
    "\n",
    "### User-Based CF\n",
    "* \"I haven't rated the item, I have little idea about it\";\n",
    "* \"There probably are **other users**, who interacted with the item and rated it\";\n",
    "* \"Let us find users who have preferences most similar to mine and construct the predicted rating for me from their ratigs\";\n",
    "* \"I am a kind person and tend to give high ratings to items. Other users can have different habits, we need to take this into account (rating bias)\";\n",
    "\n",
    "\n",
    "### Item-Based CF\n",
    "* \"I haven't rated the item, I have little idea about it\";\n",
    "* \"But **I rated other items**, maybe some of them are similar to the one in question?\";\n",
    "* \"Let us construct the predicted rating from the ratings I gave to other items\";\n",
    "* \"We construct the predicted rating from my own ratings, so my bias is taken into account automatically\";\n",
    "\n",
    "# Recommendation in Action\n",
    "What do we recommend? Books? Movies? Music? Games?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RHCP</th>\n",
       "      <th>Radiohead</th>\n",
       "      <th>Clown_Core</th>\n",
       "      <th>Pink_Floyd</th>\n",
       "      <th>ACDC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>user_1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Active_User</th>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             RHCP  Radiohead  Clown_Core  Pink_Floyd  ACDC\n",
       "user_1        3.0        NaN         2.0         3.0   3.0\n",
       "user_2        4.0        3.0         4.0         3.0   NaN\n",
       "user_3        3.0        2.0         1.0         4.0   4.0\n",
       "user_4        NaN        5.0         4.0         3.0   1.0\n",
       "Active_User   5.0        NaN         3.0         4.0   NaN"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Interaction matrix: ratings given to items by users\n",
    "data = {\n",
    "    'user_1' : {'item_1' : 3, 'item_2' : np.nan, 'item_3' : 2, 'item_4' : 3, 'item_5' : 3},\n",
    "    'user_2' : {'item_1' : 4, 'item_2' : 3, 'item_3' : 4, 'item_4' : 3, 'item_5' : np.nan},\n",
    "    'user_3' : {'item_1' : 3, 'item_2' : 2, 'item_3' : 1, 'item_4' : 4, 'item_5' : 4},\n",
    "    'user_4' : {'item_1' : np.nan, 'item_2' : 5, 'item_3' : 4, 'item_4' : 3, 'item_5' : 1},\n",
    "    'Active_User' : {'item_1' : 5, 'item_2' : np.nan, 'item_3' : 3, 'item_4' : 4, 'item_5' : np.nan}\n",
    "}\n",
    "\n",
    "# Name the items\n",
    "items = ['RHCP', 'Radiohead', 'Clown_Core', 'Pink_Floyd', 'ACDC']\n",
    "\n",
    "df = pd.DataFrame(data).T\n",
    "df.columns = items\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-Based CF\n",
    "Find similar users, recommend according to their respective similarity.\n",
    "\n",
    "### Which rating do we predict?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Active_User> <ACDC> previous value: nan\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RHCP</th>\n",
       "      <th>Radiohead</th>\n",
       "      <th>Clown_Core</th>\n",
       "      <th>Pink_Floyd</th>\n",
       "      <th>ACDC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>user_1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Active_User</th>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             RHCP  Radiohead  Clown_Core  Pink_Floyd  ACDC\n",
       "user_1        3.0        NaN         2.0         3.0   3.0\n",
       "user_2        4.0        3.0         4.0         3.0   NaN\n",
       "user_3        3.0        2.0         1.0         4.0   4.0\n",
       "user_4        NaN        5.0         4.0         3.0   1.0\n",
       "Active_User   5.0        NaN         3.0         4.0   NaN"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set User-Item to predict score\n",
    "target_user = 'Active_User'\n",
    "target_item = 'ACDC'\n",
    "\n",
    "print(\"<\"+target_user + \"> <\" + target_item + \"> previous value: \" + str(df.loc[target_user,target_item]))\n",
    "\n",
    "# Create a copy of the data to mess with it\n",
    "df_pred = df.copy(deep = True)\n",
    "\n",
    "# Set the score to predict to NaN for convenience (It may have beem NaN already)\n",
    "df_pred.loc[target_user,target_item] = np.nan # NaN value used so far, here we need the prediction\n",
    "df_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding top K similar users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_1         0.866025\n",
       "user_2         0.000000\n",
       "user_3         0.654654\n",
       "user_4        -1.000000\n",
       "Active_User    1.000000\n",
       "Name: Active_User, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating similarities between users\n",
    "# because of NaNs we automatically ignore rows and columns with missing ratings\n",
    "user_similarity = df_pred.T.corr(method='pearson') ## Pearson's correlation\n",
    "## T: transpose: users become columns and are so keys to work with\n",
    "## getting a table full of correlation relations within the users\n",
    "user_similarity[target_user]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_1    0.866025\n",
       "user_3    0.654654\n",
       "Name: Active_User, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selecting users that have actually rated the target item\n",
    "potential_neighbors = list(df_pred[df_pred[target_item].notnull()].index) ## ignore users which are not \n",
    "## correlated to active user\n",
    "neighbors = user_similarity.loc[target_user].filter(potential_neighbors) ## potential_neighbors as \n",
    "## boolean mask\n",
    "\n",
    "# Saving weights and Names of Top K users closest to the Target one\n",
    "top_k_users = 2\n",
    "closest_users = neighbors.sort_values(ascending=False)[:top_k_users]\n",
    "closest_users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting the Rating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item <ACDC> is recommended to the user <Active_User> with score 4.658975830340907 out of 5\n"
     ]
    }
   ],
   "source": [
    "sum_weight = closest_users.sum() # weight-normalization term\n",
    "target_rating_bias = df.loc[target_user].mean()\n",
    "\n",
    "predicted_score = target_rating_bias # start from user's own bias\n",
    "\n",
    "for user in closest_users.keys():\n",
    "    user_rating_bias = df.loc[user].mean()\n",
    "    \n",
    "    user_weight = (closest_users[user]/sum_weight)\n",
    "    user_rating_dev = df.loc[user, target_item] - user_rating_bias\n",
    "    \n",
    "    predicted_score += user_weight * user_rating_dev # take other ratings into account, apply weighting\n",
    "    ## same as on slide pdf page 20\n",
    "print(\"Item <\" + target_item +\n",
    "      \"> is recommended to the user <\" + target_user +\n",
    "      \"> with score \" + str(predicted_score) +\n",
    "      \" out of 5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>TASK 1/4</font>: User-Based CF\n",
    "Compose the above code into a function which returns a single value: predicted score. Consult lecture slides to make sure you don't miss a thing! Use the template below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.658975830340907"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def user_based_CF(df: pd.DataFrame, target_user: str, target_item: str, top_k = 2) -> float:\n",
    "    '''\n",
    "    df - Data Frame - interation matrix\n",
    "    target_user - String - user to predict rating for\n",
    "    target_item - String - item to predict rating for\n",
    "    top_k - Int - number of neighbors to predict ratings from (default 2)\n",
    "    \n",
    "    returns - float - predicted rating \n",
    "    '''\n",
    "\n",
    "    # Create a copy of the data to mess with it\n",
    "    df_pred = df.copy(deep = True) ## dataframe to work with\n",
    "\n",
    "    # Set the score to predict to NaN for convenience (It may have beem NaN already)\n",
    "    df_pred.loc[target_user,target_item] = np.nan ## NaN value used so far, here we  will need the prediction\n",
    "    ## location of prediction\n",
    "    \n",
    "    \n",
    "    # Calculating similarities between users\n",
    "    # because of NaNs we automatically ignore rows and columns with missing ratings\n",
    "    user_similarity = df_pred.T.corr(method='pearson') ## Pearson's correlation\n",
    "    ## T: transpose: users become columns and are so keys to work with\n",
    "    ## getting a table full of correlation relations within the users\n",
    "        \n",
    "    \n",
    "    # Selecting users that have actually rated the target item\n",
    "    potential_neighbors = list(df_pred[df_pred[target_item].notnull()].index) ## ignore users which are not \n",
    "    ## correlated to active user\n",
    "    neighbors = user_similarity.loc[target_user].filter(potential_neighbors) ## potential_neighbors as \n",
    "    ## boolean mask\n",
    "\n",
    "    \n",
    "    # Saving weights and Names of Top K users closest to the Target one\n",
    "    closest_users = neighbors.sort_values(ascending=False)[:top_k]\n",
    "    \n",
    "    sum_weight = closest_users.sum() # weight-normalization term\n",
    "    target_rating_bias = df.loc[target_user].mean()\n",
    "\n",
    "    predicted_score = target_rating_bias # start from user's own bias\n",
    "\n",
    "    for user in closest_users.keys():\n",
    "        user_rating_bias = df.loc[user].mean()\n",
    "\n",
    "        user_weight = (closest_users[user]/sum_weight)\n",
    "        user_rating_dev = df.loc[user, target_item] - user_rating_bias\n",
    "\n",
    "        predicted_score += user_weight * user_rating_dev # take other ratings into account, apply weighting\n",
    "        ## same as on slide pdf page 20\n",
    "    \n",
    "    \n",
    "    return predicted_score\n",
    "\n",
    "# quick check\n",
    "user_based_CF(df, target_user = target_user, target_item = target_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Item-Based CF\n",
    "### What do we predict?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Active_User> <ACDC> previous value: nan\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RHCP</th>\n",
       "      <th>Radiohead</th>\n",
       "      <th>Clown_Core</th>\n",
       "      <th>Pink_Floyd</th>\n",
       "      <th>ACDC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>user_1</th>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Active_User</th>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             RHCP  Radiohead  Clown_Core  Pink_Floyd  ACDC\n",
       "user_1        3.0        NaN         2.0         3.0   3.0\n",
       "user_2        4.0        3.0         4.0         3.0   NaN\n",
       "user_3        3.0        2.0         1.0         4.0   4.0\n",
       "user_4        NaN        5.0         4.0         3.0   1.0\n",
       "Active_User   5.0        NaN         3.0         4.0   NaN"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set User-Item to predict score\n",
    "target_user = 'Active_User'\n",
    "target_item = 'ACDC'\n",
    "\n",
    "print(\"<\"+target_user + \"> <\" + target_item + \"> previous value: \" + str(df.loc[target_user,target_item]))\n",
    "\n",
    "# Create a copy of the data to mess with it\n",
    "df_pred = df.copy(deep = True)\n",
    "\n",
    "# Set the score to predict to NaN for convenience (It may have beem NaN already)\n",
    "df_pred.loc[target_user,target_item] = np.nan\n",
    "df_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding top K Similar Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RHCP          0.770826\n",
       "Pink_Floyd    0.644237\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating Item-similarity\n",
    "\n",
    "df_adjusted = df.apply(lambda x: x - x.mean(), axis=1) ## x.mean() refers to average rating of individual user\n",
    "## axis=1 refers to the rows/users of the df\n",
    "## x describes each entry value\n",
    "## we now get a dataframe where each entry = user?_rating_item?? - user?_mean \n",
    "## --> needed for adjusted cosine similarity!!\n",
    "\n",
    "item_mask = df_adjusted.loc[target_user].notnull() # items rated by target_user\n",
    "item_mask[target_item] = True # explicitly add target item there for convenience ## Because we want to \n",
    "## make a prediction for the target item which was not rated by active user so far\n",
    "## In contrast, other items which were not rated by the active shall be deleted\n",
    "\n",
    "user_mask = df_adjusted[target_item].notnull() # users who rated target_item\n",
    "\n",
    "\n",
    "# Interaction matrix prepared for adjusted cosine similarity calculation\n",
    "df_adjusted = df_adjusted.loc[user_mask, item_mask]\n",
    "## user_mask: delete users/rows who not rated target item\n",
    "## item_mask: delete items/columns which were not rated by active user\n",
    "\n",
    "res = pd.Series(dtype=float) # array to be filled with cosine similarities between the target and other users!!\n",
    "\n",
    "for item in df_adjusted.columns: ## all column names/items (target item inclusive) which are relevant for\n",
    "    ## determining the target location\n",
    "    if not(item == target_item):\n",
    "        res[item] = cosine_similarity(df_adjusted[[item, target_item]].dropna().T)[0,1]\n",
    "        ## get cosine similarity between one relevant item and the target item\n",
    "        \n",
    "# taking two most similar items\n",
    "top_k_items = 2\n",
    "closest_items = res.sort_values(ascending=False)[:top_k_items]\n",
    "\n",
    "closest_items\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.5447291009838064"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting the rating\n",
    "# My solution:\n",
    "sum_similars = closest_items.sum() ## sum up similarity over k-nearest items\n",
    "enumerator = 0\n",
    "\n",
    "for index, single_item_simi_value in enumerate(closest_items):\n",
    "    title_cur_item = closest_items.keys()[index]\n",
    "    rating_active_user_cur_item = df_pred[title_cur_item][target_user]\n",
    "    enumerator += single_item_simi_value * rating_active_user_cur_item\n",
    "    \n",
    "final_itembased_prediction = enumerator/sum_similars\n",
    "\n",
    "final_itembased_prediction\n",
    "# ... something is missing here...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>TASK 2/4</font>: Item-Based CF\n",
    "Complete the above code and compose it into a function. Consult lecture slides to make sure you don't miss a thing! Use the following template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for top_k = 15 as given by jupyter notebook:\n",
      "6.305436935363652\n",
      "\n",
      "Prediction for top_k = 2 as given by the function:\n",
      "4.5447291009838064\n"
     ]
    }
   ],
   "source": [
    "def item_based_CF(df: pd.DataFrame, target_user: str, target_item: str, top_k = 2) -> float:\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    df - Data Frame - interation matrix\n",
    "    target_user - String - user to predict rating for\n",
    "    target_item - String - item to predict rating for\n",
    "    top_k - Int - number of neighbors to predict ratings from (default 2)\n",
    "    \n",
    "    returns - float - predicted rating \n",
    "    '''\n",
    "    # Calculating Item-similarity\n",
    "\n",
    "    df_adjusted = df.apply(lambda x: x - x.mean(), axis=1) ## x.mean() refers to average rating of individual user\n",
    "    ## axis=1 refers to the rows/users of the df\n",
    "    ## x describes each entry value\n",
    "    ## we now get a dataframe where each entry = user?_rating_item?? - user?_mean \n",
    "    ## --> needed for adjusted cosine similarity!!\n",
    "\n",
    "    item_mask = df_adjusted.loc[target_user].notnull() # items rated by target_user\n",
    "    item_mask[target_item] = True # explicitly add target item there for convenience ## Because we want to \n",
    "    ## make a prediction for the target item which was not rated by active user so far\n",
    "    ## In contrast, other items which were not rated by the active shall be deleted\n",
    "\n",
    "    user_mask = df_adjusted[target_item].notnull() # users who rated target_item\n",
    "\n",
    "\n",
    "    # Interaction matrix prepared for adjusted cosine similarity calculation\n",
    "    df_adjusted = df_adjusted.loc[user_mask, item_mask]\n",
    "    ## user_mask: delete users/rows who not rated target item\n",
    "    ## item_mask: delete items/columns which were not rated by active user\n",
    "\n",
    "    res = pd.Series(dtype=float) # array to be filled with cosine similarities between the target and other users!!\n",
    "\n",
    "    ## Compute the cosine_similarity:\n",
    "    for item in df_adjusted.columns: ## all column names/items (target item inclusive) which are relevant for\n",
    "        ## determining the target location\n",
    "        if not(item == target_item):\n",
    "            res[item] = cosine_similarity(df_adjusted[[item, target_item]].dropna().T)[0,1]\n",
    "            ## get cosine similarity between one relevant item and the target item\n",
    "\n",
    "    # taking two most similar items\n",
    "    closest_items = res.sort_values(ascending=False)[:top_k]\n",
    "\n",
    "    \n",
    "    # My added part to compute the final prediction:\n",
    "    \n",
    "    sum_similars = closest_items.sum() ## sum up similarity over k-nearest items\n",
    "    enumerator = 0\n",
    "\n",
    "    for index, single_item_simi_value in enumerate(closest_items):\n",
    "        title_cur_item = closest_items.keys()[index]\n",
    "        rating_active_user_cur_item = df_pred[title_cur_item][target_user]\n",
    "        enumerator += single_item_simi_value * rating_active_user_cur_item\n",
    "\n",
    "    return float(enumerator/sum_similars) ## the final item based prediction\n",
    "    # error was here!!! Very important to use float()because in first line by function initalization:\n",
    "    # '-> float'\n",
    "\n",
    "\n",
    "# quick check\n",
    "print(f'Prediction for top_k = 15 as given by jupyter notebook:\\n{item_based_CF(df, target_user = target_user, target_item = target_item, top_k = 15)}\\n') \n",
    "\n",
    "print(f'Prediction for top_k = 2 as given by the function:\\n{item_based_CF(df, target_user = target_user, target_item = target_item)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>TASK 3/4</font>: Closer to the Real World\n",
    "In real world situations recommender systems may face all kinds of extreme circumstances. For example there may be not enough data to make a prediction.\n",
    "Revise and improve the functions you have written for Tasks 1 and 2.\n",
    "\n",
    "In particular address the following issues:\n",
    "* If there is no suitable neighbors - return user bias as a prediction;\n",
    "* If it is impossible to calculate user bias - return average rating: **3**;\n",
    "* The functions should not corrupt input data;\n",
    "* The functions should act sensibly when given top_k argument from range [1, +inf];\n",
    "* The functions should not take into account users / items with zero or negative similarity to the target;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go back to slides and look what is missing\n",
    "\n",
    "# calculate every value in the table and then calculate the rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.658975830340907"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## For user_based prediction:\n",
    "## Something new is marked as '## NEW'\n",
    "def user_based_CF(df: pd.DataFrame, target_user: str, target_item: str, top_k = 2) -> float:\n",
    "    '''\n",
    "    df - Data Frame - interation matrix\n",
    "    target_user - String - user to predict rating for\n",
    "    target_item - String - item to predict rating for\n",
    "    top_k - Int - number of neighbors to predict ratings from (default 2)\n",
    "    \n",
    "    returns - float - predicted rating \n",
    "    '''\n",
    "    \n",
    "    ## NEW: \n",
    "    ## Target user hasn't done a rating so far --> return a 3 as prediction\n",
    "    if df.loc[target_user].isnull().all():\n",
    "        return 3\n",
    "    \n",
    "\n",
    "    # Create a copy of the data to mess with it\n",
    "    df_pred = df.copy(deep = True) \n",
    "\n",
    "    # Set the score to predict to NaN for convenience (It may have beem NaN already)\n",
    "    df_pred.loc[target_user,target_item] = np.nan\n",
    "    \n",
    "    \n",
    "    # Calculating similarities between users\n",
    "    user_similarity = df_pred.T.corr(method='pearson')\n",
    "    ## NEW comment: there has to be at least one rating of a user when he/she shall be considered in the\n",
    "    ## similarity. Therefore, the mean of user rating (user bias) also exists and can be computed.\n",
    "    \n",
    "    \n",
    "    # Selecting users that have actually rated the target item\n",
    "    potential_neighbors = list(df_pred[df_pred[target_item].notnull()].index)\n",
    "    \n",
    "    neighbors = user_similarity.loc[target_user].filter(potential_neighbors) \n",
    "    \n",
    "    ## NEW: Let's say that top_k shall be maximal 49. That should be enough to make meaningful predictions.\n",
    "    if top_k >= 50:\n",
    "        top_k = 49 ## 49 is the closest valid choice to k >= 50; good k typically in praxis: 20 < k < 50\n",
    "        \n",
    "    elif top_k < 10 and top_k < len(neighbors):\n",
    "        if len(neighbors) < 50:\n",
    "            k = len(neighbors)\n",
    "        else:\n",
    "            k = 21 ## NEW: a good k is in practice bigger than 20. Furthermore, 21 is closest valid k choice \n",
    "            ## to the inputted k\n",
    "    \n",
    "\n",
    "    closest_users = neighbors.sort_values(ascending=False)[:top_k]\n",
    "    \n",
    "    \n",
    "    ## NEW:\n",
    "    if len(closest_items) == 0:\n",
    "        return df.mean(axis=1)[target_user]\n",
    "    \n",
    "    closest_users_reduced = closest_users[closest_users > 0] ## NEW: we only consider users which have a\n",
    "    ## higher similarity value than 0\n",
    "    \n",
    "    target_rating_bias = df.loc[target_user].mean()\n",
    "    \n",
    "    ## NEW: When there exists no similar user to target user we simply return the target user's mean/bias\n",
    "    if len(closest_users_reduced) == 0:\n",
    "        return target_rating_bias \n",
    "    \n",
    "\n",
    "    predicted_score = target_rating_bias # start from user's own bias\n",
    "    sum_weight = closest_users_reduced.sum() # weight-normalization term\n",
    "\n",
    "    for user in closest_users_reduced.keys(): ## NEW: We make same procedure as before but this time with\n",
    "        ## closest users which have a similarity > 0\n",
    "        \n",
    "        \n",
    "        user_rating_bias = df.loc[user].mean()               \n",
    "        \n",
    "        user_weight = (closest_users_reduced[user]/sum_weight)\n",
    "        user_rating_dev = df.loc[user, target_item] - user_rating_bias\n",
    "\n",
    "        predicted_score += user_weight * user_rating_dev\n",
    "    \n",
    "    \n",
    "    return predicted_score\n",
    "\n",
    "# quick check\n",
    "user_based_CF(df, target_user = target_user, target_item = target_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for top_k = 15 as given by jupyter notebook:\n",
      "4.5447291009838064\n",
      "\n",
      "Prediction for top_k = 2 as given by the function:\n",
      "4.5447291009838064\n"
     ]
    }
   ],
   "source": [
    "## For item_based prediction:\n",
    "\n",
    "def item_based_CF(df: pd.DataFrame, target_user: str, target_item: str, top_k = 2) -> float:\n",
    "    '''\n",
    "    df - Data Frame - interation matrix\n",
    "    target_user - String - user to predict rating for\n",
    "    target_item - String - item to predict rating for\n",
    "    top_k - Int - number of neighbors to predict ratings from (default 2)\n",
    "    \n",
    "    returns - float - predicted rating \n",
    "    ''' \n",
    "    \n",
    "    ## NEW: Target user hasn't done a rating so far --> return a 3 as prediction\n",
    "    if df.loc[target_user].isnull().all():\n",
    "        return 3\n",
    "    \n",
    "    # Calculating Item-similarity\n",
    "    df_adjusted = df.apply(lambda x: x - x.mean(), axis=1) \n",
    "\n",
    "    item_mask = df_adjusted.loc[target_user].notnull() # items rated by target_user\n",
    "    item_mask[target_item] = True # explicitly add target item there for convenience \n",
    "\n",
    "    user_mask = df_adjusted[target_item].notnull() # users who rated target_item\n",
    "\n",
    "\n",
    "    # Interaction matrix prepared for adjusted cosine similarity calculation\n",
    "    df_adjusted = df_adjusted.loc[user_mask, item_mask]\n",
    "\n",
    "    res = pd.Series(dtype=float) \n",
    "\n",
    "    ## Compute the cosine_similarity:\n",
    "    for item in df_adjusted.columns: \n",
    "        ## determining the target location\n",
    "        if not(item == target_item):\n",
    "            try: ## NEW: cosine_smilarity only works when there is a item to compare with.\n",
    "                res[item] = cosine_similarity(df_adjusted[[item, target_item]].dropna().T)[0,1]\n",
    "                ## NEW comment: we only calculate similarities from df_adjusted, i. e. the user biases are\n",
    "                ## computeable here; furthermore, we don't need elsewhere the user bias\n",
    "            except: \n",
    "                return df.mean(axis=1)[target_user]\n",
    "\n",
    "            \n",
    "    ## NEW: Let's say that top_k shall be maximal 49. That should be enough to make meaningful predictions.\n",
    "    if top_k >= 50:\n",
    "        top_k = 49 ## 49 is the closest valid choice to k >= 50; good k typically in praxis: 20 < k < 50\n",
    "        \n",
    "    elif top_k < 10 and top_k < len(res):\n",
    "        if len(res) < 50:\n",
    "            k = len(res)\n",
    "        else:\n",
    "            k = 21 ## NEW: a good k is in practice bigger than 20. Furthermore, 21 is closest valid k choice \n",
    "            ## to the inputted k\n",
    "    ## Remark: The choice of k above is valid for user_based CF, I claim that we can also use these values\n",
    "    ## for item_based CF.\n",
    "              \n",
    "    # taking two most similar items\n",
    "    closest_items = res.sort_values(ascending=False)[:top_k] ## NEW comment: when top_k is too large, we\n",
    "    ## simply include all res-values here and will select the fitting ones in the next step\n",
    "\n",
    "    \n",
    "    ## NEW:\n",
    "    if len(closest_items) == 0:\n",
    "        return df.mean(axis=1)[target_user]\n",
    "        \n",
    "    sum_similars = 0 ## NEW\n",
    "    enumerator = 0\n",
    "    counter = 0 ## NEW\n",
    "\n",
    "    for index, single_item_simi_value in enumerate(closest_items):\n",
    "        \n",
    "        ## NEW:\n",
    "        if single_item_simi_value <= 0: ## the item doesn't fit for our prediction\n",
    "            counter += 1\n",
    "        else: # Only items which fit to the searched one shall be considered\n",
    "            sum_similars += single_item_simi_value \n",
    "            \n",
    "            title_cur_item = closest_items.keys()[index]\n",
    "            rating_active_user_cur_item = df_pred[title_cur_item][target_user]\n",
    "            enumerator += single_item_simi_value * rating_active_user_cur_item\n",
    "\n",
    "    ## New:    \n",
    "    if counter == len(closest_items): ## no neighbour is fitting --> return user bias as prediction\n",
    "        return df.mean(axis=1)[target_user] ## the user bias is the mean value of the given ratings of a user\n",
    "\n",
    "        \n",
    "    return enumerator/sum_similars ## the final item based prediction\n",
    "\n",
    "\n",
    "# quick check\n",
    "print(f'Prediction for top_k = 15 as given by jupyter notebook:\\n{item_based_CF(df, target_user = target_user, target_item = target_item, top_k = 15)}\\n') \n",
    "\n",
    "print(f'Prediction for top_k = 2 as given by the function:\\n{item_based_CF(df, target_user = target_user, target_item = target_item)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Recap: Evaluation I\n",
    "|        | item_1 | item_2 | item_3 | item_4 | item_5 |\n",
    "| ---    |   ---  |   ---  |   ---  |   ---  |   ---  |\n",
    "| user_1 | 3 |   | 2 | 3 | 3 |\n",
    "| user_2 | 4 | 3 | 4 | 3 |   |\n",
    "| user_3 | 3 | 2 | 1 | 4 | 4 |\n",
    "| user_4 |   | 5 | 4 | 3 | 1 |\n",
    "| user_5 | 5 |   | 3 | 4 | **(?)** |\n",
    "\n",
    "* How do we know if predictions given by our systems are any good? How do we compare systems to each other?\n",
    "* We have been predicting ratings for unseen items so far. What if we predict known ratings and compare the result to the actual values?\n",
    "* Let us take all known ratings as test set;\n",
    "\n",
    "$T$ - Test set\\\n",
    "$u$ - User\\\n",
    "$i$ - Item\\\n",
    "$r_{u,i}$ - Ground truth rating for the user $u$ and the item $i$\\\n",
    "$r'_{u,i}$ - Predicted rating for the user $u$ and the item $i$\n",
    "\n",
    "### Mean Absolute Error (MAE)\n",
    "\n",
    "* literal approach: aggregate differences between predictions and actual values.\n",
    "\n",
    "$MAE = \\frac{1}{|T|}\\sum \\limits _{(u,i)\\in T}{|r'_{u,i} - r_{u,i}|}$\n",
    "\n",
    "**What is worse: if we make two mistakes by 1 star, or one mistake by 2 stars**?\\\n",
    "**MAE doesn't care, but you should!**\n",
    "\n",
    "| Item_id | Actual Values | System 1 | System 2 |\n",
    "| - |      :---      |    ---   |    ---   |\n",
    "|1|⭐️⭐️⭐️⭐️| 4 | 4 |\n",
    "|2|⭐️⭐️⭐️⭐️⭐️ | <font color='red'>**4**</font> | 5 |\n",
    "|3|⭐️⭐️⭐️⭐️ | <font color='red'>**5**</font> | <font color='red'>**2**</font> |\n",
    "\n",
    "### Root Mean Squared Error (RMSE)\n",
    "\n",
    "* RMSE pays more attention to more prominent deviations;\n",
    "\n",
    "$RMSE = \\sqrt{\\frac{1}{|T|}\\sum \\limits _{(u,i)\\in T}{(r'_{u,i} - r_{u,i})^2}}$\n",
    "\n",
    "| Metric | System 1 | System 2 |\n",
    "| :---   |    ---   |    ---   |\n",
    "| MAE | 0.67 | 0.67 |\n",
    "| RMSE |  0.82 | 1.15 |\n",
    "\n",
    "* Can we normalize RMSE (to [0, 1])? How do we calculate max RMSE? What normalized RMSE can be good for?\n",
    "\n",
    "## <font color='red'>TASK 4/4</font>: RMSE\n",
    "Write a function that can be used to evaluate CF techniques from the first part of the exercise. The function needs to take a DataFrame and a callable name and return a single float value. **No normalization required!** See template below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization is good to compare over different types of ratings the RMSE values. To the internet normalization is possible for RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0213167223785722 1.2199081371708123\n"
     ]
    }
   ],
   "source": [
    "def rmse(TS: pd.DataFrame, rec_function, top_k=2)->float:\n",
    "    '''\n",
    "    TS - Data Frame - interation matrix with NaNs for unknown values\n",
    "    rec_function - collaborative filtering function to evaluate (see example calls at the very bottom of the cell)\n",
    "    top_k - Int - top k users or items to infer from\n",
    "    \n",
    "    returns - float - RMSE value \n",
    "    '''    \n",
    "    # Now, I am totally seeing what you meant by 'at this place df is wrong'. \n",
    "    # It is rather obvious. I only was too stressed to get it. :D\n",
    "    # So, thank you for the second chance.\n",
    "    \n",
    "    series_target_users = TS.T.keys()\n",
    "    series_target_items = TS.keys()\n",
    "    counter_not_nan = 0 ## is |T|\n",
    "    rmse_sum = 0\n",
    "    \n",
    "    for single_target_user in series_target_users:\n",
    "        for single_target_item in series_target_items:\n",
    "            \n",
    "            true_rating = TS.loc[single_target_user,single_target_item]\n",
    "            \n",
    "            if np.isnan(true_rating) == False: ## Ignore all slots which are NaN\n",
    "                counter_not_nan += 1\n",
    "                predicted_rating = rec_function(TS, single_target_user, single_target_item, top_k)\n",
    "                rmse_sum += (predicted_rating - true_rating)**2\n",
    "\n",
    "    return (rmse_sum/counter_not_nan)**(0.5) ## = RMSE\n",
    "\n",
    "    \n",
    "# quick check\n",
    "print(rmse(df, item_based_CF), rmse(df, user_based_CF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UB-CF returned 4.658975830340907\n",
      "IB-CF returned 4.5447291009838064\n",
      "RMSE4UB-CF returned 1.2199081371708123\n",
      "RMSE4IB-CF returned 1.0213167223785722\n"
     ]
    }
   ],
   "source": [
    "# Check-list:\n",
    "# Task 1\n",
    "print(\"UB-CF returned \"+str(user_based_CF(df, target_user = target_user, target_item = target_item)))\n",
    "# Task 2\n",
    "print(\"IB-CF returned \"+str(item_based_CF(df, target_user = target_user, target_item = target_item)))\n",
    "# Task 3 - no separate function to write\n",
    "# Task 4\n",
    "print(\"RMSE4UB-CF returned \"+ str(rmse(df, user_based_CF, 3)))\n",
    "print(\"RMSE4IB-CF returned \"+ str(rmse(df, item_based_CF, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave this cell the way it is, please."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
