{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: FUN(!) with Singular Vector Decomposition\n",
    "*Learning from User-generated Data, CP MMS, JKU Linz 2021*\n",
    "## Details:\n",
    "This is Exercise 2, an introduction to Matrix Factorization on example of Singular Vector Decomposition.<br>\n",
    "You don't need to submit anything for this exercise.<br>\n",
    "\n",
    "Your goal is to work with this notebook at home: read, run, experiment, make sure you understand what is happening here.<br>\n",
    "This exercise will be graded basing on your results on the next online test.<br>\n",
    "\n",
    "The test is (probably) going to be composed of multiple choice and insert-a-number questions and consern materal presented here and on corresponding lecture slides. Concentrate on applications and high-level understanding of algorithms, their pros and cons.<br>\n",
    "\n",
    "The test may contain questions related to the two previous lectures (Memory-based collaborative filtering, Evaluation).\n",
    "\n",
    "## Matrix Factorization\n",
    "Remember our old interaction matrix $R$?<br>\n",
    "Let us try applying factorization to it.\n",
    "\n",
    "|        | item_1 | item_2 | item_3 | item_4 | item_5 |\n",
    "| ---    |   ---  |   ---  |   ---  |   ---  |   ---  |\n",
    "| user_1 | 3 |   | 2 | 3 | 3 |\n",
    "| user_2 | 4 | 3 | 4 | 3 |   |\n",
    "| user_3 | 3 | 2 | 1 | 4 | 4 |\n",
    "| user_4 |   | 5 | 4 | 3 | 1 |\n",
    "| user_5 | 5 |   | 3 | 4 |   |\n",
    "\n",
    "\n",
    "The idea is to turn a huge (as above, but much bigger) interaction matrix $R$ of size:\n",
    "*    $n * m$ ($n$ - number of users, $m$ - number of items)\n",
    "    \n",
    "Into two smaller matrices:\n",
    "*    $U$ of size $n * f$ to represent users;\n",
    "*    $V$ of size $m * f$ to represent items;<br>\n",
    "($f$ - number of latent factors, typically $20<<f<<100$)<br><br>\n",
    "The $f$ dimensions are not neccesserily interpretable (usually they are not).<br>We can imagine each of the dimensions as a recommendation-related characteristic (e.g. **mainstreamness**).<br>This way element $u_{i,1}$ of matrix $U$ tells us how much User $i$ values this characteristic in items. (how much they value mainstream items)<br>Similarly $v_{j,1}$ of matrix $V$ tells how much the characteristic is applicable to the Item $j$ (how mainstream the item is).\n",
    "\n",
    "### Why factorize?\n",
    "* The two new matrices combined (should) take less space than the interaction matrix and thus easier fit into memory;\n",
    "* Selecting a reasonable $f$ means that we operate with shorter vectors during all kinds of calculations, this decreases computational load during recommendation making online inference easier;\n",
    "* Matrix factorization compresses sparce information contained in the interaction matrix into an elegant representation and can potentially encode **hidden dependencies**;\n",
    "* Having both items and users represented in the the same $f$-dimensional space opens new possibilities for recommendation;\n",
    "\n",
    "## Singular Vector Decomposition\n",
    "A $n * m$ matrix $R$ can be decomposed into a product of 3 matrices:<br>\n",
    "$R = U\\Sigma V^T$\n",
    "\n",
    "$U$ -- orthogonal ($n * n$) matrix composed of left singular vectors (it corresponds to users);<br>\n",
    "$\\Sigma$ -- ($n * m$) diagonal matrix, containing singular values;<br>\n",
    "$V$ -- orthogonal ($m * m$) matrix composed of right singular vectors (it corresponds to items);<br>\n",
    "\n",
    "### <font color='#666666'>Thin Variant</font> of  Singular Vector Decomposition\n",
    "As before:<br>\n",
    "$R = U\\Sigma V^T$\n",
    "\n",
    "We can exploit the fact that $R$ (usually) is not square and cannot have *full rank*.<br>\n",
    "$k = min(n, m)$\n",
    "\n",
    "As a result $U$, $\\Sigma$ and $V$ have different dimensions:<br>\n",
    "$U$ -- ($n *$ <font color='red'>$k$</font>) of left singular vectors (it corresponds to users);<br>\n",
    "$\\Sigma$ -- (<font color='red'>$k$</font> $*$ <font color='red'>$k$</font>) square diagonal matrix, containing singular values;<br>\n",
    "$V$ -- ($m *$ <font color='red'>$k$</font>) of right singular vectors (it corresponds to items);<br>\n",
    "\n",
    "This is a more efficient approach (requires less memory) and it also makes the following demonstration clearer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "from scipy import linalg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstration\n",
    "Our good old interaction matrix.<br>\n",
    "We will consider only the first three users.<br>Having distinctly $3$ users and $5$ items will help us easier understand how it all works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3. nan  2.  3.  3.]\n",
      " [ 4.  3.  4.  3. nan]\n",
      " [ 3.  2.  1.  4.  4.]] \n",
      "\n",
      "Users:\n",
      " ['user_1', 'user_2', 'user_3'] \n",
      "\n",
      "Items:\n",
      " ['RHCP', 'Radiohead', 'Bob_Marley', 'Pink_Floyd', 'ACDC']\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    'user_1' : {'item_1' : 3, 'item_2' : np.nan, 'item_3' : 2, 'item_4' : 3, 'item_5' : 3},\n",
    "    'user_2' : {'item_1' : 4, 'item_2' : 3, 'item_3' : 4, 'item_4' : 3, 'item_5' : np.nan},\n",
    "    'user_3' : {'item_1' : 3, 'item_2' : 2, 'item_3' : 1, 'item_4' : 4, 'item_5' : 4},\n",
    "    'user_4' : {'item_1' : np.nan, 'item_2' : 5, 'item_3' : 4, 'item_4' : 3, 'item_5' : 1},\n",
    "    'Active_User' : {'item_1' : 5, 'item_2' : np.nan, 'item_3' : 3, 'item_4' : 4, 'item_5' : np.nan}\n",
    "} # dictionary\n",
    "\n",
    "df = DataFrame(data).T # items become columns and user rows\n",
    "a = np.array(df)\n",
    "\n",
    "a_users = ['user_1','user_2','user_3','user_4','Active_User']\n",
    "a_items = ['RHCP', 'Radiohead', 'Bob_Marley', 'Pink_Floyd', 'ACDC']\n",
    "\n",
    "# Leaving a 3 by 5 matrix\n",
    "b = a[:3,:].copy()\n",
    "\n",
    "# having such arrays helps to keep track of what you recommend to whom \n",
    "b_items = a_items\n",
    "b_users = a_users[:3]\n",
    "\n",
    "print(b,\n",
    "      \"\\n\\nUsers:\\n\", b_users,\n",
    "     \"\\n\\nItems:\\n\", b_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying SVD\n",
    "SVD is included in many linear algebra modules. It is present in both Numpy and Scipy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "array must not contain infs or NaNs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-f071f1ced4bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# linalg.svd returns us the three desired components: U, s, V(transposed)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# scipy version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/linalg/decomp_svd.py\u001b[0m in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv, overwrite_a, check_finite, lapack_driver)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \"\"\"\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0ma1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_asarray_validated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_finite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'expected matrix'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/_lib/_util.py\u001b[0m in \u001b[0;36m_asarray_validated\u001b[0;34m(a, check_finite, sparse_ok, objects_ok, mask_ok, as_inexact)\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'masked arrays are not supported'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0mtoarray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray_chkfinite\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcheck_finite\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mobjects_ok\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'O'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/lib/function_base.py\u001b[0m in \u001b[0;36masarray_chkfinite\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtypecodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'AllFloat'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         raise ValueError(\n\u001b[0;32m--> 486\u001b[0;31m             \"array must not contain infs or NaNs\")\n\u001b[0m\u001b[1;32m    487\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: array must not contain infs or NaNs"
     ]
    }
   ],
   "source": [
    "# linalg.svd returns us the three desired components: U, s, V(transposed)\n",
    "U, s, Vh = linalg.svd(b) # scipy version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THIS ⬆\n",
    "Dont forget, **SVD requires a fully known matrix**, no NaNs allowed!<br>\n",
    "Ways to cope:\n",
    "* Augmentation techniques (see slides);\n",
    "\n",
    "Or better:\n",
    "* Choose a different factorization algorithm. Minimize squared error on known values (see slides);\n",
    "\n",
    "Below we replace NaNs with user biases (and without thinking too much) as a stop gap solution.<br>\n",
    "<font color='red'>In reality we'd rather not learn values that we inserted into the matrix ourselves.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Augmented\" interaction matrix:\n",
      " [[3.   2.75 2.   3.   3.  ]\n",
      " [4.   3.   4.   3.   3.5 ]\n",
      " [3.   2.   1.   4.   4.  ]]\n"
     ]
    }
   ],
   "source": [
    "# Creating an array with user-biases\n",
    "defaults = np.nanmean(a,axis=1) # we calculate the means of each row and ignore the nan-values totally\n",
    "\n",
    "for i in range(len(b)):\n",
    "    b[i][np.isnan(b[i])] = defaults[i] # np.isnan(b[i]) as mask: everywhere where row b[i] has nan we replace \n",
    "                                        # it with an default value\n",
    "\n",
    "print('\"Augmented\" interaction matrix:\\n',b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ok, let's try again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.  , 2.75, 2.  , 3.  , 3.  ],\n",
       "       [4.  , 3.  , 4.  , 3.  , 3.5 ],\n",
       "       [3.  , 2.  , 1.  , 4.  , 4.  ]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b # rows: users, columns: items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Matrix (R):\n",
      " [[3.   2.75 2.   3.   3.  ]\n",
      " [4.   3.   4.   3.   3.5 ]\n",
      " [3.   2.   1.   4.   4.  ]] \n",
      "\n",
      "Users (U):\n",
      " [[-0.52131022  0.01639708 -0.8532097 ]\n",
      " [-0.6515434  -0.65333981  0.38553637]\n",
      " [-0.55111419  0.75688719  0.35127613]] \n",
      "\n",
      "Data Variance per latent feature:\n",
      " [11.8767332   2.33326291  0.55820492] \n",
      "\n",
      "Same in a form of a diagonal matrix (Sigma):\n",
      " [[11.8767332   0.          0.        ]\n",
      " [ 0.          2.33326291  0.        ]\n",
      " [ 0.          0.          0.55820492]] \n",
      "\n",
      "Items (already transposed!) (Vh):\n",
      " [[-0.49032396 -0.37808896 -0.35362487 -0.481868   -0.5092974 ]\n",
      " [-0.12579227 -0.17192794 -0.78159983  0.47860897  0.33860336]\n",
      " [ 0.06511011 -0.87273558  0.3350064   0.00373437  0.34907025]]\n"
     ]
    }
   ],
   "source": [
    "U, s, Vh = np.linalg.svd(b, full_matrices = False)\n",
    "print(\"Initial Matrix (R):\\n\",\n",
    "      b,\n",
    "      \"\\n\\nUsers (U):\\n\",\n",
    "      U, # each row represents one user \n",
    "      \"\\n\\nData Variance per latent feature:\\n\",\n",
    "      s,\n",
    "      \"\\n\\nSame in a form of a diagonal matrix (Sigma):\\n\",\n",
    "      np.diag(s),\n",
    "      \"\\n\\nItems (already transposed!) (Vh):\\n\",\n",
    "      Vh) # When Vh wouldn't be transposed then each row would represent one item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$R = U\\Sigma V^T$\n",
    "\n",
    "Note, that we do **Thin SVD** here, meaning that $U$ is ($n *$ <font color='red'>$k$</font>) and $V^T$ is (<font color='red'>$k$</font> $* m$); Set the parameter **full_matrices** to **True** for vanilla SVD. (see documentation)<br>\n",
    "\n",
    "Both Scipy and Numpy implementations return the diagonal matrix $\\Sigma$ as an array of diagonal values $s$. How very kind of them.<br>\n",
    "\n",
    "We can immediately see if SVD works the way we expect it to. Don't forget to convert $s$ to a matrix with *np.diag(s)*.<br>Use *@* for matrix multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_r = U @ np.diag(s) @ Vh # reconstructing the original matrix # @ stands for matrix multiplication np.matmul(a,b)\n",
    "\n",
    "print(\"Original matrix:\\n\",b)\n",
    "print(\"\\n\\nReconstructed matrix:\\n\", b_r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cool things about SVD\n",
    "SVD projects all the variance contained in the data onto orthogonal basis of $k$ vectors.<br>\n",
    "Singular values ($\\Sigma$ or $s$) allow us to judge how much variance is \"situated\" along each vector. It is also acts as weighting for the $k$ dimensions;\n",
    "\n",
    "| $\\Sigma$ |  |  |\n",
    "|--|--|--|\n",
    "| **11.9** | 0.0 | 0.0 |\n",
    "| 0.0 | **2.3** | 0.0  |\n",
    "| 0.0 | 0.0 | **0.6** |\n",
    "\n",
    "Basing on that, we can choose $f < k$ (remember $f$?⬆) dimensions to represent the whole data. Choosing dimensions corresponding to lagest singular values we make sure to keep data approximately the same while decreasing its size and maybe even filtering some noise out. $U$ and $V^T$ become ($n *$ <font color='red'>$f$</font>) and (<font color='red'>$f$</font> $* m$) respectively.<br><br>\n",
    "Let's select only $f = 2$ latent features out of $3$ we got, and check how the matrix will change. **Truncated SVD**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 2 \n",
    "b_r_f = U[:,:f] @ np.diag(s[:f]) @ Vh[:f,:] # reconstructed with only f features\n",
    "# U[:,:f]: simply only use part of U. U[row_slicing, column_slicing]\n",
    "diff = b - b_r_f\n",
    "\n",
    "print(\"Initial matrix:\\n\",\n",
    "      b,\n",
    "      \"\\n\\nMatrix reconstructed with \", f, \" features:\\n\",\n",
    "      b_r_f.round(decimals=2),\n",
    "      \"\\n\\nProportion of variance captured:\",s[:f].sum() / s.sum(),\n",
    "     \"\\n\\nDifferences ( std = \",np.std(diff.round(decimals=2)),\"):\\n\", diff.round(decimals=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we use it now?\n",
    "Thanks to matrix factorization we have vector representations of both users ($u_i$) and items ($v_j$) (in the same space).\n",
    "\n",
    "* Project Users and Items to the latent space and select most informative features (corresponding to the higher variance/singular values) to make overall representation more compact;\n",
    "* By the definition predicted rating for ($User_i$ & $Item_j$) is $u_i \\Sigma  v_j$;\n",
    "* User/Item representations can still be used to estimate similarity in User-/Item- based **Collaborative Filtering**;\n",
    "* Now that both users and items can be represented in the same space - we can **directly find closest items for a given user**;\n",
    "\n",
    "\n",
    "* Bearing in mind that vectors of $U$ and $V^T$ are bases of the new f-dimensional space, we can use them to project new users and new items into that space and recommend them (or to them). <font color='red'>Of course this way our model does not take into account additional variance the new items and users may introduce! In order to include the new data to your model you will have to do SVD again on the new interaction matrix!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projecting a new user to the f-dimensional space\n",
    "user = np.array([3.  , 2., 1.  , 4.  , 4.  ])\n",
    "\n",
    "print(\"Initial matrix:\\n\",\n",
    "      b,\n",
    "      \"\\n\\nU:\\n\",\n",
    "      U)\n",
    "\n",
    "print(\"\\n\\nUser: \",user, \"projected into the f-dim space:\\n\", np.diag(s**(-1)) @ Vh @ user.T)\n",
    "user.T @ Vh.T @ np.diag(s**(-1)) # get same result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projecting a new item to the f-dimensional space\n",
    "# try projecting a known item and see if the resulting vector is the same as produced by SVD\n",
    "item = [3., 3.5, 4] # we need exactly 3 items bc rated by the 3 given users ???\n",
    "\n",
    "print(\"Initial matrix:\\n\",\n",
    "      b,\n",
    "      \"\\n\\nVh:\\n\",\n",
    "      Vh)\n",
    "\n",
    "print(\"\\n\\nItem: \",item, \"projected into the f-dim space:\\n\", item @ U @ np.diag(s**(-1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user2 = [5.  , 2., 3.  , 4.  , 4.  ]\n",
    "user2_p = np.diag(s**(-1)) @ Vh @ user2\n",
    "print(\"\\n\\nUser: \",user2, \"projected into the f-dim space:\\n\",np.diag(s**(-1)) @ Vh @ user2)\n",
    "\n",
    "item2 = [1., 1, 1]\n",
    "item2_p = item2 @ U @ np.diag(s**(-1))\n",
    "print(\"\\n\\nItem: \",item2, \"projected into the f-dim space:\\n\",item2 @ U @ np.diag(s**(-1)))\n",
    "\n",
    "print(\"\\n\\nPredicted score for the two items:\\n\", user2_p @ np.diag(s) @ item2_p.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "* In general you don't have to follow the $u_i \\Sigma  v_j$ mantra for estimating the rating. SVD gives you vector representations and you are free to interpret them as you wish (make sure you are consistent). For example $\\Sigma$ makes us treat dimensions differently depending on how much variance they carry which can be not always desirable;\n",
    "* SVD becomes classical matrix factorization when we get our two matrices, one for users ($U \\Sigma^{1/2}$) and items ($\\Sigma^{1/2} V^T$);\n",
    "* Learning latent factors is a more direct approach, it just splits the interaction matrix into two sets of vectors (matrices), taking into account only known values;\n",
    "* Try different approaches and decide what works best for you in every particular case;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions and Exercises:\n",
    "* What would be an easy way to calculate RSME for the 2-factor model we created above? How many lines of code would you need?\n",
    "* What are benefits of using Matrix Factorization?\n",
    "* Is it easy to update such a model with new items and users? Is it easier than updating a memory-based model?\n",
    "* What are disadvantages of using SVD for recommendation?\n",
    "* What are advantages of learning latent factors (ALS, Gradient Descent) compared to SVD? What is the idea of learning latent factors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What would be an easy way to calculate RSME for the 2-factor model we created above? How many lines of code would you need? <br>\n",
    "lines:\n",
    "1: import stuff, data<br>\n",
    "2: transform data to df<br>\n",
    "3: np.linalg to get S sigma V.T<br>\n",
    "4: Build diagonal matrix of sigma<br>\n",
    "5: choose first two columns of S, sigma, V because f=2 <br>\n",
    "6: Compute R with truncated SVD<br>\n",
    "7: for loop over all users<br>\n",
    "8: for loop over all items<br>\n",
    "9: RMSE-sum<br>\n",
    "10: out of loop; RMSE formula<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What are benefits of using Matrix Factorization?<br>\n",
    "We can also track other characteristica of users and items without knowing them explicitly.<br>\n",
    "in f-dimensional space we can compare users with items by distances. <br>\n",
    "with a f-dimensional space we consider the most important factors without much loss of information --> less memory needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Is it easy to update such a model with new items and users? Is it easier than updating a memory-based model?\n",
    "Yes, it is easy to update the model by simple formulas. <br>\n",
    "Easier? Difficult to answer. You need ALL item/user values which are considered in the f-dimesional space. When new user comes with new item then we would have a problem. <br>\n",
    "Pros: No need of similarity computing which compares new object with all other objects. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What are disadvantages of using SVD for recommendation?\n",
    "Not interpretable<br>\n",
    "cold-start problem<br>\n",
    "nans not allowed/hard to handle<br>\n",
    "need of other users<br>\n",
    "Computationally expensive<br>\n",
    "Compute SVD new when over time too much new users/items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What are advantages of learning latent factors (ALS, Gradient Descent) compared to SVD? What is the idea of learning latent factors?<br>\n",
    "nans can be in user rating matrix<br>\n",
    "only learns on test set: Makes predictions w_u.T*h_i on the fly when needed (less memory space, no need ) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
