{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "id": "RP4bfxlPUuc5",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<!-- Assignment 1 - SS 2023 -->\n",
    "\n",
    "# Deep Learning with Pytorch (0 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5X_W2eeUuc9",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This notebook contains one of the assignments for the exercises in Deep Learning and Neural Nets 2.\n",
    "It provides a skeleton, i.e. code with gaps, that will be filled out by you in different exercises.\n",
    "All exercise descriptions are visually annotated by a vertical bar on the left and some extra indentation,\n",
    "unless you already messed with your jupyter notebook configuration.\n",
    "Any questions that are not part of the exercise statement do not need to be answered,\n",
    "but should rather be interpreted as triggers to guide your thought process.\n",
    "\n",
    "**Note**: The cells in the introductory part (before the first subtitle)\n",
    "perform all necessary imports and provide utility functions that should work without (too much) problems.\n",
    "Please, do not alter this code or add extra import statements in your submission, unless explicitly allowed!\n",
    "\n",
    "<span style=\"color:#d95c4c\">**IMPORTANT:**</span> Please, change the name of your submission file so that it contains your student ID!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-FIbcwcqUudA",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In this assignment, the main goal is to get familiar with deep learning in pytorch.\n",
    "At the same time it should help you refresh what you (should have) learned in DL & NN 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4ZvGmkEHUudC",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "torch.manual_seed(1806);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Iea72FSvUudD",
    "outputId": "da6058ee-b8fc-4fef-90d6-f722a6e32367",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n",
      "gdrive/MyDrive/.pytorch\n"
     ]
    }
   ],
   "source": [
    "# google colab data management\n",
    "import os.path\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    _home = 'gdrive/MyDrive/'\n",
    "except ImportError:\n",
    "    _home = '~'\n",
    "finally:\n",
    "    data_root = os.path.join(_home, '.pytorch')\n",
    "\n",
    "print(data_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FWjJ2w3IUudF",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Deep Learning Frameworks\n",
    "\n",
    "There are plenty of freely available frameworks that can be used for deep learning.\n",
    "Although most frameworks are written using low-level programming languages for speed,\n",
    "most of them provide bindings for [python](https://docs.python.org/3).\n",
    "Therefore, python has become the number one machine/deep learning language.\n",
    "\n",
    "In DL&NN 1, we already learned how [numpy](https://docs.scipy.org/doc/numpy) can be used to implement a neural network.\n",
    "Although numpy provides highly optimised code for linear algebra, it does not support any hardware acceleration.\n",
    "Graphical Processing Units (GPUs) have become one of the main hardware accelerators for deep learning \n",
    "(and more generally, for matrix multiplications).\n",
    "In order to accelerate python and/or numpy code on a GPU, one could use a framework like [numba](http://numba.pydata.org/) or even [pycuda](https://documen.tician.de/pycuda/).\n",
    "More recently, Tensor Processing Units (TPUs) were developed (by Google) to further accelerate deep learning computations.\n",
    "Accelerating your python code for TPUs (also works for GPUs) can be achieved by [jax](https://jax.readthedocs.io/en/latest/).\n",
    "\n",
    "Apart from the TPU/GPU acceleration, jax also provides automatic differentiation for python or numpy code.\n",
    "This means that once the forward pass of the network has been implemented, \n",
    "the framework is able to compute the gradients for the backward pass automatically.\n",
    "Whereas all of these frameworks enhance numpy to make it a more powerful tool for deep learning,\n",
    "the \"*true*\" deep learning frameworks offer hardware acceleration and differentation out of the box."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jlgw59TLUudH",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###### Theano and Tensorflow v1\n",
    "\n",
    "One of the first frameworks for deep learning, [Theano](http://www.deeplearning.net/software/theano/) was developed by MILA.\n",
    "The core idea of Theano is to build computational graphs that can then be optimised and are executed in a second phase.\n",
    "```python\n",
    "# single layer network in Theano (untested)\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "class Network:\n",
    "    \"\"\" Represents the computational graph of the network. \"\"\"\n",
    "\n",
    "    def __init__(self, x):\n",
    "        self.x = x   # x would be a graph node representing the input\n",
    "        self.w = theano.shared(value=np.zeros(x.shape[-1], 1), name='W')\n",
    "        self.b = theano.shared(value=np.zeros(1, ), name='b')\n",
    "\n",
    "    @property\n",
    "    def pre_activation(self):\n",
    "        \"\"\" \n",
    "        Construct graph node that computes pre-activations.\n",
    "        Does NOT compute pre-activations!\n",
    "        \"\"\"\n",
    "        return T.dot(self.x, self.w) + self.b\n",
    "\n",
    "    @property\n",
    "    def activation(self):\n",
    "        \"\"\" \n",
    "        Construct graph node that computes pre-activations.\n",
    "        Does NOT compute activations!\n",
    "        \"\"\"\n",
    "        return 1 / (1 + T.exp(-self.pre_activation))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7PxdYu3vUudJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Theano by itself has been discontinued, but the ideas in Theano have been adopted by the Google engineers\n",
    "that created the first version of [Tensorflow](https://www.tensorflow.org/versions#tensorflow_1x).\n",
    "```python\n",
    "# single layer network in Tensorflow v1\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class Network:\n",
    "    \"\"\" Represents the computational graph of the network. \"\"\"\n",
    "\n",
    "    def __init__(self, x):\n",
    "        self.x = x   # x would be a graph node representing the input\n",
    "        self.w = tf.Variable(tf.zeros((x.shape[-1], 1)))\n",
    "        self.b = tf.Variable(tf.zeros(1))\n",
    "\n",
    "    @property\n",
    "    def pre_activation(self):\n",
    "        \"\"\" \n",
    "        Construct graph node that computes pre-activations.\n",
    "        Does NOT compute pre-activations!\n",
    "        \"\"\"\n",
    "        return tf.matmul(self.x, self.w) + self.b\n",
    "\n",
    "    @property\n",
    "    def activation(self):\n",
    "        \"\"\" \n",
    "        Construct graph node that computes pre-activations.\n",
    "        Does NOT compute activations!\n",
    "        \"\"\"\n",
    "        return 1 / (1 + tf.exp(-self.pre_activation))\n",
    "\n",
    "# construct graph\n",
    "x = tf.placeholder(dtype=tf.float32, shape=(None, 5))\n",
    "network = Network(x)\n",
    "w_grad,  = tf.gradients(network.activation, [network.w])  # automatic differentiation\n",
    "\n",
    "# run through the network\n",
    "data = np.random.randn(10, 5)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    a, w_grad = sess.run([network.activation, w_grad], feed_dict={network.x: data})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KwkcwKaWUudL",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###### Pytorch and Tensorflow v2\n",
    "\n",
    "The graph building in theano and tensorflow v1 can be used for optimisation,\n",
    "but its main goal is to allow automatic differentiation through this graph (in reverse mode).\n",
    "Instead of relying on automatic differentation through a graph, \n",
    "it is also possible to just implement the analytical derivatives for every basic function.\n",
    "This is the approach that the Facebook engineers used in torch and [pytorch](https://pytorch.org/).\n",
    "By tracking the function calls in the forward pass, \n",
    "the backward pass does not need to be implemented separately.\n",
    "```python\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Network(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_in: int):\n",
    "        super().__init__()\n",
    "        self.w = nn.Parameter(torch.zeros(num_in, 1))\n",
    "        self.b = nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        s = torch.dot(x, self.w) + self.b\n",
    "        return torch.sigmoid(s)\n",
    "\n",
    "x = torch.randn(10, 5)\n",
    "network = Network(x.shape[-1])\n",
    "network.zero_grad()\n",
    "a = network(x)\n",
    "a.backward()\n",
    "w_grad = network.w.grad\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BLEsBQy7UudM",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "A similar approach has been taken for the second version of [Tensorflow](https://www.tensorflow.org/versions#tensorflow_20).\n",
    "Instead of building graphs, tensorflow v2 operates in the so-called *eager mode* by default\n",
    "and makes use of a *gradient tape* to keep track of the graph.\n",
    "```python\n",
    "# single layer network in Tensorflow v2\n",
    "import tensorflow as tf\n",
    "\n",
    "class Network:\n",
    "    \"\"\" Represents the network. \"\"\"\n",
    "\n",
    "    def __init__(self, num_in):\n",
    "        self.w = tf.Variable(tf.zeros((num_in, 1)))\n",
    "        self.b = tf.Variable(tf.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        s = tf.matmul(x, self.w) + self.b\n",
    "        return 1 / (1 + tf.exp(-s))\n",
    "\n",
    "# run through the network\n",
    "x = tf.random.normal((10, 5))\n",
    "network = Network(x.shape[-1])\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(network.w)\n",
    "    a = network.forward(x)\n",
    "w_grad = tape.gradient(a, network.w)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hVP-H6L7UudP",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###### Other Libraries\n",
    "\n",
    "Although Tensorflow and Pytorch are the most commonly used frameworks for deep learning,\n",
    "there are plenty of alternatives, most of which are open source. Some examples are:\n",
    " * [CogNitive ToolKit (CNTK)](https://docs.microsoft.com/en-us/cognitive-toolkit/) (Microsoft)\n",
    " * [nnabla](https://nnabla.org/) (Sony)\n",
    " * [mxnet](https://mxnet.apache.org/) (Apache)\n",
    " * [Chainer](https://chainer.org/) (Preferred Networks)\n",
    " * ...\n",
    "\n",
    "There are also plenty of libraries that build upon these frameworks.\n",
    "Mostly, these frameworks are easier to use, but therefore harder to customise.\n",
    "Some noteable examples are:\n",
    " * [Lasagne](https://lasagne.readthedocs.io/en/latest/) (unmaintained) \n",
    " was intended to make stacking layers with Theano easier.\n",
    " * [Keras](https://keras.io/) (F. Chollet, Google) is to be considered an API. \n",
    " Reference implementations exist for Theano, TF, CNTK and numpy (for inference).\n",
    " NOTE: tensorflow completely incorporated the Keras API.\n",
    " * [Caffe](https://caffe.berkeleyvision.org/) (BAIR) and [Caffe 2](https://caffe2.ai) (Facebook, discontinued) \n",
    " allow(ed) to define models by means of configuration files. Caffe 2 has been merged into pytorch.\n",
    " * [Sonnet](https://github.com/deepmind/sonnet) and [dm-haiku](https://github.com/deepmind/dm-haiku) (Deepmind, experimental)\n",
    " are frameworks built on top of tensorflow and jax, respectively, to abstract away some of the pesky details.\n",
    " * ...\n",
    "\n",
    "In this course, we will be using **pytorch** as the main framework,\n",
    "because it has gained massive popularity over the last few years\n",
    "and enables to write pythonic code for deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mpBB_1v1UudP",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Neural Networks\n",
    "\n",
    "The pytorch code for building neural networks is mainly grouped in the `torch.nn` submodule.\n",
    "E.g. fully connected layers are implemented in the [`torch.nn.Linear`](https://pytorch.org/docs/stable/nn.html#torch.nn.Linear) class, \n",
    "convolutional layers (for images) can be used as [`torch.nn.Conv2d`](https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d),\n",
    "and highly optimised versions of LSTMS are available through [`torch.nn.LSTM`](https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM).\n",
    "Also plenty of [activation functions](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity),\n",
    "[loss functions](https://pytorch.org/docs/stable/nn.html#loss-functions),\n",
    "[normalisation methods](https://pytorch.org/docs/stable/nn.html#normalization-layers) \n",
    "are readily available in the `torch.nn` submodule. To easily stack all of these layers,\n",
    "there is also the [`torch.nn.Sequential`](https://pytorch.org/docs/stable/nn.html#sequential) module and much more.\n",
    "Because this submodule provides so much useful tools,\n",
    "it is common to import the submodule directly using `from torch import nn`,\n",
    "as has been done at the start of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQqxG9dJUudQ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Exercise 1: Fully Connected Network (0 points)\n",
    "\n",
    "Time to get familiar with the DL module in the `nn` submodule.\n",
    "Let us build a fully connected network to classify the [MNIST](http://yann.lecun.com/exdb/mnist/) images.\n",
    "\n",
    " > Construct a new pytorch module, by writing a class that inherits from `nn.Module`.\n",
    " > The pytorch module should represent a network with three fully connected layers.\n",
    " > The hidden layers should have 500 and 300 units and the output layer returns the logits.\n",
    " > For the activation function in the hidden layers, you can use the hyperbolic tangent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "4pk_90w6UudQ",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib.cbook import index_of\n",
    "class FCNetwork(nn.Module):\n",
    "    \"\"\" Network with fully-connected layers. \"\"\"\n",
    "    \n",
    "    def __init__(self, in_features: int, out_features: int):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        in_features : int\n",
    "            Number of input dimensions.\n",
    "        out_features : int\n",
    "            Number of output dimensions.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features=out_features\n",
    "\n",
    "    def forward(self,x):\n",
    "        #torch.randn(10, self.in_features)\n",
    "        l1 = torch.nn.Linear(self.in_features,500)\n",
    "        out = l1(x)\n",
    "        l2 = torch.nn.Linear(500,300)\n",
    "        out = l2(out)\n",
    "        out = torch.tanh(out)\n",
    "\n",
    "        l3 = torch.nn.Linear(300,self.out_features)\n",
    "        return l3(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uVlmwpHqUudS",
    "outputId": "f4d30187-3c0d-4a72-d408-164b40482d37"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "fc_net = FCNetwork(784, 10)\n",
    "fc_net(torch.randn(1, 784))\n",
    "sum([par.numel() for par in fc_net.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9EDU1ApUudT",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Exercise 2: Convolutional Network (0 points)\n",
    "\n",
    "Instead of creating a new class, we can also use the `Sequential` module.\n",
    "Since convolutional networks are generally better suited to images,\n",
    "we will build a MNIST classifier with convolutional layers this time.\n",
    "Any other benefits of convolutional layers you can think of?\n",
    "\n",
    " > Use the `Sequential` module to construct a network with two convolutional layers.\n",
    " > The inputs for this network are single-channel 28x28 images.\n",
    " > The result of the first convolutional layer should have **five 24x24 feature maps**.\n",
    " > The second convolutional layer should receive **five 6x6 feature maps** as input\n",
    " > and produce **ten 3x3 feature maps**.\n",
    " > Use average pooling for the dimensionality reduction between the two convolutional layers.\n",
    " > Finally, add a fully-connected layer to map the feature maps to logits for 10 distinct classes.\n",
    " > Use the Exponential Linear Unit (ELU) as activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4wVEdm6lUudU",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: create network with `nn.Sequential`\n",
    "conv_net = nn.Sequential(nn.Conv2d(in_channels=1, out_channels=5, kernel_size=(5,5)), # output_size = input_size - kernel_size + 1 \n",
    "                         nn.ELU(),\n",
    "                         nn.AvgPool2d(kernel_size=(4,4)), # keep 1/4 of the size\n",
    "                         nn.Conv2d(in_channels=5, out_channels=10, kernel_size=(4,4)),\n",
    "                         nn.ELU(),\n",
    "                         nn.Flatten(),\n",
    "                         nn.Linear(10*3*3, 10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Srcit2zTUudU",
    "outputId": "a2f9c69a-bf71-40e1-a3df-eead94679a66"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1850"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "conv_net(torch.randn(1, 1, 28, 28))\n",
    "sum([par.numel() for par in conv_net.parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7IoMGHyXUudV",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data\n",
    "\n",
    "Any machine learning method requires data. \n",
    "Neural networks often require more data than other methods to learn well.\n",
    "Obtaining good data is often one of the hardest tasks in machine learning.\n",
    "Luckily, there are plenty of standardly available datasets around\n",
    "to play around with and test new methods on.\n",
    "\n",
    "These standard datasets can easily be used in pytorch. \n",
    "In this course, we will make use of some standard image datasets\n",
    "that are made available through the [`torchvision`](https://pytorch.org/docs/stable/torchvision/datasets.html) package.\n",
    "However, there are also [`torchaudio`](https://pytorch.org/audio/datasets.html) and\n",
    "[`torchtext`](https://pytorch.org/text/datasets.html) packages to work with standard audio, resp. text datasets.\n",
    "In this assignment, we will focus mainly on the `MNIST` dataset from the `torchvision` package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6VYalEJQUudW",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###### Pre-processing\n",
    "\n",
    "Once the data is available, some sort of pre-processing is generally necessary.\n",
    "The datasets provided by torchvision can easily be pre-processed \n",
    "by using the `transform` and/or `transform_target` parameters in the constructor.\n",
    "These parameters allow to pre-process the inputs, resp. target outputs for the network.\n",
    "As a matter of fact, the torchvision datasets require pre-processing,\n",
    "since they return pillow (PIL) images, rather than pytorch tensors.\n",
    "\n",
    "The [`torchvision.transforms`](https://pytorch.org/docs/stable/torchvision/transforms.html) module provides a wide variety of pre-processing operations.\n",
    "The necessary transformation to use the MNIST data for deep learning is `torvision.transforms.ToTensor`.\n",
    "To use different pre-processing steps, the `Compose` transformation can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "33Rzj5vWUudW",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###### Data Loader\n",
    "\n",
    "In order to maximise the computation when training a neural network,\n",
    "it is important that the data is efficiently transported from the hard drive to (GPU/TPU) memory.\n",
    "In pytorch, this data transport is implemented by means of the [`DataLoader`](https://pytorch.org/docs/stable/data.html) class.\n",
    "\n",
    "A dataloader allows to iterate over a mini-batches with a specific batch-size.\n",
    "When iterating over the dataloader, it collects data at random indices from some `Dataset`,\n",
    "e.g. `MNIST`, to form a mini-batch of data on-the-fly.\n",
    "In order to assure that the network never has to wait for new data to be loaded in memory,\n",
    "the dataloader can use multiple processes load data by means of the `num_workers` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AgG_t7cQUudX",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Exercise 3: The MNIST Dataset (0 points)\n",
    "\n",
    "An exercise to make you familiar with pytorch datasets.\n",
    "\n",
    " > Implement the `get_mnist_loader` function so that it returns a pytorch dataloader\n",
    " > that iterates over mini-batches of MNIST images.\n",
    " > Normalise the data so that it has zero mean and unit variance.\n",
    " > Make sure to use all keyword arguments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "e2uMm7I8UudX",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_mnist_loader(root, test=True, batch_size=1, num_workers=0, shuffle=False):\n",
    "    \"\"\"\n",
    "    Create a dataloader for the MNIST data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    root : str\n",
    "        Path to the MNIST dataset.\n",
    "    test : bool\n",
    "        Flag to use either test or train data.\n",
    "    batch_size : int\n",
    "        Number of samples in each mini-batch.\n",
    "    num_workers : int\n",
    "        Number of parallel processes for pre-processing.\n",
    "    shuffle : bool\n",
    "        If `True`, data will be shuffled\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    loader : DataLoader\n",
    "        A dataloader to iterate over the MNIST data.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # preprocessing:\n",
    "    from torchvision import transforms as T\n",
    "\n",
    "    preprocess = T.Compose([\n",
    "    #T.Resize(256),\n",
    "    #T.CenterCrop(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(\n",
    "       mean=[0],\n",
    "       std=[1]\n",
    "    )\n",
    "    ])\n",
    "\n",
    "\n",
    "    tensored_imgs = torchvision.datasets.MNIST(root, train=(not test),download=True,transform=preprocess)\n",
    "\n",
    "    loader = DataLoader(dataset=tensored_imgs,batch_size=batch_size,num_workers=num_workers,shuffle=shuffle)\n",
    "\n",
    "    return loader\n",
    "\n",
    "\n",
    "    '''\n",
    "    # shuffle and get minibatches iteratively:\n",
    "    if shuffle == True:\n",
    "      tensored_imgs = tensored_imgs[torch.randperm(tensored_imgs.size()[0])]\n",
    "    \n",
    "    copied_imgs = tensored_imgs.copy_()\n",
    "\n",
    "    #for ind, single_img in enumerate(tensored_imgs):\n",
    "    while copied_imgs.size > 0:\n",
    "      to_yield = copied_imgs[:batch_size]\n",
    "      copied_imgs = copied_imgs[batch_size:]\n",
    "      yield to_yield\n",
    "    ''';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v1Yw-QxsUudY"
   },
   "source": [
    "## Differentiation and Optimisation\n",
    "\n",
    "Just like most of the deep learning frameworks, \n",
    "pytorch relieves the burden of computing gradients through the entire network.\n",
    "Given a pytorch tensor that is the result of applying a series of (differentiable) pytorch functions,\n",
    "it is essentially possible to get the gradients w.r.t. any intermediate pytorch tensor in the computation.\n",
    "Most of this functionality can be found in the [`torch.autograd`](https://pytorch.org/docs/stable/autograd.html) submodule. \n",
    "\n",
    "###### Computing Gradients\n",
    "\n",
    "When working with neural networks, gradients from a scalar loss w.r.t. parameters are of special interest.\n",
    "Therefore, pytorch provides this functionality practically by default (as soon as a variable is marked to require gradients).\n",
    "In order to allow gradient computation by default,\n",
    "pytorch must keep track of which functions were called to get from one variable to another.\n",
    "This of course introduces a certain amount of overhead,\n",
    "which might not be necessary if you do not need the gradients.\n",
    "\n",
    "Consider the very simple example of computing the sum of the elements in some random vector.\n",
    "```python\n",
    "x = torch.randn(10)\n",
    "y = x.sum()\n",
    "print(y)\n",
    "```\n",
    "In this case, there are no variables that require gradients, and it is not possible to compute gradients.\n",
    "However, if we explicitly require gradients for `x`, \n",
    "`y` will have a `grad_fn` to keep track of the computations.\n",
    "```python\n",
    "x = torch.randn(10, requires_grad=True)\n",
    "y = x.sum()\n",
    "print(y)\n",
    "```\n",
    "In order to compute the gradients of a scalar w.r.t. all the tensors that require gradients,\n",
    "we can then simply call `backward` on the scalar result tensor\n",
    "and the gradients will be stored in the `grad` field of the respective tensors.\n",
    "In our example, `y.backward()` would compute the gradients for `x` and store these in `x.grad`.\n",
    "\n",
    "###### Controlling the Gradients\n",
    "\n",
    "When building neural networks with the default pytorch layers from the `nn` submodule,\n",
    "all trainable parameters will require grads, which will trigger the computational overhead.\n",
    "In order to explicitly disable this overhead, it is possible to use `torch.no_grad()` either\n",
    "as [context manager](https://docs.python.org/3/reference/compound_stmts.html#with) \n",
    "or [decorator](https://docs.python.org/3/glossary.html#term-decorator).\n",
    "```python\n",
    "x = torch.randn(10, requires_grad=True)\n",
    "# use as context manager\n",
    "with torch.no_grad():\n",
    "    y = x.sum()\n",
    "print(y)\n",
    "```\n",
    "\n",
    "The gradient overhead is not only computational, but can also have an impact on memory.\n",
    "A tensor with a non-empty `grad_func` field implicitly stores the entire computational graph\n",
    "and caches the tensors it needs to backpropagate through the network.\n",
    "Therefore, it is important to clean this graph when storing such tensors for longer.\n",
    "In pytorch the `detach` method allows to create a copy of a tensor without the computational graph.\n",
    "For scalar values, also the `item` method can be used to convert a scalar tensor to a native python variable.\n",
    "This is especially useful when monitoring loss values, e.g. for plotting.\n",
    "For non-scalar tensors, the equivalent method is `tolist`.\n",
    "However, often it is more interesting to convert to numpy arrays using something like\n",
    "`y.detach().numpy()`.\n",
    "\n",
    "Finally, an important feature/pitfall of pytorch, is the fact that gradients are accumulated.\n",
    "This allows to easily combine gradients from e.g. different terms in the loss (remember L2 regularisation).\n",
    "However, this also means that it is crucial to reset the gradients to get the correct values!\n",
    "A simple example of how this can cause confusion and/or problems is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4uCWfi2QUudj",
    "outputId": "aa667b6a-56c1-455f-a391-fb0481fa606b",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y1      gradient: tensor([1., 1., 1., 1., 1.])\n",
      "y1 + y2 gradient: tensor([1., 3., 5., 7., 9.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(5.).requires_grad_(True)\n",
    "y1, y2 = x.sum(), torch.sum(x ** 2)\n",
    "y1.backward()\n",
    "print(\"y1      gradient:\", x.grad) # gradient of y1\n",
    "y2.backward()\n",
    "print(\"y1 + y2 gradient:\", x.grad) # gradient of y1 + y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uj6_0EuIUudk",
    "outputId": "77818016-a3a0-4f10-c2d9-6f0d811d803b",
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrong   y1 gradient: tensor([ 2.,  4.,  6.,  8., 10.])\n",
      "correct y1 gradient: tensor([1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# compute gradient from y1 again (wrong)\n",
    "y1.backward()\n",
    "print(\"wrong   y1 gradient:\", x.grad) # gradient of y1 + y2 + y1\n",
    "# compute gradient from y1 again (right)\n",
    "del x.grad  # reset gradient\n",
    "y1.backward()\n",
    "print(\"correct y1 gradient:\", x.grad) # gradient of y1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oV5ocRtiUudk",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###### Optimisation\n",
    "\n",
    "In order to update the network parameters, we need an optimisation strategy.\n",
    "Most of the update algorithms from DL & NN 1 can be found in the \n",
    "[`torch.optim`](https://pytorch.org/docs/stable/optim.html) submodule.\n",
    "E.g. stochastic gradient descent (with momentum) can be found as `torch.optim.SGD`.\n",
    "These optimisers update the parameters of a network in a direction\n",
    "that is computed from the gradients.\n",
    "\n",
    "A pytorch optimiser stores a reference to the parameters of a network.\n",
    "By using the `step` method, all of the parameters will be updated,\n",
    "using the information that is stored in the `grad` field of these parameters.\n",
    "In order for this `grad` field to contain meaningful information,\n",
    "a call to `backward` is necessary before calling `step`.\n",
    "In order to make sure that the gradients are not corrupted,\n",
    "pytorch optimisers also provide a `zero_grad` method.\n",
    "This method resets the gradients for all the parameters it can update.\n",
    "This method should be called before gradients are computed, as noted earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAIbvYTHUudl"
   },
   "source": [
    "### Exercise 4: Propagating and Updating the Network (0 points)\n",
    "\n",
    "Two very common tasks in deep learning are\n",
    " 1. updating the network to get better predictions,\n",
    " 2. evaluating the function to see how good predictions really are.\n",
    "Because functions performing these tasks are useful for practically *any* \"standard\" project,\n",
    "it is generally not a waste of time to implement them early on.\n",
    "\n",
    " > Implement the `evaluate` and `update` functions to perform the above tasks.\n",
    " > They should both iterate over all the mini-batches in the given data-loader, `data`, once\n",
    " > and compute the `loss` or `metric` given the resp. functions.\n",
    " > Additionally, the `update` function should optimise the loss\n",
    " > with the specified optimisation algorithm `opt`.\n",
    " > Both functions should return the list of errors for each mini-batch.\n",
    " > Make sure to keep your gradients under control! Both computationally and memory-wise.\n",
    "\n",
    "**NOTE:** these are functions you will want to use in future assignments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "_msNWJG6Uudm",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(network: nn.Module, data: DataLoader, metric: callable) -> list:\n",
    "    \"\"\"\n",
    "    Evaluate the performance of a network on some metric.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    network : nn.Module\n",
    "        Pytorch module representing the network.\n",
    "    data : DataLoader\n",
    "        Pytorch dataloader that is able to \n",
    "        efficiently sample mini-batches of data.\n",
    "    metric : callable\n",
    "        Function that computes a scalar metric\n",
    "        from the network logits and true data labels.\n",
    "        The function should expect pytorch tensors as inputs.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    errors : list\n",
    "        The computed metric for each mini-batch in `data`.\n",
    "    \"\"\"\n",
    "    #data_loader = get_mnist_loader(data_root, test=True, batch_size=1, num_workers=0, shuffle=True)\n",
    "    network.eval()\n",
    "    errors = torch.tensor([]) # np.array([])\n",
    "    for mini_batch_x, mini_batch_y in data: \n",
    "        mini_batch_x.requires_grad_ = False\n",
    "        logits = network.forward(mini_batch_x)\n",
    "        logits.detach()\n",
    "        error_one_batch = metric(logits, mini_batch_y)\n",
    "        #errors = np.append(errors, error_one_batch)\n",
    "        errors = torch.cat((errors,torch.tensor([error_one_batch])))\n",
    "\n",
    "    #values = np.array([np.sum(errors)/len(errors)]) # we only want one error values in the end\n",
    "\n",
    "    return torch.sum(errors)/errors.size()[0] #values\n",
    "    \n",
    "\n",
    "def update(network: nn.Module, data: DataLoader, loss: nn.Module, \n",
    "           opt: optim.Optimizer) -> list:\n",
    "    \"\"\"\n",
    "    Update the network to minimise some loss using a given optimiser.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    network : nn.Module\n",
    "        Pytorch module representing the network.\n",
    "    data : DataLoader\n",
    "        Pytorch dataloader that is able to \n",
    "        efficiently sample mini-batches of data.\n",
    "    loss : nn.Module\n",
    "        Pytorch function that computes a scalar loss\n",
    "        from the network logits and true data labels.\n",
    "    opt : optim.Optimiser\n",
    "        Pytorch optimiser to use for minimising the objective.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    errors : list\n",
    "        The computed loss for each mini-batch in `data`.\n",
    "    \"\"\"\n",
    "    network.train()\n",
    "    #loss.train()\n",
    "    errors = torch.tensor([]) # np.array([])\n",
    "    \n",
    "    for mini_batch_x, mini_batch_y in data: \n",
    "\n",
    "        logits = network.forward(mini_batch_x)\n",
    "        logits.detach()\n",
    "        opt.zero_grad()\n",
    "        error_one_batch = loss(logits, mini_batch_y)    #.forward(logits, mini_batch_y)\n",
    "        #errors = np.append(errors, error_one_batch)\n",
    "        errors = torch.cat((errors,torch.tensor([error_one_batch])))\n",
    "\n",
    "        error_one_batch.backward()\n",
    "        #dlogits, dtargets = loss.backward(torch.tensor([1]))\n",
    "       # dx = network.backward(dlogits) \n",
    "\n",
    "        opt.step() # no return value\n",
    "\n",
    "    #errors = np.array([np.sum(errors)/len(errors)]) # we only want one error values in the end\n",
    "   \n",
    "    return torch.sum(errors)/errors.size()[0] #errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ViYnGnwQUudn"
   },
   "source": [
    "### Exercise 5: Comparing Architectures (0 points)\n",
    "\n",
    "Time to put everything together and compare the performance of the fully connected network and the convolutional network on a few samples from the MNIST dataset. Which architecture is better? Is this a fair experiment? What conclusions can be drawn?\n",
    "\n",
    "\n",
    "###### a) training\n",
    "\n",
    "> Train a fully connected and convolutional network on the MNIST datasets.\n",
    "> You can use the networks from the first two exercises.\n",
    "> Use the MNIST *test* data (10&nbsp;000 images) for training\n",
    "> and stochastic gradient descent *with momentum* as optimiser.\n",
    "> Train only for a few epochs (maximum 25) to keep running times short.\n",
    "\n",
    "**Hint:** Make sure to use the correct loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g0kSA3XEUudo",
    "outputId": "91f7d018-1b7b-4acb-8d67-36ad59cc33c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[tensor(5012.8125)], [tensor(47.1608)]],\n",
       " [[tensor(2.3057)], [tensor(2.3037)]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%time\n",
    "epochs = 2\n",
    "train_loader = get_mnist_loader(data_root, test=True, batch_size=1, num_workers=0, shuffle=True)\n",
    "optimiser = torch.optim.SGD(conv_net.parameters(), lr=0.1, momentum=0.9)\n",
    "loss = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "fc_net = nn.Sequential(nn.Flatten(), FCNetwork(784, 10))\n",
    "\n",
    "conv_errors = []\n",
    "fc_errors = []\n",
    "\n",
    "for _ in range(epochs):\n",
    "  \n",
    "  # fully connected net:\n",
    "  fc_errs = [update(fc_net, train_loader, loss.eval(), optimiser)]\n",
    "  fc_errors.append(fc_errs)\n",
    "\n",
    "  # convolutional net:\n",
    "  conv_errs = [update(conv_net, train_loader, loss.eval(), optimiser)]\n",
    "  conv_errors.append(conv_errs)\n",
    "\n",
    "conv_errors, fc_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "EXfre8GEUudq",
    "outputId": "4a397267-bc1a-4e79-aa8b-5ee9cf4bec72"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAEICAYAAABLdt/UAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAAjKElEQVR4nO3deXSU933v8fdXuxAgQAsIacYyOwgkjcBrvGBWDd6S25xeO3WW2o2Pc4+bpr0513V870maunHSky5OmzbxTXzsODVZ3CS1kwybDTaxjQPWiN1sNp6RQAjEvmj/3T9m7KtSMAKNNDOPPq9zOEfzzDPPfH+M+PCb3/PMd8w5h4iIeEdGsgsQEZHEUrCLiHiMgl1ExGMU7CIiHqNgFxHxGAW7iIjHKNjFc8zse2b2fxK972XWUGlmzsyyEn1skUsxXccuqcTM9gN/4pxbk+xaBsLMKoH3gGznXHeSy5FhRjN2SSuaAYtcmoJdUoaZPQf4gZfM7LSZ/a8+SxoPmFkEeCW+78/NrMXMTpjZa2ZW1ec4z5jZ4/Gf55tZk5n9TzNrNbODZvbHV7hvkZm9ZGYnzWyjmT1uZr/r59gmmtmLZnbUzPaa2ef73HetmW2KH/eQmf19fHuemf3YzNrM7Hj8OccP6C9ZhgUFu6QM59yngQhwp3NupHPub/vcfSswE1gavx0CpgKlQAPwbx9x6AlAIVAOPAB818zGXsG+3wXOxPf5bPxPf/0EaAImAp8EvmFmC+L3PQk86ZwbDUwGfhbf/tl4LT6gCHgIOHcZzynDlIJd0sXXnHNnnHPnAJxzTzvnTjnnOoCvATVmVniRx3YBX3fOdTnnfgucBqZfzr5mlgn8AfBV59xZ59wO4Nn+FG5mPuBjwCPOuXbnXCPwA+AzfZ5zipkVO+dOO+c29NleBExxzvU45952zp3sz3PK8KZgl3QR/eAHM8s0s2+a2T4zOwnsj99VfJHHtp13AvMsMPIy9y0BsvrWcd7PH2UicNQ5d6rPtveJvSuA2DuDacA78eWWO+LbnwNWAj8xswNm9rdmlt3P55RhTMEuqeZil2n13f4p4G5gEbGlisr4dhu8sjgMdAMVfbb5+vnYA8A4MxvVZ5sfaAZwzu1xzt1LbFnpW8ALZlYQf9fwV865WcCNwB38/1m+yEUp2CXVHAImXWKfUUAH0AaMAL4x2EU553qAXwBfM7MRZjaDfoascy4KvAE8ET8hWk1slv5jADO7z8xKnHO9wPH4w3rN7DYzmxNfBjpJbGmmN6EDE09SsEuqeQL43/GrQL58kX1+RGwpoxnYAWy4yH6J9jCxdwgtxJZJlhP7D6Y/7iX2zuIA8Etia/UfXKtfD2w3s9PETqTeEz+XMAF4gVio7wRejT+vyEfSB5RErpCZfQuY4Jy7nKtjRAadZuwi/WRmM8ys2mKuJbac8stk1yVyPn2KT6T/RhFbfplI7FzA3wH/kdSKRC5ASzEiIh6jpRgREY9JiaWY4uJiV1lZmewyRETSyttvv33EOVdy/vaUCPbKyko2bdqU7DJERNKKmb1/oe1JXYoxszvN7KkTJ04kswwREU9JarA7515yzj1YWHix3k0iInK5dPJURMRjUmKNXUTkSnV1ddHU1ER7e3uySxk0eXl5VFRUkJ3dv+aeSQ12M7sTuHPKlCnJLENE0lhTUxOjRo2isrISs8Fs8Jkczjna2tpoamri6quv7tdjtMYuImmtvb2doqIiT4Y6gJlRVFR0We9ItMYuImnPq6H+gcsdX1oH+8a9rfzirfc4ebYz2aWIiKSMtL6O/a09rXx/1Q4+9Y8v88QvwjTuP4J634jIUGtpaeGee+5h8uTJzJ07l2XLlrF7927MjH/6p3/6cL+HH36YZ555BoDPfe5zlJeX09ERa+l/5MgREvUJ/LReY384OJt/+fzN1Ad8bNzbyiPPvcX9/7KOn76+j2On+/v9ByIiV845xyc+8Qnmz5/Pvn37ePvtt3niiSc4dOgQpaWlPPnkk3R2XnhVITMzk6effjrhNaX1UgzA5AmjeTg4m+f/fBFfvquGcSPzePqVd/ijJ1/m6z9/m417W+np1SxeRAbH2rVryc7O5qGHHvpwW01NDT6fj5KSEhYuXMizzz57wcd+6Utf4h/+4R/o7u6+4P1XyjPXsedlZ7K4poLFNRVEjpxmRTjCmi3NvP5OC6WF+Syt9bG0toKS0fnJLlVEBsm/rtzOu4dOJvSYk8aP5gtLqy56/7Zt25g7d+5F73/kkUcIBoPcf//9/+U+v9/PTTfdxHPPPcedd96ZkHrBQ8Hel794JA8unsXnbpvOm7sOEQpHee7V3fzba7uZN6WUYMDHtVNKycpM+zcsIpLiJk2axHXXXcfzzz9/wfsfffRR7r77bm6//faEPacng/0DOVmZ3Fo1kVurJnLw2FlWhCOs2tzEX+1pZdzIXBbXVBAM+CkbOyLZpYpIAnzUzHqwVFVV8cILL3zkPl/5ylf45Cc/ya233vpf7ps6dSq1tbX87Gc/S1hNaX1VzOUoGzuCP14wgx//2QK++odzmVJWyM/f2Mfn/nktj/x4A+u2H6Czu2fQ6xARb1mwYAEdHR089dRTH27bsmUL0Wj0w9szZsxg1qxZvPTSSxc8xmOPPca3v/3thNWU1lfFXInMjAxunD6Bv77nGp790wV8+tZpHDh6lid+Eea+J1/h+6t3EDlyesjqEZH0Zmb88pe/ZM2aNUyePJmqqioeffRRJkyY8J/2e+yxx2hqarrgMaqqqqirq0tcTalw3fe8efNcMr9oo6fX0fDuYULhKBt2H6Kn11HlG0sw4OfmWWXkZWcmrTYR+Wg7d+5k5syZyS5j0F1onGb2tnNu3vn7enqNvb8yM4xrppRyzZRSjp3uYNXmJlY0Rvj2i5v515XbWTCnnGDAz+QJo5NdqojIJSnYzzN2ZC7//WOT+cMbJ7H5/TZCDVFWhKO8tOl9pk0sJBjwM79qIiNy9VcnIqlJ6XQRZkZtZTG1lcWcPNvJy1ubCYUjPPmbrXx/1Q7mz55IMOBn+sRCzzcgEpH0omDvh9EjcvjEdVfz8Wsr2dl8nFBDhLXbDrAiHOXq0lEE6/wsmF3OqPz+NcEXERlM+qKNy2BmzKoYy6yKsTy0ZBZrtx8g1BDhX1Zs5wdrdnLzzDKCAR+z/eM0ixeRpElqsDvnXgJemjdv3ueTWceVKMjL5o65V3HH3KvYc/AEoXCEtVsP8PLWZiqKCqgP+FhcXcGYgtxklyoiw4w+U58AU8sK+eKyOSz/84X8xZ3VjMrP5gdr3uGP/vFlHn+hgbffPUxvClxWKiKDw8y47777Przd3d1NSUkJd9xxBwDPPPMMDz/88H95XGVlJXPmzKG6upolS5bQ0tKSkHq0xp5AeTlZ8WZjPva3niIUjvDy1mbW7zzIhDH5H95XNCov2aWKSAIVFBSwbds2zp07R35+PqtXr6a8vLxfj127di3FxcV85Stf4Rvf+Abf+c53BlyPZuyDpLJ0FF9YWsXzX1rIIx+vpbQwn2fX7ea+J1/hqz/dFP8gVG+yyxSRBFm2bBm/+c1vAFi+fDn33nvvZT3+lltuYe/evQmpRTP2QZaTlcmCOeUsmFNOc9sZVjRGWbU59gnX4lF5LKmtoL7Wx/gxakQmMmA/+R5E9iX2mP7JcM9Dl9ztnnvu4etf/zp33HEHW7Zs4f7772f9+vX9fppf//rXzJkzZyCVfkjBPoTKiwp4YOEMPjt/Ght2x9oJL1+/l+Xr91I3uYRgwMf108aTrXbCImmnurqa/fv3s3z5cpYtW9bvx912221kZmZSXV3N448/npBaFOxJkJWZwU0zy7hpZhmHjp9lZWMTKzdHefyFBsYU5LC4OtZOuLyoINmliqSXfsysB9Ndd93Fl7/8ZdatW0dbW1u/HvPBGnsiKdiTbPyYEXxm/jT+6JapbNrXSqghyr9veI+fv/ku1VeNIxjwc9PMCeRkqRGZSKq7//77GTNmDHPmzGHdunVJq0PBniIyM4zrpo7nuqnjaTvVHmtEFo7wrV818t0V2SyqjjUiqywdlexSReQiKioq+OIXv3jB+5555hl+9atffXh7w4YNg1aH2vamsF7naHyvjVA4whvvtNDd65hZPoZgnZ9bZ5WRl6P/l0XUtjfF2vamW0uBoZZhRt2kYuomFXP8TAdrtjSzIhzh71/awvdWxhqRLavzM7Vs6L6oRERSn2bsacY5x/boMULhCK/tOEhndy9TJoymPuBnweyJFOSpEZkML5qxp9iMXS6fmTHbP47Z/nF8YWkVr2xtJhSO8s+hbfzfNTu5ZVasEdmsirFqRCbDhnPO07/vlzsBV7CnsZF52dx1TSV3zruK3QdPEGqIsG77AVZvbsJfPJJgnZ9Fc8oZPSIn2aWKDJq8vDza2tooKiryZLg752hrayMvr/+tSLQU4zFnO7p5dccBQg1Rdh04TnZmBh+bMYFgwEd1ZREZHvzFl+Gtq6uLpqYm2tvbk13KoMnLy6OiooLs7P+81HqxpRgFu4e9e+gkoXCEV7Y2c7q9m7KxIwgGfCyuqWDcSDUiE0l3CvZhrKOrh/U7DxIKR9kWOUpmhnH91FKCdX7qJpWQmaFZvEg60snTYSw3O5NF1RUsqq4gcuQ0K8IR1mxp5vVdhygtzGdpTQVLan2UFuYnu1QRSQDN2Iepzu4e3twVa0QWfu8IGQbzJpcQDPi5dmopWWpEJpLytBQjF9Vy7CwrGqOsbIxy9HQH40bmsrgm1k544jg1IhNJVQp2uaSe3l5+v+cwoXCEjXtb6XVQe3URwYCfG6ePVyMykRSjYJfLcvjkOVY1NrGyMcqhE+cYnZ/NopoKgrU+/CVqRCaSChTsckV6eh3h944Qaojw5u5D9PQ6qnxjCQb83DyrjLxszeJFkkXBLgN27HQHq7c0sSIcpfnoGQpys1gwp5xgwMfkCWpEJjLUhizYzWwm8GdAMfCyc+5fL/UYBXt6cc6x5f2jhMIRfrezha6eXqaVFRKs8zO/aiIjcnUVrchQGFCwm9nTwB1Aq3Nudp/t9cCTQCbwA+fcN/vclwH8yDl336WOr2BPXyfPdcYakTVE2X/4FHnZmcyvmkiwzsf0iWM82btDJFUMNNhvAU4TC+rZ8W2ZwG5gMdAEbATudc7tMLO7gC8Azznnnr/U8RXs6c85xzvNxwmFI6zbfpCOrh6uLh1FMOBjwZwKRuWrnbBIog14KcbMKoFf9wn2G4CvOeeWxm8/CuCce6LPY37jnLv9UsdWsHvLmY4u1m07QCgcZc/BE+RkZXDzzFg74dn+cZrFiyTIYLQUKAeifW43AdeZ2XzgvwG5wG8/oqAHgQcB/H7/AMqQVFOQm83tc6/i9rlXsffgiVgjsm0HeHlrMxVFBdQHfCyurmBMQW6ySxXxpIHM2D8J1Dvn/iR++9PAdc65hy+3CM3Yva+9s5vXdh4k1BBlR9MxsjKMG6aPJxjwE5hUrHbCIldgMGbszYCvz+2K+LbLKUrfeTpM5OVksaTGx5IaH/tbT7GiMcqaLU2s39nC+DH51NfG7iserXbCIgM1kBl7FrGTpwuJBfpG4FPOue2XW4Rm7MNTZ3cPr7/TQigcZfP+NjIMrp0Sayd8zZQSMjPUiEzkowxoxm5my4H5QLGZNQFfdc790MweBlYSu9zx6SsJdRm+crIyuW12ObfNLqf56BlWhqOs2tzEhj2bKB6Vx5KaCpYGfEwYMyLZpYqklaR+8rTPUszn9+zZk7Q6JHV09/Ty1p5WQuEIm/YeBqBuUjHBgJ/rp48nW+2ERT6klgKSdlpPnGNlY5QVjVGOnGyncEQOS2oqqA/4qCgamezyRJJOwS5pq6fX8fa+WDvhDbtb6XWO6qvGEQz4uWnmBLUTlmFLwS6e0HaqndWbm1jRGOXgsbOMzMtmUXU5wYCfylK1E5bhJSWDXWvscqV6nWPz/jZCDRHe2HWIrp5eZpaPIVjn59ZZZeTlqBGZeF9KBvsHNGOXgThxtpM1W5oINUSItp1hRE4W82dPJBjwMbWsUC0MxLMU7OJ5zjm2R48RCkdYv+MgHd29TB4/mmCdjwWzyynIUyMy8RYFuwwrp9u7WLst1k5436GT5GZlcMusWDvhWRVjNYsXT0jJYNcauww25xx7Dp4gFI6ydlsz5zp78BePJBjwsai6gtEjcpJdosgVS8lg/4Bm7DIUznV28+r2WDvhd5qPk52ZwcdmTCAY8FFdWaRGZJJ2FOwifbx36CShcJSXtzZxur2bsrEjCAZ8LK6pYNxINSKT9KBgF7mAjq4efrfzIKFwlK2Ro2SYcf20UoIBP3Mnl5CZoVm8pK7BaNsrkvZyszNZWF3BwuoKokdOs6IxyurNTbyx6xAlo/NYWutjaa2P0sL8ZJcq0m86eSpynq6eXt7cdYgV4QgN7x4BYN6UEoIBP9dNLSVLjcgkRWgpRuQKtBw7y8rGKCs3R2k71cG4kbksro41Ips4riDZ5ckwp2AXGYCe3l427j1MqCHC7/e20uugtrKIYMDPjTPGqxGZJIWCXSRBjpxsZ9XmKCvCUQ6dOMfo/GwWVVcQDPjwl6gRmQwdBbtIgvU6R/jdI4TCEd7cdYjuXkeVbyzBgJ+bZ5WRl61ZvAyulAx2nTwVrzh+piPWTjgcpenoGQpys1gwp5xgwMfkCYXJLk88KiWD/QOasYtXOOfYGjlKqCHC+p0tdPX0Mq2skPqAj/mzJ1KQq0ZkkjgKdpEhdvJcJ69sjTUi23/4FHnZmdxaVUYw4GdG+Rg1IpMBU7CLJIlzjl0HjhNqiLJu+wHau3qoLBkVayc8p5zR+WpEJldGwS6SAs50dPHq9oOEGiLsPniC7MwMbp45gWCdnzn+cZrFy2VRsIukmH0tJ+KNyJo529FNxbgC6uONyMYU5Ca7PEkDCnaRFNXe1cP6HQcJhSNsjx4jK8O4Yfp4ggE/gUnFaicsF6UmYCIpKi87k8U1FSyuqeD9w6dYEY6yZksT63e2MH5MPvW1PpbU+CgerXbC0j+6jl0kBXV29/DGO4cIhSM07m8jw+DaKaXUB/xcO7WEzAw1IhMtxYikrQNHz3zYTvjo6Q6KRuWypMZHfa2PCWNHJLs8SSIFu0ia6+7p5fd7WgmFI2zcexiAwKRiggE/N0wfT7baCQ87CnYRD2k9cY5VjVFWNEY5fLKdwhE5LK6poL7Wh694ZLLLkyGiYBfxoJ5eR8O7sXbCG/a00tPrmOMfRzDg46aZZeSqEZmnKdhFPO7o6XZWb24iFI5y8NhZRuZlsXBOrJ3w1eNHJ7s8GQQKdpFhotc5tuxvIxSO8vo7sUZkM8rHEAz4uLVqIvk5usrZKxTsIsPQibOdvLwlNouPHDlNfk4mt82OtROeWlaoFgZpTsEuMow559jRdIxQQ5TXdhygo7uXyeNHE6zzcdvsckbmqZ1wOlKwiwgAp9u7WLvtACvCEfa2nCQ3K4ObZ8XaCVf5xmoWn0ZSMtj1yVOR5Npz8AS/bYiwbtsBznZ24y8eSX3Ax6LqCgpHqJ1wqkvJYP+AZuwiyXWus5vXdsTaCe9sPk52ZgY3Th9PsM5PTWWRGpGlKDUBE5GLys/JYmmtj6W1Pt47dJIVjVHWbGnm1R0HKRs7gvraWDvholFqRJYONGMXkQvq6Orh9XdaCIUjbHn/KBlmXD+tlGDAz9zJJWRmaBafbJqxi8hlyc3OZMGcchbMKaep7TQrwlFWbW7ijV2HKBmd9+EMv7QwP9mlynk0YxeRfuvq6WXDrlg74YZ3jwAwb0oJwYCf66aWkqVGZENKJ09FJKFajp9lZTjKys1R2k51MLYglyU1FSwN+CgfV5Ds8oYFBbuIDIqe3l427j1MKBzl93ta6XWOmsoiggEfH5sxgZwsNSIbLAp2ERl0bafaWRlvJ3zo+DlG5WezqDrWiOyqklHJLs9zFOwiMmR6nSP83hFCDVHe3NVCd69jVsVYgnU+bpk1kTy1E04IBbuIJMXxMx2s3tLEioYoTUfPMCI3iwWzJxIM+JlSVpjs8tKagl1Ekso5x7bIUULhKOt3HqSzu5epZYUEAz7mz55IQa4akV0uBbuIpIxT57p4ZWusnfB7rafIzc5kflWsEdmM8jFqRNZPCnYRSTnOOXYdOEEoHGtE1t7VQ2XJKOoDPhZWlzM6X43IPoqCXURS2tmObtZtP0AoHGH3gRNkZ2Zw08wJBAN+qq8ap1n8BQxZsJvZx4HbgdHAD51zqy71GAW7iPS1r+UkoXCEV7Y2c6ajm/JxBdQHfCypqWBMQW6yy0sZAwp2M3sauANodc7N7rO9HngSyAR+4Jz7Zp/7xgLfds49cKnjK9hF5ELau3pYv+MgoXCE7dFjZGYYN0yLtROum1Q87NsJDzTYbwFOAz/6INjNLBPYDSwGmoCNwL3OuR3x+/8O+DfnXMOljq9gF5FLiRw+RagxyprNTZw818X4wnyW1vpYUltByejh2YhswEsxZlYJ/LpPsN8AfM05tzR++9H4rt+M/1ntnFvzEcd7EHgQwO/3z33//ff7PxoRGbY6u3t4I96IrPG9NjIMrpkSayd87dQSMjOGTyOywWjbWw5E+9xuAq4D/hRYBBSa2RTn3Pcu9GDn3FPAUxCbsQ+gDhEZRnKyMplfNZH5VRM5cPQMKxqjrN7cxFt7NlE0KpclNT7qa31MGDsi2aUmTcL7sTvnvgN8J9HHFRE538RxBdy/YAafuXUav9/TSigc4aev72X57/ZSN6mYYMDPDdPHkz3M2gkPJNibAV+f2xXxbf3W58usB1CGiAx3WZkZ3DhjAjfOmEDriXOs2tzEysYof/PvDRSOyGFxTQX1tT58xSOTXeqQGMgaexaxk6cLiQX6RuBTzrntl1uETp6KSKL19Doa3o21E96w+xA9vY7Z/nEEAz5unllGrgcakQ30qpjlwHygGDgEfNU590MzWwb8I7HLHZ92zv3NlRSnYBeRwXT0dDurNzezojHCgaNnGZmXxYI55QQDfiaNH53s8q5YSn7ytM9SzOf37NmTtDpEZHjodY4t77cRaojy+jstdPX0Mn3iGIJ1PuZXTSQ/J72+Bjolg/0DmrGLyFA7ebaTNVubCTVEiBw5TX5O7GqbYJ2faWWFadHCQMEuInIBzjl2NB0jFI7y2vYDdHT3Mmn8aIIBHwvmlDMyL3XbCadksGspRkRSyZn2Ll7ZdoAV4Qh7W06Sm5XBzbNi7YSrfGNTbhafksH+Ac3YRSTV7DkYaye8dusBznZ24ysqoD7gZ3FNBYUjUqOdsIJdROQKtHd282q8EdnOpuNkZRg3zoi1E669uiipjcgGo6WAiIjn5eVksbTWx9JaH/tbTxEKR1izpZnXdhykbOyIWCOymgqKRuUlu9QPaY1dROQydXb38LudLYTCEba8f5QMM66bWkqwzse8yaVkZgzNLF5LMSIig6C57QyhcITVW5o4fqaT4tF5LK3xsbS2gvFjBrcRmYJdRGQQdfX0smH3IULhKA37DgMwd3IJwYCP66eNJ2sQGpEp2EVEhkjL8bOsamxi5eYoR062M7YgN9aILOCjfFxBwp5HwS4iMsR6eh2b9rUSaojy1p5Wep2jprKIYMDHx2ZMICdrYI3IUjLYdfJURIaLtlPtrNrcxIpwhJbj5xiVn82i6go+fk3lFX8pSEoG+wc0YxeR4aLXORrfayMUjvDGOy08cd91VF9VdEXH0nXsIiIpIMOMuknF1E0q5sTZTkbnJ74XjYJdRCRJBqs1wfD6IkARkWEgqcFuZnea2VMnTpxIZhkiIp6S1GB3zr3knHuwsLAwmWWIiHiKlmJERDxGwS4i4jEKdhERj1Gwi4h4jIJdRMRjFOwiIh6j69hFRDxG17GLiHiMlmJERDxGwS4i4jEKdhERj1Gwi4h4jIJdRMRjFOwiIh6jYBcR8RgFu4iIx+iTpyIiHqNPnoqIeIyWYkREPEbBLiLiMQp2ERGPUbCLiHiMgl1ExGMU7CIiHqNgFxHxGAW7iIjHKNhFRDxGwS4i4jEKdhERj1Gwi4h4jIJdRMRjEh7sZjbJzH5oZi8k+tgiInJp/Qp2M3vazFrNbNt52+vNbJeZ7TWzvwRwzr3rnHtgMIoVEZFL6++M/Rmgvu8GM8sEvgsEgVnAvWY2K6HViYjIZetXsDvnXgOOnrf5WmBvfIbeCfwEuLu/T2xmD5rZJjPbdPjw4X4XLCIiH20ga+zlQLTP7Sag3MyKzOx7QMDMHr3Yg51zTznn5jnn5pWUlAygDBER6Ssr0Qd0zrUBDyX6uCIi0j8DmbE3A74+tyvi2/pNX2YtIpJ4Awn2jcBUM7vazHKAe4AXL+cA+jJrEZHE6+/ljsuBN4HpZtZkZg8457qBh4GVwE7gZ8657YNXqoiI9Ee/1tidc/deZPtvgd9e6ZOb2Z3AnVOmTLnSQ4iIyHmS2lJASzEiIomnXjEiIh6T1GDXVTEiIomnpRgREY/RUoyIiMco2EVEPEZr7CIiHqM1dhERj9FSjIiIxyjYRUQ8RsEuIuIxOnkqIuIxOnkqIuIxWooREfEYBbuIiMco2EVEPEbBLiLiMboqRkTEY3RVjIiIx2gpRkTEYxTsIiIeo2AXEfEYBbuIiMco2EVEPEaXO4qIeIwudxQR8RgtxYiIeIyCXUTEYxTsIiIeo2AXEfGYrGQXMCD/8Ry8tTbZVSSOWbIrkAvx1OviobF45XX59Bdh2uyEHjK9g71oPFw9PdlVJIZzya4ggTw0Fg8NRb9jKSovL+GHTO9gv2lJ7I+IiHxIa+wiIh6jYBcR8Ri1FBAR8Ri1FBAR8RgtxYiIeIyCXUTEYxTsIiIeo2AXEfEYcynwaTQzOwy8f4UPLwaOJLCcdKAxDw8a8/AwkDFf5ZwrOX9jSgT7QJjZJufcvGTXMZQ05uFBYx4eBmPMWooREfEYBbuIiMd4IdifSnYBSaAxDw8a8/CQ8DGn/Rq7iIj8Z16YsYuISB8KdhERj0mbYDezejPbZWZ7zewvL3B/rpn9NH7/W2ZWmYQyE6ofY/4LM9thZlvM7GUzuyoZdSbSpcbcZ78/MDNnZml9aVx/xmtmfxh/nbeb2fNDXWOi9eP32m9ma80sHP/dXpaMOhPJzJ42s1Yz23aR+83MvhP/O9liZnUDekLnXMr/ATKBfcAkIAfYDMw6b5//AXwv/vM9wE+TXfcQjPk2YET85y8MhzHH9xsFvAZsAOYlu+5Bfo2nAmFgbPx2abLrHoIxPwV8If7zLGB/sutOwLhvAeqAbRe5fxkQIvaltNcDbw3k+dJlxn4tsNc5965zrhP4CXD3efvcDTwb//kFYKFZWn/b7SXH7Jxb65w7G7+5AagY4hoTrT+vM8BfA98C2oeyuEHQn/F+Hviuc+4YgHOudYhrTLT+jNkBo+M/FwIHhrC+QeGcew04+hG73A38yMVsAMaYWdmVPl+6BHs5EO1zuym+7YL7OOe6gRNA0ZBUNzj6M+a+HiD2P346u+SY429Rfc653wxlYYOkP6/xNGCamb1uZhvMrH7Iqhsc/Rnz14D7zKwJ+C3wp0NTWlJd7r/3j5TeX2YtAJjZfcA84NZk1zKYzCwD+Hvgc0kuZShlEVuOmU/sHdlrZjbHOXc8mUUNsnuBZ5xzf2dmNwDPmdls51xvsgtLF+kyY28GfH1uV8S3XXAfM8si9haubUiqGxz9GTNmtgh4DLjLOdcxRLUNlkuNeRQwG1hnZvuJrUW+mMYnUPvzGjcBLzrnupxz7wG7iQV9uurPmB8AfgbgnHsTyCPWKMvL+vXvvb/SJdg3AlPN7GozyyF2cvTF8/Z5Efhs/OdPAq+4+FmJNHXJMZtZAPg+sVBP97VXuMSYnXMnnHPFzrlK51wlsfMKdznnNiWn3AHrz+/1r4jN1jGzYmJLM+8OYY2J1p8xR4CFAGY2k1iwHx7SKofei8Bn4lfHXA+ccM4dvOKjJfts8WWcVV5GbLayD3gsvu3rxP5hQ+zF/zmwF/g9MCnZNQ/BmNcAh4DG+J8Xk13zYI/5vH3XkcZXxfTzNTZiy087gK3APcmueQjGPAt4ndgVM43AkmTXnIAxLwcOAl3E3oU9ADwEPNTndf5u/O9k60B/r9VSQETEY9JlKUZERPpJwS4i4jEKdhERj1Gwi4h4jIJdRMRjFOwiIh6jYBcR8Zj/BzrNeR2JVRGVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot learning curves\n",
    "plt.title(\"training loss\")\n",
    "plt.semilogy(conv_errors, 'steelblue', label=\"CNN\")\n",
    "plt.semilogy(fc_errors, 'tomato', label=\"MLP\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IjynKwnUUudr"
   },
   "source": [
    "###### b) evaluation\n",
    "\n",
    "> Compare the performance of both models using the MNIST *train* data (60&nbsp;000 images) as test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jiS8yoqcUudr"
   },
   "outputs": [],
   "source": [
    "train_loader = get_mnist_loader(data_root, test=False, batch_size=1, num_workers=0, shuffle=True)\n",
    "\n",
    "\n",
    "conv_errs_eval = [evaluate(conv_net, train_loader, loss.eval())]\n",
    "\n",
    "fc_errs_eval = [evaluate(fc_net, train_loader, loss.eval())]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
