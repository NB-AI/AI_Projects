{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5VAV_sBeG3c"
   },
   "source": [
    "<!-- Assignment 2 - SS 2023 -->\n",
    "\n",
    "# Vision Networks and Fast Training (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ThIYhZxbeG3f"
   },
   "source": [
    "This notebook contains one of the assignments for the exercises in Deep Learning and Neural Nets 2.\n",
    "It provides a skeleton, i.e. code with gaps, that will be filled out by you in different exercises.\n",
    "All exercise descriptions are visually annotated by a vertical bar on the left and some extra indentation,\n",
    "unless you already messed with your jupyter notebook configuration.\n",
    "Any questions that are not part of the exercise statement do not need to be answered,\n",
    "but should rather be interpreted as triggers to guide your thought process.\n",
    "\n",
    "**Note**: The cells in the introductory part (before the first subtitle)\n",
    "perform all necessary imports and provide utility functions that should work without (too much) problems.\n",
    "Please, do not alter this code or add extra import statements in your submission, unless explicitly allowed!\n",
    "\n",
    "<span style=\"color:#d95c4c\">**IMPORTANT:**</span> Please, change the name of your submission file so that it contains your student ID!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "65W1efTFeG3g"
   },
   "source": [
    "In this assignment, we will take a closer look at some famous vision architectures.\n",
    "Since most of these architectures are very large, it requires high-end hardware to train from scratch.\n",
    "To leverage the limited availability of hardware, also *Transfer Learning* can be used. \n",
    "By using stored weights of a large network a new network can be trained cheaply on new datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5sIPqssleG3i"
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import trange\n",
    "\n",
    "torch.manual_seed(1806)\n",
    "torch.cuda.manual_seed(1806)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9Twmj6OEeG3k",
    "outputId": "2f13c9a8-cdd8-4bac-a6dd-8f6eb57d0e4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~/.pytorch\n"
     ]
    }
   ],
   "source": [
    "# google colab data management\n",
    "import os.path\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    _home = 'gdrive/MyDrive/'\n",
    "except ImportError:\n",
    "    _home = '~'\n",
    "finally:\n",
    "    data_root = os.path.join(_home, '.pytorch')\n",
    "\n",
    "print(data_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tGugWKBieG3m"
   },
   "source": [
    "## LeNet-5 and its Offspring\n",
    "\n",
    "![LeNet-5 architecture](https://miro.medium.com/max/2154/1*1TI1aGBZ4dybR6__DI9dzA.png)\n",
    "\n",
    "The LeNet-5 architecture (depicted above) is one of the first convolutional networks.\n",
    "Since convolutions are extremely well suited for many computer vision tasks,\n",
    "a wide variety of network architectures using convolutional layers has become available.\n",
    "Although the differences in performance are sometimes large,\n",
    "the architectures can generally be considered variations on the same theme."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SYA2hSoveG3n"
   },
   "source": [
    "### Alexnet\n",
    "\n",
    "![alex-net architecture](https://cdn-images-1.medium.com/max/1000/1*wzflNwJw9QkjWWvTosXhNw.png)\n",
    "\n",
    "In 2012 Alex Krizhevsky et al. won the [Imagenet Large Scale Visual Recognition Challenge](http://www.image-net.org/challenges/LSVRC/) (ILSVRC).\n",
    "The network they used, which is known as *Alex-net*, is depicted below and follows the same basic principles as LeNet-5.\n",
    "Alex-net has quite a bit more parameters than LeNet-5, therefore it requires a large amount of computational resources to train.\n",
    "\n",
    "To speed up training time, Alex-net was trained on GPU.\n",
    "Since GPUs have access to little memory compared to CPUs (especially back in the days),\n",
    "alex-net did not fit on a single GPU and required 2 GPUs to train the model,\n",
    "hence the distinction between two paths in the illustration of the network.\n",
    "\n",
    "On modern GPUs, it is no longer a problem to fit alex-net on a single GPU.\n",
    "Due to the fact that deep learning frameworks mostly support hardware acceleration, \n",
    "it has even become extremely easy and almost common to train (large) networks on GPUs.\n",
    "A more detailed description on how to achieve this in pytorch, is given below.\n",
    "\n",
    "Another important add-on, is the use of the dropout regularisation technique in the fully connected layers.\n",
    "From DL & NN 1 you should remember that dropout behaves differently during testing and training.\n",
    "When using Dropout or other modules with different behaviour, e.g. BatchNorm, in pytorch, \n",
    "it is important to make sure that your network operates in the right mode.\n",
    "To do this, the `nn.Module` class provides the `train` and `eval` methods\n",
    "and invokes it on all submodules to assure that the desired behaviour is triggered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HUWl31mneG3p"
   },
   "source": [
    "### Pytorch GPU acceleration\n",
    "\n",
    "In pytorch, training a model on GPU is relatively easy.\n",
    "To copy a tensor `x` from main memory (or wherever it may be) to GPU memory,\n",
    "all we need to do is call `x.to('cuda')` or equivalently `x.cuda()`.\n",
    "When multiple GPUs are available, `x.to('cuda:0')` copies a tensor to the first GPU,\n",
    "`x.to('cuda:1')` to the second, etc.\n",
    "Similarly, to copy a tensor from a GPU (or again wherever it may be) to main memory,\n",
    "`x.to('cpu')` or equivalently `x.cpu()` can be used.\n",
    "\n",
    "Whenever a computation is done on tensors that reside on a specific device,\n",
    "the result will also be on that device.\n",
    "It is not possible, however, to make computations with tensors from different devices.\n",
    "This means that the training of an entire network automatically takes place on e.g. a GPU,\n",
    "as soon as all the variables reside on the same device.\n",
    "When working with neural networks, \n",
    "this is the case if both the network parameters and the data are moved to the same device.\n",
    "\n",
    "To move all parameters of a network to the correct device,\n",
    "The `nn.Module` class provides a convenience `to` method \n",
    "that moves all registered parameters, buffers and submodules to the correct device.\n",
    "\n",
    "As for the data, it is often possible to fit the entire dataset in GPU memory.\n",
    "However, often it does not provide any advantages or it even comes with disadvantages.\n",
    "E.g. the `MNIST` dataset from `torchvision` provides PIL images that can not reside on GPU.\n",
    "If the dataset would be stored on the GPU, the data would have to move to CPU first,\n",
    "where the pre-processing is done on the PIL images, and then move back to the GPU.\n",
    "Therefore, it is considered good practice to keep the dataset in main memory\n",
    "and move the samples to the GPU only when they are needed for computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CcloCH41eG32"
   },
   "source": [
    "### Exercise 1: Hardware Acceleration (3 points)\n",
    "\n",
    "In order to allow our computations to be accelerated,\n",
    "the utility functions `evaluate` and `update` require some minor adjustments.\n",
    "\n",
    " > Alter the `evaluate` and `update` functions from assignment 1\n",
    " > so that it is assured that the inputs are on the same device as the network parameters.\n",
    " > Also put the networks in the right modes so that dropout etc. work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "atZ8PkdMeG33"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(network: nn.Module, data: DataLoader, metric: callable) -> list:\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    cuda_bool = torch.cuda.is_available() # check for cuda\n",
    "    if cuda_bool == True:\n",
    "      device = 'cuda'\n",
    "\n",
    "    else:\n",
    "      device = 'cpu'\n",
    "      print('use cpu as no gpu available')\n",
    "\n",
    "    network.to(device) # new # recursively convert their parameters and buffers to device specific tensors\n",
    "\n",
    "\n",
    "    network.eval() # tells if the network shall do a dropout or not --> .eval()/.train(False) gives dropout=False \n",
    "    errors = torch.tensor([]) \n",
    "    for mini_batch_x, mini_batch_y in data: \n",
    "\n",
    "        mini_batch_x = mini_batch_x.to(device) # new\n",
    "        mini_batch_y = mini_batch_y.to(device) # new\n",
    "        mini_batch_x.requires_grad_ = False\n",
    "\n",
    "        logits = network.forward(mini_batch_x)\n",
    "        logits.detach()\n",
    "        error_one_batch = metric(logits, mini_batch_y)\n",
    "\n",
    "        errors = torch.cat((errors,torch.tensor([error_one_batch])))\n",
    "\n",
    "    return errors\n",
    "\n",
    "    \n",
    "@torch.enable_grad()\n",
    "def update(network: nn.Module, data: DataLoader, loss: nn.Module, \n",
    "           opt: optim.Optimizer) -> list:\n",
    "    # YOUR CODE HERE\n",
    "    # using ideas from: https://pytorch.org/tutorials/beginner/ptcheat.html?ref=blog.hackajob.com\n",
    "    \n",
    "    cuda_bool = torch.cuda.is_available() # check for cuda\n",
    "    if cuda_bool == True:\n",
    "      device = 'cuda'\n",
    "\n",
    "    else:\n",
    "      device = 'cpu'\n",
    "      print('use cpu as no gpu available')\n",
    "\n",
    "    network.to(device) # new # recursively convert their parameters and buffers to device specific tensors\n",
    "\n",
    "    network.train() # tells if the network shall do a dropout or not --> .train() gives dropout=True\n",
    "\n",
    "    errors = torch.tensor([]) \n",
    "    \n",
    "    for mini_batch_x, mini_batch_y in data: \n",
    "\n",
    "        mini_batch_x = mini_batch_x.to(device).requires_grad_(True) # new\n",
    "        mini_batch_y = mini_batch_y.to(device) # new\n",
    "\n",
    "        logits = network.forward(mini_batch_x)\n",
    "        logits.detach()\n",
    "        opt.zero_grad()\n",
    "        error_one_batch = loss(logits, mini_batch_y)   \n",
    "        errors = torch.cat((errors,torch.tensor([error_one_batch])))\n",
    "\n",
    "        error_one_batch.backward()\n",
    "        opt.step() # no return value\n",
    "\n",
    "    return errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "TaQycy6VeG35"
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "uA2Gft0FeG36"
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "OqYkgO0heG36"
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "VVsHx7WOeG37"
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "oDrXaDX_eG37"
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "WLemxya_eG37"
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4XzyzfGeG39"
   },
   "source": [
    "### VGG\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/2628/1*lZTWFT36PXsZZK3HjZ3jFQ.png\" \n",
    "     alt=\"VGG architecture\" style=\"width: 70%; margin: auto\" />\n",
    "\n",
    "The Visual Geometry Group at Oxford University introduced \n",
    "different versions of architectures that are now known as VGG net.\n",
    "11-layer, 16-layer and 19-layer variants exist,\n",
    "all of which use only 3x3 convolutions in the feature extraction part.\n",
    "\n",
    "After winning the Imagenet Large Scale Visual Recognition Challenge (ILSVRC) in 2014,\n",
    "the weights of the winning models were made [available](https://www.robots.ox.ac.uk/~vgg/research/very_deep/).\n",
    "This made it possible for researchers with lower computational budgets\n",
    "to make use of the features the network has extracted for natural images.\n",
    "Since 2021, large pre-trained models often end up serving as [foundation models](https://en.wikipedia.org/wiki/Foundation_models).\n",
    "Note that VGG is a very small model compared to modern \"large\" models today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJ5alOK-eG3-"
   },
   "source": [
    "### Exercise 2: VGG for CIFAR-10 (2 points)\n",
    "\n",
    "Most vision architectures have been trained on the ImageNet dataset, which is hard to come by:\n",
    "it is very large (a few 100GB) and requires registration to get access to the images.\n",
    "[CIFAR-10 and CIFAR-100](https://www.cs.toronto.edu/~kriz/cifar.html)\n",
    "are similar datasets that are much easier to obtain\n",
    "and they are one of the standard datasets in `torchvision.datasets`.\n",
    "In this exercise the goal is to modify a vision network that was trained on ImageNet\n",
    "to make predictions on CIFAR-10 so that we can reuse large parts of the weights.\n",
    "\n",
    " > Create a network with the same feature extraction architecture as\n",
    " > `torchvision.models.VGG` so that it can be used for CIFAR images.\n",
    " > Concretely, the goal is to replace the classifier to predict CIFAR labels\n",
    " > instead of the Imagenet labels.\n",
    " > Use global average pooling to make the classifier independent of the exact image size.\n",
    " > Keep the classifier architecture rectangular, i.e. same width for all layers (except for the classes).\n",
    "\n",
    "**Hint:** Take a look at [`torchvision.models.VGG`](https://pytorch.org/vision/0.13/_modules/torchvision/models/vgg.html) if you need some inspiration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "d28nTRZpeG3_"
   },
   "outputs": [],
   "source": [
    "class CifarVGG(nn.Module):\n",
    "    \"\"\" Variant of the VGG network for classifying CIFAR images. \"\"\"\n",
    "    \n",
    "    def __init__(self, features: nn.Module, num_classes: int = 10):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        features : nn.Module\n",
    "            The convolutional part of the VGG network.\n",
    "        num_classes : int\n",
    "            The number of output classes in the data.\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.size_out = (7,7,512) # that is the output size of the convolutional part according to torchvision.models.vgg (https://pytorch.org/vision/stable/_modules/torchvision/models/vgg.html)\n",
    "        #(1,1) # default values which shouldn't be used and are overwritten in def forward()\n",
    "        \n",
    "\n",
    "        self.features = features\n",
    "\n",
    "        self.GlobalAvgPooling = nn.AvgPool2d(kernel_size=(self.size_out[0],self.size_out[1]))\n",
    "\n",
    "        #dropout = 0.2\n",
    "        self.classifier = nn.Sequential(\n",
    "          # in trochvision.models.VGG we use nn.Linear(512 * 7 * 7, 4096) where (7,7) is the shape of the single 'image' and 512 its number of channels.\n",
    "          # Here through Global average pooling we have 1*1*512:\n",
    "          nn.Linear(512, 1003),\n",
    "\t\t      nn.ReLU(True),\n",
    "          #nn.Dropout(p=dropout),\n",
    "\n",
    "          nn.Linear(1003,1003),\n",
    "          nn.ReLU(True),\n",
    "          #nn.Dropout(p=dropout),\n",
    "          \n",
    "          nn.Linear(1003, self.num_classes),\n",
    "          nn.Softmax()\n",
    "\n",
    "        ) # aim of the classifier is a rectengular architecture, meaning that the number of nodes are the same for each hidden layer.\n",
    "        # this classifier has several hidden layers. At least one hidden layer is in general needed to solve more complex task, e. g. XOR. Also dropout is here implemented\n",
    "        # on a basis of p=0.2. In Exercise 1 the ability for dropout is implemented, therefore, it wasn't let out here.\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        conv_part_out = self.features(x)\n",
    "        self.size_out = (conv_part_out.shape[2], conv_part_out.shape[3])\n",
    "        remaining_pixels = self.GlobalAvgPooling(conv_part_out) # number of channels/kernels should be kept the same --> we don't get a scalar as output.\n",
    "        # the classifier shall be independent of image size, thus we trim its input values to the same size with global average pooling.\n",
    "\n",
    "        remaining_pixels = torch.flatten(remaining_pixels, 1) # tensor = self.Squeeze_Dims(tensor)  # getting vector of length of number of channels\n",
    "\n",
    "        pred = self.classifier(remaining_pixels) \n",
    "\n",
    "        return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "cwsQms0leG3_"
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "lhG9KgRHeG4A"
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Ky4LrVrCeG4A"
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nz_87SHgeG4A"
   },
   "source": [
    "### Exercise 3: Existing Features (2 points)\n",
    "\n",
    "Training a network like VGG (or any of the other networks in this assignment)\n",
    "can take a few hours when training on a GPU.\n",
    "Therefore it is often useful to be able to load pre-trained weights into the network.\n",
    "Also, saving a model that has been trained for hours can often save a lot of time.\n",
    "In pytorch this is possible through what is called \n",
    "[`state_dict`s](https://pytorch.org/tutorials/beginner/saving_loading_models.html).\n",
    "Saving the parameters of a pytorch module can be done with `torch.save(module.state_dict(), path)`,\n",
    "whereas loading saved parameters is done with `module.load_state_dict(torch.load(path))`.\n",
    "\n",
    " > Write a function `vgg_init_` to initialise a `CifarVGG` network.\n",
    " > It should load the pre-trained weights for the **11-layer variant of VGG** from `torchvision.models.vgg`\n",
    " > to initialise the feature extractor of the model\n",
    " > and reasonably initialise the classifier using initialisation functions from `torch.nn.init`.\n",
    "\n",
    "**Hint:** you can use all of the functions available in `torchvision.models.vgg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "a0cSxVsueG4B"
   },
   "outputs": [],
   "source": [
    "def vgg_init_(network: CifarVGG):\n",
    "    \"\"\"\n",
    "    Initialise a CifarVGG network with a pre-trained VGG feature extractor.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    network : CifarVGG\n",
    "        The model to initialise.\n",
    "    \"\"\"\n",
    "    from torchvision.models import vgg\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # how to get weights:\n",
    "    pretrained_weights = vgg.VGG11_Weights.IMAGENET1K_V1 # or weights with batch normalization: VGG11_BN_Weights\n",
    "    \n",
    "    pretrained_weights = vgg.VGG11_Weights.verify(pretrained_weights)\n",
    "    pretrained_weights = pretrained_weights.get_state_dict(progress=True) # gives ordered weight dictionary, loading weights\n",
    "\n",
    "    # remove classifier keys, keep the feature keys/weights (convolutional part):\n",
    "   # remove_list = [ i for i in pretrained_weights.keys() if 'classifier' in i]\n",
    "    #[pretrained_weights.pop(key) for key in remove_list]\n",
    "\n",
    "\n",
    "    # Integrate weights into the given network:\n",
    "    for ind,m in enumerate(network.modules()):\n",
    "        if isinstance(m, nn.Conv2d): # for convolutional part, pretrained\n",
    "\n",
    "              searched_key = list(pretrained_weights.keys())\n",
    "              m.weight = nn.Parameter(pretrained_weights[searched_key[0]])\n",
    "              m.requires_grad_ = False\n",
    "              pretrained_weights.pop(searched_key[0])\n",
    "\n",
    "              if m.bias is not None:\n",
    "\n",
    "                m.bias = nn.Parameter(pretrained_weights[searched_key[1]]) # nn.Parameter should move them automatically to gpu (as I read)\n",
    "                m.requires_grad_ = False\n",
    "                pretrained_weights.pop(searched_key[1])\n",
    "          \n",
    "        elif isinstance(m, nn.Linear): # for classifier, initialize weights\n",
    "              nn.init.normal_(m.weight, 0, 0.01)\n",
    "              nn.init.constant_(m.bias, 0)\n",
    "              m.requires_grad_ = True\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QZCtjN_LeG4B"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg11-8a719046.pth\" to /home/c/.cache/torch/hub/checkpoints/vgg11-8a719046.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efc4e96638434cae882533edb8cbb98f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/507M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sanity check\n",
    "vgg11 = torchvision.models.vgg11() # gives the architecture of the convolutional part embedded in Sequential()\n",
    "network = CifarVGG(vgg11.features, num_classes=10)\n",
    "vgg_init_(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TdaTuc3beG4C"
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m07eGT6OeG4C"
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "egic_AVheG4D"
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UEOH9g1PeG4D"
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F52j8WiveG4D"
   },
   "source": [
    "### Exercise 4: Training (part of) the Network (4 points)\n",
    "\n",
    "Obviously, a classifier for CIFAR 10 will be different from a classifier for Imagenet.\n",
    "With the initialisation above, the `CifarVGG` has a ready-to-go feature extractor,\n",
    "but the classifier part still has to be trained.\n",
    "To do this training efficiently, there are a few things left to do.\n",
    "\n",
    " > The code below should train the entire network on the entire dataset for a few epochs.\n",
    " > Modify the code so that \n",
    " > 1. it only trains the classifier part of the network using SGD\n",
    " > and leaves the convolutional feature extractor untouched, i.e., *frozen*.\n",
    " > 2. the 32x32 CIFAR images are upscaled to 224x224 pixels.\n",
    " > 3. training is done on the GPU, which is generally faster. \n",
    " > 4. it uses a parallel dataloader to make sure that the GPU does not have to wait for data.\n",
    " > 5. only a random subset of 500 images from the CIFAR data is used for training.\n",
    " > 6. a random subset of 500 images from the CIFAR data is used as validation data.\n",
    " > You will also need to include a round of validation in the training loop.\n",
    " > This should give you some confidence that the classifier is learning something useful.\n",
    " \n",
    "**Hint:** you might find useful tools under\n",
    "[`torch.utils.data`](https://pytorch.org/docs/stable/data.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wF9uzLGfeG4E"
   },
   "outputs": [],
   "source": [
    "def get_cifar10(root: str, batch_size: int = 32, resize: tuple[int, int] = (224, 224),\n",
    "                num_train: int = 500, num_valid: int = 500, num_workers: int = 4):\n",
    "    \"\"\"\n",
    "    Get dataloader(s) for CIFAR-10.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    root : str\n",
    "        Path to directory where CIFAR-10 dataset is stored.\n",
    "    batch_size : int, optional\n",
    "        The number of samples per mini-batch.\n",
    "    resize : tuple of int, optional\n",
    "        Desired width and height of loaded images.\n",
    "    num_train : int, optional\n",
    "        Number of (random) samples to use for training.\n",
    "    num_valid : int, optional\n",
    "        Number of (random) samples to use for validation.\n",
    "    num_workers : int, optional\n",
    "        Number of parallel processes to use for loading data.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    train_batches : DataLoader\n",
    "        A dataloader that loads mini-batches of CIFAR-10 for training.\n",
    "    valid_batches : DataLoader\n",
    "        A dataloader that loads (mini-)batches of CIFAR-10 for validation.\n",
    "    \"\"\"\n",
    "    normalise=torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(resize), # 2. resize/upscale images\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261))\n",
    "    ])\n",
    "    cifar10_train = torchvision.datasets.CIFAR10(root, transform=normalise, download=True, train=True) \n",
    "\n",
    "\n",
    "    train_sampler = torch.utils.data.RandomSampler(data_source=cifar10_train, replacement=False, num_samples=num_train, generator=None) # 5. only use 500 shuffled pics for train/validation\n",
    "\n",
    "    train_batches = DataLoader(cifar10_train, batch_size=batch_size, num_workers=num_workers,sampler=train_sampler) # 4. num_workers make data loading parallel\n",
    "    # 5. shuffle=True to randomize the order.\n",
    "\n",
    "\n",
    "    cifar10_val = torchvision.datasets.CIFAR10(root, transform=normalise, download=True, train=False) \n",
    "    val_sampler = torch.utils.data.RandomSampler(data_source=cifar10_val, replacement=False, num_samples=num_valid, generator=None) # 6. only use 500 shuffled pics for train/validation\n",
    "    valid_batches = DataLoader(cifar10_val, batch_size=batch_size, num_workers=num_workers,sampler=val_sampler) # 4. num_workers make data loading parallel\n",
    "\n",
    "    return train_batches, valid_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lP09f27zeG4F",
    "outputId": "72064493-e9a0-4802-b7bf-c0b34cac3159"
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "train_batches, valid_batches = get_cifar10(data_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oEyyeLFXeG4G"
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k9ODwW1seG4H"
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GapT5wwLeG4H"
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oYg0GZwbeG4I"
   },
   "outputs": [],
   "source": [
    "def get_vgg(num_classes: int = 10, device: str = \"cpu\"):\n",
    "    \"\"\"\n",
    "    Create and initialise VGG network on given device.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    num_classes : int, optional\n",
    "        The number of output units for the network.\n",
    "    device : str, optional\n",
    "        A string representing the device to work on\n",
    "    \"\"\"\n",
    "    vgg11 = torchvision.models.vgg11()\n",
    "    net = CifarVGG(vgg11.features, num_classes=10)\n",
    "    vgg_init_(net)\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    net.to(device) # was already ensured in def update()/evaluate() but also made here as demanded\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dvhwPN-SeG4J"
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "network = get_vgg(device='cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GdT9H9DweG4J"
   },
   "outputs": [],
   "source": [
    "from pickle import TRUE\n",
    "class TransferTrainer:\n",
    "    \n",
    "    def __init__(self, model: nn.Module):\n",
    "        \"\"\"\n",
    "        Create a trainer for transfer learning.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        model : nn.Module\n",
    "            The model to train.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.objective = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "        self.optimiser = None\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        # 1.: make gradient computation impossible for network parts for which we already have pretrained weights:\n",
    "        # was already ensured with initializing the network in exercise 3 but also demanded here\n",
    "        for ind,m in enumerate(self.model.modules()):\n",
    "          if isinstance(m, nn.Conv2d): # for convolutional part, pretrained\n",
    "              \n",
    "              m.weight.requires_grad = False\n",
    "\n",
    "              if m.bias is not None:\n",
    "\n",
    "                m.bias.requires_grad = False\n",
    "\n",
    "            \n",
    "          elif isinstance(m, nn.Linear): # 20233003 new\n",
    "              m.weight.requires_grad = True\n",
    "              if m.bias is not None:\n",
    "\n",
    "                m.bias.requires_grad = True\n",
    "\n",
    "        self.optimiser = torch.optim.SGD(self.model.parameters(), lr=1e-3, momentum=0.9)\n",
    "\n",
    "    \n",
    "    def train(self, train_batches: DataLoader, valid_batches: DataLoader, \n",
    "              num_epochs: int = 1):\n",
    "        \"\"\"\n",
    "        Train (part of) the network for a number of epochs.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        train_batches : DataLoader\n",
    "            The training data for updating the network.\n",
    "        valid_batches : DataLoader\n",
    "            The validation data for evaluating the network.\n",
    "        num_epochs : int, optional\n",
    "            The number of iterations over the training data.\n",
    "        \"\"\"\n",
    "        train_errs, valid_errs = [], []\n",
    "        for _ in trange(num_epochs):\n",
    "            local_errs = update(self.model, train_batches, self.objective, self.optimiser)\n",
    "            train_errs.append(sum(local_errs) / len(local_errs) / train_batches.batch_size)\n",
    "            # YOUR CODE HERE\n",
    "\n",
    "            # 6. also make use of validation set every loop:\n",
    "            val_errs = evaluate(self.model, valid_batches, self.objective)\n",
    "            valid_errs.append(sum(val_errs) / len(val_errs) / valid_batches.batch_size)\n",
    "\n",
    "            # 3. training on gpu guaranteed by exercise 1, see update() and evaluate()\n",
    "        \n",
    "        return train_errs, valid_errs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "referenced_widgets": [
      "61ebe1d0be2e46eda35148f9c11925fb",
      "c4fef93f48da41cf85d8a582a8c5e659",
      "5062f6bd4360449ba926bb4e5f4ee306",
      "34a5e1633bbd4a6bb67ad19e8ea17b31",
      "e6406dc1301f494790d1e73025980f28",
      "08c53739e26d4418b0acd8671f3d7479",
      "6ff8f0b34b664042b6382f73bb36e056",
      "ce89c12d283042e8b39d3393fea762d2",
      "cefc8f63eb9442dd87f13aee7922775f",
      "dccb2bee5a984a4094494afa867ef021",
      "7a3d6ecf2f8749b9b5454dbac8cd898d"
     ]
    },
    "id": "YkIoMKHEeG4K",
    "outputId": "e0a12022-3199-425f-baa8-29ac60a4be15"
   },
   "outputs": [],
   "source": [
    "trainer = TransferTrainer(network)\n",
    "train_errs, valid_errs = trainer.train(train_batches, valid_batches, 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kCHHH-zmeG4L"
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M2dgJ0UjeG4L"
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "id": "CSKlN4YdeG4M",
    "outputId": "9c8dc26d-f99c-4561-844f-e7d0df5b1147"
   },
   "outputs": [],
   "source": [
    "\n",
    "# plot learning curves\n",
    "from matplotlib import pyplot as plt\n",
    "plt.plot(range(1, len(train_errs) + 1), train_errs, label=\"train\")\n",
    "plt.plot(range(1, len(valid_errs) + 1), valid_errs, label=\"valid\")\n",
    "plt.legend()\n",
    "print(f\"ran on {next(network.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a-PxhdoSeG4M"
   },
   "source": [
    "## Skip-connections\n",
    "\n",
    "One of the most popular modern network architectures for vision is the residual network.\n",
    "The main feature of this architecture is the so-called skip-connection,\n",
    "which allows to combine the activations with the original inputs in each layer.\n",
    "Since these skip-connections open up a gradient highway,\n",
    "they make it possible to train much deeper networks than is possible without the skip-connections.\n",
    "\n",
    "Mathematically, the simplest form of a skip connection can be written as\n",
    "$$\\boldsymbol{s} = \\boldsymbol{x} + f(\\boldsymbol{x}).$$\n",
    "In order for this to work, the dimensions of $\\boldsymbol{x}$ and $f(\\boldsymbol{x})$ must line up.\n",
    "This means that in this formulation, only square layers,\n",
    "i.e. layers with the same number of inputs and outputs, are possible.\n",
    "\n",
    "In order to use a skip-connection on layers that reduce the dimensionality,\n",
    "a linear transform on $\\boldsymbol{x}$ can be inserted in the equation.\n",
    "Since also other operations are possible, on both inputs and (pre-)activations, \n",
    "we can generalise the skip-connection formula to\n",
    "$$\\boldsymbol{s} = \\boldsymbol{C} \\cdot \\boldsymbol{x} + \\boldsymbol{T} \\cdot f(\\boldsymbol{x}),$$\n",
    "where $\\boldsymbol{C}$ and $\\boldsymbol{T}$ are linear transformations (a.k.a. matrices)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sidQyISCeG4N"
   },
   "source": [
    "### Exercise 5: Pre-Residual Networks (4 points)\n",
    "\n",
    "The original and most commonly used residual networks actually do not implement skip connections as in the formula above.\n",
    "Upon closer inspection (e.g. `torchvision.models.resnet`), it becomes clear that the most famous skip-connection looks more like\n",
    "\n",
    "$$\\boldsymbol{a} = \\phi(\\boldsymbol{x} + f(\\boldsymbol{x})),$$\n",
    "\n",
    "where $\\phi$ is some non-linear activation function.\n",
    "This non-linearity typically interferes with the signal propagation of the network.\n",
    "As a result, gradients might still vanish despite the skip-connection.\n",
    "\n",
    "Pre-Residual Networks aim to counter this problem by moving skip-connections to the level of pre-activations, such that\n",
    "\n",
    "$$\\boldsymbol{a} = \\boldsymbol{x} + f(\\phi(\\boldsymbol{x})).$$\n",
    "\n",
    "This way, clean signal propagation can be guaranteed and learning should become easier.\n",
    "\n",
    " > Implement the `PreResBlock` class so that it can be used as a layer in a pre-residual network.\n",
    " > The residual part of the network, $f$, should be a small network with two convolutional layers.\n",
    " > Both layers should use the given `kernel_size` and preserve the image size if `stride` is one.\n",
    " > If `stride` is greater than one, the network should reduce the spatial dimensions by this factor.\n",
    " > Make sure that the network also works if `in_channels != out_channels` and `stride > 1`.\n",
    " > Try to avoid unnecessary parameters, especially for the skip-connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nz3zzOyReG4N"
   },
   "outputs": [],
   "source": [
    "class PreResBlock(nn.Module):\n",
    "    \"\"\" Residual block using skip-connections on pre-activation level. \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int = 3,\n",
    "                 stride: int = 1, phi: nn.Module = nn.ReLU(), extra_pars: bool = True):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        in_channels : int\n",
    "            Number of input channels.\n",
    "        out_channels : int\n",
    "            Number of output channels.\n",
    "        kernel_size : int\n",
    "            Size of the kernel in all dimensions.\n",
    "        stride : int\n",
    "            Factor by which to reduce the spatial dimensions.\n",
    "        phi : nn.Module\n",
    "            The activation function to use in the residual branch.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        # Used to solve the issue, the formula to compute the output size, solve it by Z the zeropadding. \n",
    "        # init: z=zeropad, a=width or height dimension of the input or outcome, k=kernel size, p=stride.\n",
    "        # For the formula I also considered pooling with the arguments kernel_size=our given kernel_size and stride=kernel_size such that the input gets in size divided by k.\n",
    "        # We end up with two conv layers with two times pooling.\n",
    "        # formula: a_out_of_layer2 = (((a_out_of_layer1)/k + 2*z_layer2 - k)/p + 1)/k\n",
    "        # where a_out_of_layer1 = (a_in - k + 2*z_layer1)/p + 1 \n",
    "        # where a_out_of_layer2 = a_in/stride    .\n",
    "        # Solving this by z_layer2 gives us the formula used in variable 'zlPlusOne_height_formula' below. Search for a z_layer1 and z_layer2 combination which solves this\n",
    "        # formula and for which every z % 0.5 == 0.\n",
    "\n",
    "        self.p = stride\n",
    "        self.k = kernel_size\n",
    "        self.pad_factor_width = 0\n",
    "        self.pad_factor_height = 0\n",
    "        self.pad_factor_width_PlusOne = 0\n",
    "        self.pad_factor_height_PlusOne = 0\n",
    "\n",
    "        self.zl_width = 0\n",
    "        self.zl_height = 0\n",
    "        self.zlPlusOne_width = 0\n",
    "        self.zlPlusOne_height =0\n",
    "\n",
    "        self.padderl1 =  nn.ZeroPad2d((self.zl_width+self.pad_factor_width,self.zl_width,self.zl_height+self.pad_factor_height,self.zl_height))\n",
    "\n",
    "        self.l1 = nn.Sequential(          \n",
    "          nn.Conv2d(in_channels, 1, kernel_size=(kernel_size, kernel_size), stride=(stride, stride), padding=0), # (in_channels, out_channels, ...)\n",
    "          # we can't use the padding argument in Conv2d because it only add rows/columns on all sides EQUALLY! However, it can happen that we want to add\n",
    "          # for example on the left one column more than on the right side of the picture.\n",
    "          nn.ReLU(inplace=True),\n",
    "          nn.MaxPool2d(kernel_size=kernel_size, stride=kernel_size, padding=0, dilation=1, ceil_mode=False)) \n",
    "\n",
    "        self.padderl2 =  nn.ZeroPad2d((self.zlPlusOne_width+self.pad_factor_width_PlusOne,self.zlPlusOne_width,self.zlPlusOne_height+self.pad_factor_height_PlusOne,self.zlPlusOne_height))\n",
    "\n",
    "        self.l2 = nn.Sequential(   \n",
    "          nn.Conv2d(1, out_channels, kernel_size=(kernel_size, kernel_size), stride=(stride, stride), padding=0),\n",
    "          nn.ReLU(inplace=True),\n",
    "          nn.MaxPool2d(kernel_size=kernel_size, stride=kernel_size, padding=0, dilation=1, ceil_mode=False))\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "      # find out with the help of the shape of x how the padding values z look like for width of the input:\n",
    "      a_width = x.shape[2]\n",
    "      a_height = x.shape[3]\n",
    "\n",
    "      a_out_width = a_width/self.p\n",
    "      a_out_height = a_height/self.p\n",
    "\n",
    "      zlPlusOne_width_formula = lambda zl: (-a_width + self.k - 2*zl - self.p + self.k**2*self.p + self.p**2*a_out_width*self.k**2 - self.p**2*self.k) / (2*self.p*self.k)\n",
    "      fixed_width_term = (-a_width + self.k - self.p + self.k**2*self.p + self.p**2*a_out_width*self.k - self.p**2*self.k)\n",
    "\n",
    "      zl_width = 0\n",
    "      saver = []\n",
    "\n",
    "      while True:\n",
    "        zlPlusOne_width = zlPlusOne_width_formula(zl_width)\n",
    "        \n",
    "        if zlPlusOne_width % 0.5 == 0:\n",
    "\n",
    "           if self.p == 1:\n",
    "            if saver == []:\n",
    "              saver.append(zl_width)\n",
    "              saver.append(zlPlusOne_width)\n",
    "\n",
    "            if zl_width == zlPlusOne_width: # when they are the same the image shape stays after each individual layer the same\n",
    "              break\n",
    "\n",
    "           else:\n",
    "             break\n",
    "\n",
    "           zl_width += 0.5\n",
    "\n",
    "        else:\n",
    "            zl_width += 0.5\n",
    "\n",
    "        if 2*zl_width > fixed_width_term:\n",
    "            print('problem in finding padding strategy!')\n",
    "            break\n",
    "\n",
    "      if self.p==1:\n",
    "        if zl_width != zlPlusOne_width:\n",
    "          zl_width = saver[0]\n",
    "          zlPlusOne_width = saver[1]\n",
    "\n",
    "\n",
    "      # same for height:\n",
    "      if a_height != a_width:\n",
    "        zlPlusOne_height_formula = lambda zl: (-a_height + self.k - 2*zl - self.p + self.k**2*self.p + self.p**2*a_out_height*self.k**2 - self.p**2*self.k) / (2*self.p*self.k)\n",
    "        fixed_height_term = (-a_height + self.k - self.p + self.k**2*self.p + self.p**2*a_out_height*self.k - self.p**2*self.k)\n",
    "        \n",
    "        zl_height = 0\n",
    "        saver2 = []\n",
    "\n",
    "        while True:\n",
    "          zlPlusOne_height = zlPlusOne_height_formula(zl_height)\n",
    "          \n",
    "          if zlPlusOne_height % 0.5 == 0:\n",
    "\n",
    "           if self.p == 1:\n",
    "            if saver2 == []:\n",
    "              saver2.append(zl_height)\n",
    "              saver2.append(zlPlusOne_height)\n",
    "\n",
    "            if zl_height == zlPlusOne_height:\n",
    "              break\n",
    "\n",
    "           else:\n",
    "             break\n",
    "          \n",
    "           zl_height += 0.5\n",
    "\n",
    "          else:\n",
    "              zl_height += 0.5\n",
    "\n",
    "          if 2*zl_height > fixed_height_term:\n",
    "              print('problem in finding padding strategy!')\n",
    "              break      \n",
    "              \n",
    "        if self.p==1:\n",
    "          if zl_height != zlPlusOne_height:\n",
    "            zl_height = saver2[0]\n",
    "            zlPlusOne_height = saver2[1]\n",
    "\n",
    "      else: # input image is rectengular. So we can reuse found z-s in width for height.\n",
    "        zl_height = zl_width\n",
    "        zlPlusOne_height = zlPlusOne_width\n",
    "\n",
    "      # --> we end up finding zl and zlplusOne_result\n",
    "\n",
    "      # When found values end on .5, then we have to pad only on one side of the image instead of two sides.\n",
    "      # Explanation: zl gives us the number of rows/columns we want to add on ONE side of the picture. E. g. we have zl=0.5 such that on EACH side there shall\n",
    "      # be 0.5 rows/columns added but that's not possible. Instead add only ONE row/column on only ONE side:\n",
    "      \n",
    "      if int(zl_width) != zl_width: # zl ends on .5\n",
    "        self.pad_factor_width = 1 # the pad_factor will be used to add a row/column only to one side of the 'image' during padding\n",
    "      \n",
    "      self.zl_width = int(zl_width)\n",
    "\n",
    "      if int(zl_height) != zl_height: \n",
    "        self.pad_factor_height = 1\n",
    "      \n",
    "      self.zl_height = int(zl_height)\n",
    "\n",
    "\n",
    "      if int(zlPlusOne_width) != zlPlusOne_width: \n",
    "        self.pad_factor_width_PlusOne = 1 \n",
    "      \n",
    "      self.zlPlusOne_width = int(zlPlusOne_width)\n",
    "\n",
    "\n",
    "      if int(zlPlusOne_height) != zlPlusOne_height: \n",
    "        self.pad_factor_height_PlusOne = 1\n",
    "      \n",
    "      self.zlPlusOne_height = int(zlPlusOne_height)\n",
    "\n",
    "        \n",
    "      self.padderl1.padding =  (self.zl_width+self.pad_factor_width,self.zl_width,self.zl_height+self.pad_factor_height,self.zl_height)\n",
    "      self.padderl2.padding = (self.zlPlusOne_width+self.pad_factor_width_PlusOne,self.zlPlusOne_width,self.zlPlusOne_height+self.pad_factor_height_PlusOne,self.zlPlusOne_height)\n",
    "      \n",
    "      \n",
    "      # Finally, we start processing x through net:\n",
    "      x_pad = self.padderl1(x)\n",
    "      out1 = self.l1(x_pad)\n",
    "      x_pad2 = self.padderl2(out1)\n",
    "      conv_x = self.l2(x_pad2)\n",
    "      \n",
    "      return conv_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VvEYYYXBeG4O",
    "outputId": "7c007d41-14a9-49ff-ef50-d763c7038694"
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "block = PreResBlock(8, 8)\n",
    "out = block(torch.zeros(1, 8, 32, 32))\n",
    "sum(par.numel() for par in block.parameters()), out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fJfS4MKReG4P",
    "outputId": "5c4a9f6b-8890-451c-fffc-5ab44b7cfff3"
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!\n",
    "strided_block = PreResBlock(8, 8, stride=2)\n",
    "out = strided_block(torch.zeros(1, 8, 32, 32))\n",
    "sum(par.numel() for par in strided_block.parameters()), out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yewzNkoLeG4P"
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8f4DQMJBeG4Q"
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_VanRq2xeG4R"
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ya9qbUkReG4R"
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6uarw0VzeG4R"
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2SCWRmiSeG4S"
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9OhUqCQEeG4S"
   },
   "outputs": [],
   "source": [
    "# Test Cell: do not edit or delete!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "08c53739e26d4418b0acd8671f3d7479": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "34a5e1633bbd4a6bb67ad19e8ea17b31": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dccb2bee5a984a4094494afa867ef021",
      "placeholder": "​",
      "style": "IPY_MODEL_7a3d6ecf2f8749b9b5454dbac8cd898d",
      "value": " 20/20 [01:44&lt;00:00,  5.12s/it]"
     }
    },
    "5062f6bd4360449ba926bb4e5f4ee306": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ce89c12d283042e8b39d3393fea762d2",
      "max": 20,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cefc8f63eb9442dd87f13aee7922775f",
      "value": 20
     }
    },
    "61ebe1d0be2e46eda35148f9c11925fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c4fef93f48da41cf85d8a582a8c5e659",
       "IPY_MODEL_5062f6bd4360449ba926bb4e5f4ee306",
       "IPY_MODEL_34a5e1633bbd4a6bb67ad19e8ea17b31"
      ],
      "layout": "IPY_MODEL_e6406dc1301f494790d1e73025980f28"
     }
    },
    "6ff8f0b34b664042b6382f73bb36e056": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7a3d6ecf2f8749b9b5454dbac8cd898d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c4fef93f48da41cf85d8a582a8c5e659": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_08c53739e26d4418b0acd8671f3d7479",
      "placeholder": "​",
      "style": "IPY_MODEL_6ff8f0b34b664042b6382f73bb36e056",
      "value": "100%"
     }
    },
    "ce89c12d283042e8b39d3393fea762d2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cefc8f63eb9442dd87f13aee7922775f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "dccb2bee5a984a4094494afa867ef021": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e6406dc1301f494790d1e73025980f28": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
