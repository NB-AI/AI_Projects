{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Assignment 4 - WS 2023 -->\n",
    "\n",
    "# Adaptive Optimisation (22 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the fourth assignment for the exercises in Deep Learning and Neural Nets 1.\n",
    "It provides a skeleton, i.e. code with gaps, that will be filled out by you in different exercises.\n",
    "All exercise descriptions are visually annotated by a vertical bar on the left and some extra indentation,\n",
    "unless you already messed with your jupyter notebook configuration.\n",
    "Any questions that are not part of the exercise statement do not need to be answered,\n",
    "but should rather be interpreted as triggers to guide your thought process.\n",
    "\n",
    "**Note**: The cells in the introductory part (before the first subtitle)\n",
    "perform all necessary imports and provide utility function that should work without problems.\n",
    "Please, do not alter this code or add extra import statements in your submission, unless it is explicitly requested!\n",
    "\n",
    "<span style=\"color:#d95c4c\">**IMPORTANT:**</span> Please, change the name of your submission file so that it contains your student ID!\n",
    "\n",
    "In this assignment, the goal is to get familiar with some of the most common (adaptive) **optimisation algorithms**. Essentially, the most common optimisation algorithms are nothing more than variants of gradient descent. Although it is often claimed that stochastic gradient descent outperforms any adaptive learning method when carefully configured, it is often more convenient to use a method that requires less tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from nnumpy import Module\n",
    "from nnumpy import Optimiser\n",
    "from nnumpy.data import get_mnist_data\n",
    "from nnumpy.utils import split_data, to_one_hot\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(logits, labels):\n",
    "    \"\"\"\n",
    "    Compute accuracy for classification network.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    logits : ndarray\n",
    "        The logit-predictions from the network.\n",
    "    labels : ndarray\n",
    "        The target labels for the task.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    acc : float\n",
    "        The fraction of correctly classified samples.\n",
    "    \"\"\"\n",
    "    idx = np.argmax(logits, axis=1)\n",
    "    return np.mean(labels[np.arange(len(idx)), idx])\n",
    "\n",
    "    \n",
    "def plot_curves(model, train_errors, valid_errors):\n",
    "    \"\"\"\n",
    "    Plot learning curves\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : Module\n",
    "        The network to plot learning curves for.\n",
    "    train_errors : ndarray\n",
    "        The training errors for each batch in every epoch.\n",
    "    valid_errors : ndarray\n",
    "        The validation errors after each epoch.\n",
    "    \"\"\"\n",
    "    plt.title(\"learning curves\")\n",
    "    train_acc, = evaluate(model, accuracy, Dataloader(*train_data))\n",
    "    loss_curve, = plt.semilogy(np.mean(train_errors, axis=1), \n",
    "                           label=f'train (acc: {100 * train_acc:2.2f}%)')\n",
    "    valid_acc, = evaluate(model, accuracy, Dataloader(*valid_data))\n",
    "    plt.semilogy(valid_errors, linestyle='--', color=loss_curve.get_color(), \n",
    "             label=f'valid (acc: {100 * valid_acc:2.2f}%)')\n",
    "    plt.legend()\n",
    "    return plt.gcf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you should know by now, the *backpropagation* algorithm is little more than a combination of the chain rule and some form of gradient descent. Although this happens to work well in practice, it is good to be aware of possible issues when using first-order optimisation methods:\n",
    "\n",
    " 1. Gradient descent can/will get stuck in *local minima*.\n",
    " 2. The *gradient magnitude* tells you nothing about how far away minima are.\n",
    " 3. When optimising the *empirical error*, gradient descent would require the gradient over the entire dataset.\n",
    "\n",
    "Note that this last point is not necessarily an issue, but it is useful to keep in mind. Also, it implies that the gradients that can be computed on the entire dataset do not need to correspond to the gradients that would be required to minimise the generalisation error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Stochasticity (3 Points)\n",
    "\n",
    "Rather than using plain gradient descent algorithm, a stochastic variant is used to train neural networks. This variant is known as *Stochastic Gradient Descent*, or *SGD* for short. Although this naming scheme seems to suggest that stochasticity is part of the algorithm, it is actually introduced by how we use the data to compute gradients.\n",
    "\n",
    "Instead of computing the gradients over the entire dataset in one go, the samples in the dataset are split up in more manageable pieces called *mini-batches*. This can speed up the computations significantly and avoids memory problems with very large datasets. Another benefit from mini-batches is that they introduce variation, or *stochasticity*, in the gradient computations. After all, the gradient for each mini-batch will be different to the gradient for other mini-batches or for all samples. This stochasticity can be useful to escape local minima in the optimisation process. To amplify this stochasticity, it is also common to shuffle the samples in the dataset so that mini-batches consist of different samples.\n",
    "\n",
    "> Complete the `Dataloader` class below to process the data in mini-batches of pre-specified size. Also make sure to shuffle the data to get more stochasticity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Some Notes on python generators\n",
    "\n",
    "In python, a [generator](https://wiki.python.org/moin/Generators) is a function with some state that can return multiple values. You probably have already used generators without realising it. Probably, the most famous generator is `range`, which could be defined as follows:\n",
    "```python\n",
    "def _range(start, stop, step=1):\n",
    "    i = start\n",
    "    while i < stop:\n",
    "        yield i\n",
    "        i += step\n",
    "```\n",
    "\n",
    "Notice the `yield` keyword. This has a similar effect as `return` in that it provides a value to the outer scope of the function. However, it does not cause the function to be exited. Instead, the current state in the function is stored until the next value is requested. To get the return values of a generator, there are essentially two options:\n",
    " 1. Using the `next` function. This will simply run the function until the next `yield` statement and give back the yielded value.\n",
    " 2. By iterating over the generator in any way. This will consequently call `next` on the generator until the function exits.\n",
    " \n",
    "For more information, please refer to the internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataloader:\n",
    "    \n",
    "    def __init__(self, x, y, batch_size=None, shuffle=False, seed=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : (N, ...) ndarray\n",
    "            the `N` input samples in the dataset\n",
    "        y : (N, ...) ndarray\n",
    "            the `N` output samples in the dataset\n",
    "        batch_size : int, optional\n",
    "            number of samples to include in a single mini-batch.\n",
    "        shuffle : bool, optional\n",
    "            whether or not the data should be shuffled.\n",
    "        seed : int, optional\n",
    "            seed for the pseudo random number generator used for shuffling.\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.batch_size = len(x) if batch_size is None else int(batch_size)\n",
    "        self.shuffle = shuffle\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Iterates over the samples of the data.\n",
    "        \n",
    "        Yields\n",
    "        ------\n",
    "        x : ndarray\n",
    "            input features for the batch\n",
    "        y : ndarray\n",
    "            target values for the batch\n",
    "            \n",
    "        Notes\n",
    "        -----\n",
    "        Each batch should contain the specified number of samples,\n",
    "        except for the last batch if the batch_size does \n",
    "        not divide the number of samples in the data.\n",
    "        \"\"\"\n",
    "        \n",
    "        # shuffle data:\n",
    "        number_samples = self.x.shape[0]\n",
    "\n",
    "        if self.shuffle:\n",
    "            shuffler = np.random.permutation(number_samples)\n",
    "            x = np.copy(self.x)[shuffler]\n",
    "            y = np.copy(self.y)[shuffler]\n",
    "        else:\n",
    "            x = np.copy(self.x)\n",
    "            y = np.copy(self.y)\n",
    "        \n",
    "        # create mini-batches:\n",
    "        if number_samples % self.batch_size == 0: # number batches fit into the number of samples:\n",
    "            number_batches = int(number_samples/self.batch_size)\n",
    "        else:\n",
    "            number_batches = int(number_samples/self.batch_size)+1 \n",
    "            # +1 because incomplete last batches are included\n",
    "\n",
    "        for single_batch_ind in range(1,number_batches+1): \n",
    "            # +1 because else number_batches value not included in for-loop;\n",
    "            # we don't need the looping variable single_batch_ind but the loop as counter how many \n",
    "            # mini-batches we can build\n",
    "\n",
    "            if single_batch_ind == number_batches: # we reached the end of our last mini-batch\n",
    "                minibatch_x, minibatch_y = x, y\n",
    "\n",
    "            else:\n",
    "                minibatch_x, minibatch_y = x[:self.batch_size], y[:self.batch_size]\n",
    "                x, y = x[self.batch_size:], y[self.batch_size:]\n",
    "\n",
    "            yield minibatch_x, minibatch_y # gives us a tuple     \n",
    "            \n",
    "            \n",
    "                    \n",
    "    def __len__(self):\n",
    "        return -(-len(self.x) // self.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "x = np.arange(7)\n",
    "data_loader = Dataloader(x, np.copy(x), batch_size=3, shuffle=True, seed=17)\n",
    "print(len(data_loader))\n",
    "for x, y in data_loader:\n",
    "    print(x.shape, y.shape)\n",
    "    print(x)\n",
    "\n",
    "print(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Gradient Descent with Momentum (3 Points)\n",
    "\n",
    "Another way to keep gradient descent from getting stuck in local minima is to use momentum. Momentum accumulates the gradient directions over different batches and accelerates/decelarates the descent when all gradients point in the same/different direction(s). This also means that the update does not directly use the magnitude of the gradient, but instead focuses on the direction. \n",
    "\n",
    "> Implement the `get_direction` and `init_state` methods for the gradient descent optimiser with momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescent(Optimiser):\n",
    "    \"\"\" NNumpy implementation of gradient descent. \"\"\"\n",
    "\n",
    "    def __init__(self, parameters, lr: float, momentum: float = 0.):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        momentum : float\n",
    "            Momentum term for the gradient descent.\n",
    "        \"\"\"\n",
    "        super().__init__(parameters, lr)\n",
    "        self.mu = momentum\n",
    "        \n",
    "        \n",
    "\n",
    "    def init_state(self, par):\n",
    "        \"\"\"\n",
    "        Create the initial optimiser state for a parameter.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        par : Parameter\n",
    "            The parameter to create the initial state for.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        state : object or tuple of objects\n",
    "            The initial optimiser state for the given parameter.\n",
    "        \"\"\"\n",
    "\n",
    "        # initalize with 0:\n",
    "        return np.zeros_like(par.grad)\n",
    "        \n",
    "    def get_direction(self, grad, state):\n",
    "        \"\"\"\n",
    "        Compute the update direction from gradient and state for single parameter.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        grad : ndarray\n",
    "            Gradient for the parameter to update.\n",
    "        state : object or tuple of objects\n",
    "            State information that is necessary to compute the update direction.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        direction : ndarray\n",
    "            The update direction according to the algorithm.\n",
    "        new_state: object or tuple of objects\n",
    "            Updated state information after computing the update direction.\n",
    "        \"\"\"\n",
    "        # nnumpy -> __init__.py --> class Optimiser --> def step:\n",
    "        # updates w with w = w - lr delta_w, where delta_w is the FIRST return-\n",
    "        # value from get_direction(). Therefore, delta_w = direction = m\n",
    "        # = mu * m + (1 - mu) * grad.\n",
    "        # This forms the solution here:\n",
    "        direction = self.mu * state + (1 - self.mu) * grad\n",
    "        \n",
    "        # 'The optimizer state is the optimizer's momentum vector or similar history-tracking properties.'\n",
    "        # (https://stats.stackexchange.com/questions/444328/what-does-it-mean-to-save-optimizer-states-in-deep-learning-libraries):\n",
    "        new_state = direction # maybe time lr\n",
    "        \n",
    "        return direction, new_state \n",
    "        \n",
    " \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: The Adamax Optimiser (5 Points)\n",
    "\n",
    "Momentum already provides a way to reduce the importance of the gradient magnitude. \n",
    "With adaptive learning rate methods, an attempt is made to ignore most of the magnitude information and the size of the update is mainly controlled by controlling the learning rate. \n",
    "One of the most popular first order adaptive methods in practice, is the Adam optimiser.\n",
    "However, Adam also has a sibling called *Adamax* that typically performs on a similar level.\n",
    "\n",
    "To understand Adamax, we have to look at the variance as a rescaled $L_2$ norm.\n",
    "Instead of dividing by the $L_2$ norm, as it is done in Adam, Adamax divides by the $L_\\inf$ norm.\n",
    "This gives rise to the following updates for first and second moment estimates:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\boldsymbol{m}^{(t)} &= \\beta_1 \\boldsymbol{m}^{(t-1)} + (1 - \\beta_1) \\boldsymbol{g}^{(t)} \\\\\n",
    "    \\boldsymbol{u}^{(t)} &= \\max\\Bigl(\\beta_2 \\boldsymbol{u}^{(t-1)}, \\bigl|\\boldsymbol{g}^{(t)}\\bigr|\\Bigr),\n",
    "\\end{aligned}$$\n",
    "\n",
    "giving rise to the update:\n",
    "\n",
    "$$\\boldsymbol{w}^{(t+1)} = \\boldsymbol{w}^{(t)} - \\eta \\frac{1}{\\boldsymbol{u}^{(t)} + \\epsilon} \\odot \\boldsymbol{m}^{(t)}.$$\n",
    "\n",
    "> Implement the `get_direction` and `init_state` methods for the Adamax optimisation algorithm.\n",
    "> Also, make sure to implement a bias correction for the exponential moving averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adamax(Optimiser):\n",
    "    \"\"\" NNumpy implementation of the Adam algorithm. \"\"\"\n",
    "\n",
    "    def __init__(self, parameters, lr: float = 1e-3, betas: tuple = (.9, .999),\n",
    "                 epsilon: float = 1e-7, bias_correction=True):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        betas : tuple of 2 floats, optional\n",
    "            Decay factors for the exponential averaging of mean, resp. variance.\n",
    "        epsilon : float, optional\n",
    "            Small number that is added to denominator for numerical stability.\n",
    "        bias_correction : bool, optional\n",
    "            Whether or not mean and bias estimates should be bias-corrected.\n",
    "        \"\"\"\n",
    "        super().__init__(parameters, lr)\n",
    "\n",
    "        beta1, beta2 = betas\n",
    "        self.beta1 = float(beta1)\n",
    "        self.beta2 = float(beta2)\n",
    "        self.eps = float(epsilon)\n",
    "        self.bias_correction = bias_correction\n",
    "\n",
    "    def init_state(self, par):\n",
    "        \"\"\"\n",
    "        Create the initial optimiser state for a parameter.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        par : Parameter\n",
    "            The parameter to create the initial state for.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        state : object or tuple of objects\n",
    "            The initial optimiser state for the given parameter.\n",
    "        \"\"\"\n",
    "        time = 1\n",
    "        m, u = np.zeros_like(par.grad), 0#np.zeros_like(par.grad)\n",
    "        state = (m, u, time)\n",
    "        return  state\n",
    "\n",
    "    def get_direction(self, grad, state):\n",
    "        \"\"\"\n",
    "        Compute the update direction from gradient and state for single parameter.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        grad : ndarray\n",
    "            Gradient for the parameter to update.\n",
    "        state : object or tuple of objects\n",
    "            State information that is necessary to compute the update direction.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        direction : ndarray\n",
    "            The update direction according to the algorithm.\n",
    "        new_state: object or tuple of objects\n",
    "            Updated state information after computing the update direction.\n",
    "        \"\"\"\n",
    "        m_old, u_old, time = state\n",
    "        \n",
    "        m_first_momentum = self.beta1 * m_old + (1 - self.beta1) * grad\n",
    "        \n",
    "        u_second_momentum_norm = max(np.max(self.beta2 * u_old), np.max(abs(grad)))\n",
    "        # 'L-infinity norm: Gives the largest magnitude among each element of a vector.'\n",
    "        # https://montjoile.medium.com/l0-norm-l1-norm-l2-norm-l-infinity-norm-7a7d18a4f40c\n",
    "        \n",
    "        new_state = m_first_momentum, u_second_momentum_norm, time + 1\n",
    "        # save state before bias correction like in\n",
    "        # https://stats.stackexchange.com/questions/232741/why-is-it-important-to-include-a-bias-correction-term-for-the-adam-optimizer-for\n",
    "\n",
    "\n",
    "        # knowledge from: \n",
    "        # https://stats.stackexchange.com/questions/232741/why-is-it-important-to-include-a-bias-correction-term-for-the-adam-optimizer-for\n",
    "        if self.bias_correction:\n",
    "            m_first_momentum /= (1 - self.beta1 ** time)        \n",
    "            # 'As u_t relies on the max operation, bias correction is not done for this term alone.'\n",
    "            # (https://blog.allmattersai.com/adamax/)\n",
    "\n",
    "            \n",
    "        direction = m_first_momentum / (u_second_momentum_norm + self.eps)\n",
    "           \n",
    "        return direction, new_state \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, you should have all components that are necessary to start training neural networks.\n",
    "With the modules you created throughout this semester, you can already build a wide variety of network architectures.\n",
    "Together with the optimisers from this assignment, you can start training networks in your very own DL framework!\n",
    "As a gift, I provide you with an extra module that is often useful when working with high-dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the modules you have written in previous assignments,\n",
    "# or alternatively, the code for these modules, in this box.\n",
    "# e.g. from nnumpy import Sequential, Linear, Conv2d, MaxPool2d, Tanh, LogitCrossEntropy\n",
    "from nnumpy import Container, LossFunction\n",
    "from nnumpy.utils import to_one_hot, sig2col\n",
    "\n",
    "# The error function:\n",
    "class LogitCrossEntropy(LossFunction): # from assignment 2 - MLP\n",
    "    \"\"\"\n",
    "    NNumpy implementation of the cross entropy loss function\n",
    "    computed from the logits, i.e. before applying the softmax nonlinearity.\n",
    "    \"\"\"\n",
    "\n",
    "    def raw_outputs(self, logits, targets):\n",
    "\n",
    "        \"\"\"\n",
    "        Computation of loss without reduction.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        logits : (N, K) ndarray\n",
    "        targets : (N, K) ndarray\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        cross_entropy : (N, 1) ndarray\n",
    "        cache : ndarray or iterable of ndarrays\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        logits = logits - np.max(logits, axis=1,keepdims=True)\n",
    "        softmaxed = np.exp(logits)/np.repeat(np.sum(np.exp(logits),axis=1).reshape(-1,1),logits.shape[1],axis=1)      \n",
    "        cross_entropy = - np.sum(targets*np.log((softmaxed)),axis=1).reshape(-1,1)\n",
    "        cache = targets,softmaxed \n",
    "        \n",
    "        \n",
    "        return cross_entropy, cache\n",
    "    \n",
    "\n",
    "\n",
    "    def raw_grads(self, grads, cache):\n",
    "        \"\"\"\n",
    "        Computation of gradients for loss without reduction.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        grads : (N, 1) ndarray\n",
    "        cache : ndarray or iterable of ndarrays\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dlogits : (N, K) ndarray\n",
    "        dtargets : (N, K) ndarray\n",
    "        \"\"\"\n",
    "        \n",
    "        self.zero_grad()\n",
    "        #targets,output,logits,log_softmaxed = cache\n",
    "        targets,output=cache\n",
    "        # extending the tutors stable solution from raw_outputs():\n",
    "        \n",
    "        dLdlogits = output - targets # = a(s) - y\n",
    "        dlogits = grads * dLdlogits\n",
    "        \n",
    "        dLdy = np.log(1/output)\n",
    "        dtargets = grads * dLdy\n",
    "        \n",
    "        return dlogits, dtargets\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assignment 2 - MLP:\n",
    "# linear layer:\n",
    "class Linear(Module):\n",
    "\n",
    "    \"\"\"\n",
    "    NNumpy implementation of a fully connected layer.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    in_features : int\n",
    "        Number of input features (D) this layer expects.\n",
    "    out_features : int\n",
    "        Number of output features (K) this layer expects.\n",
    "    use_bias : bool\n",
    "        Flag to indicate whether the bias parameters are used.\n",
    "\n",
    "    w : Parameter\n",
    "        Weight matrix.\n",
    "    b : Parameter\n",
    "        Bias vector.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> fc = Linear(10, 1)\n",
    "    >>> fc.reset_parameters()  # init parameters\n",
    "    >>> s = fc.forward(np.random.randn(1, 10))\n",
    "    >>> fc.zero_grad()  # init parameter gradients\n",
    "    >>> ds = fc.backward(np.ones_like(s))\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features, use_bias=True):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.use_bias = use_bias\n",
    "        \n",
    "        # register parameters 'w' and 'b' here (mind use_bias!)\n",
    "        \n",
    "        # only considering shape becsaue below with self.reset_parameters() random values initialized\n",
    "        self.w = self.register_parameter('w', np.empty((self.out_features,self.in_features)))\n",
    "        if use_bias == True:\n",
    "            self.b = self.register_parameter('b',np.empty((self.out_features,)))\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "        \n",
    "        \n",
    "    def reset_parameters(self, seed: int = None):\n",
    "        \"\"\" \n",
    "        Reset the parameters to some random values.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        seed : int, optional\n",
    "            Seed for random initialisation.\n",
    "        \"\"\"\n",
    "        rng = np.random.default_rng(seed)\n",
    "        self.w = rng.standard_normal(size=self.w.shape)\n",
    "        if self.use_bias:\n",
    "            self.b = np.zeros_like(self.b)\n",
    "    \n",
    "    def compute_outputs(self, x):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : (N, D) ndarray\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        s : (N, K) ndarray\n",
    "        cache : ndarray or iterable of ndarrays\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        if self.use_bias==True:\n",
    "            s = x @ self.w.T + self.b\n",
    "        else:\n",
    "            s = x @ self.w.T\n",
    "        cache = x\n",
    "\n",
    "        return s, cache \n",
    "        \n",
    "    \n",
    "    def compute_grads(self, grads, cache):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        grads : (N, K) ndarray\n",
    "        cache : ndarray or iterable of ndarrays\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dx : (N, D) ndarray\n",
    "        \"\"\"\n",
    "        # grads = dL/ds\n",
    "        # dL/dw = dL/ds * ds/dw = grads * (xw.T+b)/dw\n",
    "        self.zero_grad()\n",
    "\n",
    "        x = cache\n",
    "        \n",
    "\n",
    "        if self.use_bias == True:\n",
    "            b_grad = np.ones((grads.shape))\n",
    "            self.b.grad = np.diag(grads.T @ b_grad)\n",
    "  \n",
    "        self.w.grad = grads.T @ x\n",
    "\n",
    "       \n",
    "        dx = grads @ self.w \n",
    "\n",
    "        return dx # = dL/dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assignment 2:\n",
    "# connect single layers in general:\n",
    "class Sequential(Container):\n",
    "    \"\"\"\n",
    "    NNumpy module that chains together multiple one-to-one sub-modules.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    Doubling a module could be done as follows:\n",
    "    >>> module = Module()\n",
    "    >>> seq = Sequential(module, module)\n",
    "    \n",
    "    Modules can be accessed by index or by iteration:\n",
    "    >>> assert module is seq[0] and module is seq[1]\n",
    "    >>> mod1, mod2 = (m for m in seq)\n",
    "    >>> assert mod1 is module and mod2 is module\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *modules):\n",
    "        super().__init__()\n",
    "        if len(modules) == 1 and hasattr(modules[0], '__iter__'):\n",
    "            modules = modules[0]\n",
    "        \n",
    "        for i, mod in enumerate(modules):\n",
    "            self.add_module(mod)\n",
    "\n",
    "    def compute_outputs(self, x):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : (N, D) ndarray\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y : (N, K) ndarray\n",
    "        cache : ndarray or iterable of ndarrays\n",
    "        \"\"\"\n",
    "\n",
    "    \n",
    "        result = self._modules[0](x)\n",
    "        result_list = [x,result]\n",
    "\n",
    "        \n",
    "        for single in self._modules[1:]:\n",
    "\n",
    "            result = single(result)\n",
    "            result_list.append(result)\n",
    "            \n",
    "        y = result\n",
    "        cache = result_list\n",
    "\n",
    "        return y, cache\n",
    "\n",
    "    \n",
    "    def compute_grads(self, grads, cache):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        grads : (N, K) ndarray\n",
    "        cache : ndarray or iterable of ndarrays\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dx : (N, D) ndarray\n",
    "        \"\"\"\n",
    "        \n",
    "        # go backward through all modules (linear layers; activation functions) by computing the gradient\n",
    "        # starting with dL/da as given grads as input of gradient computation of last layer activation\n",
    "        # function. Then reuse the grads_new = dL/ds = dL/da * da/ds where a=a(s)=activation_func(s) \n",
    "        # to put it into grad computation linear layer such that grads_new_new_w = dL/dw =\n",
    "        # = dL/ds * ds/dw = dL/da * da/ds  *    d(x@w.T+b)/dw \n",
    "        # For dx and db replace simply the 'dw' by\n",
    "        # 'dx','db'.\n",
    "\n",
    "        result_list = cache\n",
    "        result_list.reverse() # [prediction, .... , x]\n",
    "            \n",
    "        list_mod = self._modules.copy()\n",
    "        list_mod.reverse()\n",
    "        \n",
    "        self.zero_grad() # new\n",
    "\n",
    "        for ind,single_after in enumerate(list_mod):\n",
    "\n",
    "            # new - but indeed the same as old version:\n",
    "            net_x = result_list[ind+1] # output_before\n",
    "            s,cache = single_after.compute_outputs(net_x)\n",
    "\n",
    "            grads = single_after.compute_grads(grads, cache)\n",
    "            \n",
    "          \n",
    "        dx = grads\n",
    "                \n",
    "        \n",
    "        return dx\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlgebraicSigmoid(Module): # assignment 2\n",
    "    \"\"\" \n",
    "    NNumpy implementation of an algebraic sigmoid function.\n",
    "    \"\"\"\n",
    "\n",
    "    def compute_outputs(self, s):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        s : (N, K) ndarray\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        a : (N, K) ndarray\n",
    "        cache : ndarray or iterable of ndarrays\n",
    "        \"\"\"\n",
    "        \n",
    "        a = s/np.sqrt(1+(s**2))\n",
    "        cache = s\n",
    "        \n",
    "        return a, cache\n",
    "        \n",
    "\n",
    "    def compute_grads(self, grads, cache):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        grads : (N, K) ndarray\n",
    "        cache : ndarray or iterable of ndarrays\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ds : (N, K) ndarray\n",
    "        \"\"\"\n",
    "        \n",
    "        dads =  1/((cache**2+1)**1.5)\n",
    "        ds = grads * dads\n",
    "        return ds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assignment 3 - CNN - contents:\n",
    "\n",
    "# all needed stuff already imported\n",
    "\n",
    "# Convolution operation:\n",
    "def multi_channel_convolution2d(x, k):\n",
    "    \"\"\"\n",
    "    Compute the multi-channel convolution of multiple samples.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : (N, Ci, A, B)\n",
    "    k : (Co, Ci, R1, R2)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    y : (N, Co, A', B')\n",
    "    \n",
    "    See Also\n",
    "    --------\n",
    "    sig2col : can be used to convert (N, Ci, A, B) ndarray \n",
    "              to (N, Ci, A', B', R1, R2) ndarray.\n",
    "    \"\"\"\n",
    "    \n",
    "    # aim: bring x into the form as it is described in the sum above: \n",
    "    # (N, A', B', (Ci x R1 x R2))\n",
    "    converted_x = sig2col(x, (k.shape[2],k.shape[3])) # (N, Ci, A', B', R1, R2)\n",
    "    converted_x = np.swapaxes(converted_x,1,2) # (N, A', Ci, B', R1, R2)\n",
    "    converted_x = np.swapaxes(converted_x,3,2) # (N, A', B', Ci, R1, R2)\n",
    "    converted_x = converted_x.reshape((converted_x.shape[0],converted_x.shape[1],converted_x.shape[2],converted_x.shape[3]*converted_x.shape[4]*converted_x.shape[5]))\n",
    "    # (N, A', B', (Ci x R1 x R2)) --> aim reached\n",
    "    \n",
    "    # next aim: bring k into the form as it is described in the sum above:\n",
    "    # ((Ci x R1 x R2), Co)\n",
    "    k = np.swapaxes(k,0,1) # (Ci, Co, R1, R2)\n",
    "    k = np.swapaxes(k,1,2) # (Ci, R1, Co, R2)\n",
    "    k = np.swapaxes(k,2,3) # (Ci, R1, R2, Co)\n",
    "    k = k.reshape((k.shape[0]*k.shape[1]*k.shape[2],k.shape[3]))\n",
    "    # ((Ci x R1 x R2), Co) --> aim reached\n",
    "\n",
    "    \n",
    "    # Using the sum concept as described in text above:\n",
    "    y = converted_x @ k # (N, A', B', Co)\n",
    "    \n",
    "    # last aim: bring y into the given shape (N, Co, A', B')\n",
    "    y = np.swapaxes(y,1,3) # (N, Co, B', A')\n",
    "    y = np.swapaxes(y,2,3) # (N, Co, A', B') --> aim reached\n",
    "    \n",
    "    return y\n",
    "    \n",
    "# Convolutional layer:\n",
    "class Conv2d(Module):\n",
    "    \"\"\" Numpy DL implementation of a 2D convolutional layer. \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, kernel_size, use_bias=True):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.use_bias = use_bias\n",
    "        \n",
    "        # register parameters 'w' and 'b' here (mind use_bias!)\n",
    "\n",
    "        self.w = self.register_parameter('w', np.empty((self.out_channels, self.in_channels, self.kernel_size[0],self.kernel_size[1])))\n",
    "        if use_bias:\n",
    "            self.b = self.register_parameter('b',np.empty((self.out_channels,)))\n",
    "            # one bias scalar per feature map\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self, seed: int = None):\n",
    "        \"\"\" \n",
    "        Reset the parameters to some random values.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        seed : int, optional\n",
    "            Seed for random initialisation.\n",
    "        \"\"\"\n",
    "        rng = np.random.default_rng(seed)\n",
    "        self.w = rng.standard_normal(size=self.w.shape)\n",
    "        if self.use_bias:\n",
    "            self.b = np.zeros_like(self.b)\n",
    "        \n",
    "    def compute_outputs(self, x):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : (N, Ci, H, W) ndarray\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        feature_maps : (N, Co, H', W') ndarray\n",
    "        cache : ndarray or tuple of ndarrays\n",
    "        \"\"\"\n",
    "   \n",
    "        \n",
    "        feature_maps = multi_channel_convolution2d(x, self.w) \n",
    "        \n",
    "        if self.use_bias:\n",
    "            feature_maps = np.swapaxes(feature_maps,0,1)\n",
    "            \n",
    "            for ind,ele in enumerate(feature_maps):\n",
    "                feature_maps[ind] += self.b[ind] # add to each feature map one\n",
    "                # bias scalar\n",
    "            \n",
    "            feature_maps = np.swapaxes(feature_maps,0,1)\n",
    "\n",
    "        \n",
    "        cache = x\n",
    "        \n",
    "        return feature_maps, cache\n",
    "    \n",
    "    def compute_grads(self, grads, cache):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        grads : (N, Co, H', W') ndarray\n",
    "        cache : ndarray or tuple of ndarrays\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        dx : (N, Ci, H, W) ndarray\n",
    "        \"\"\"\n",
    "        \n",
    "        self.zero_grad()\n",
    "        \n",
    "        x = cache\n",
    "        \n",
    "        # dw:\n",
    "        # grads as dL/ds = delta_error\n",
    "        grads2 = np.swapaxes(grads,1,0) # (Co, N, H', W')\n",
    "        x = np.swapaxes(x,0,1) # (Ci, N, H, W) because we have for \n",
    "        # multi_channel_convolution2d(x,k) x.shape=(_,Ci,_,_), \n",
    "        # k.shape=(_,Ci,_,_) --> Both have same dimension on second position\n",
    "        # of the shape.\n",
    "        dw = multi_channel_convolution2d(x,grads2) \n",
    "        dw = np.swapaxes(dw,0,1) # bring dw to shape of self.w\n",
    "        self.w.grad = dw\n",
    "        \n",
    "        # db:\n",
    "        if self.use_bias:\n",
    "            \n",
    "            grads3 = grads2.reshape(grads2.shape[0],grads2.shape[1]*grads2.shape[2]*grads2.shape[3])\n",
    "            # grads3: (Co, (N * H' * W')) = (Co, A)\n",
    "            db = np.sum(grads3, axis=1) \n",
    "            # because we have sum_over_a delta_Co,a * 1 where all shapes with\n",
    "            # exception of Co vanish in a.\n",
    "            self.b.grad = db\n",
    "            \n",
    "        # dx:\n",
    "        w2 = np.swapaxes(self.w,0,1) # (Ci, Co, R1, R2)\n",
    "        w2 = np.flip(w2,(2,3)) \n",
    "\n",
    "        size1 = int(self.kernel_size[0])-1\n",
    "        size2 = int(self.kernel_size[1])-1\n",
    "        \n",
    "        # zero-pad grads for each outcoming channel Co, therefore use grads2\n",
    "        # with (Co, N, H', W'). Reason: dx shall have shape of original input x.\n",
    "        # The outcome feature_maps becomes smaller depending on the kernel size.\n",
    "        # Therefore, make the grads in the shape of feature_maps bigger through\n",
    "        # padding such that dx doesn't get too small.:\n",
    "        grads_pad = np.pad(grads2, ((0,0),(0,0),(size1,size1),(size2,size2)), 'constant', constant_values=0)\n",
    "        # at the first two shapes there shall be no 0-frame added, therefore\n",
    "        # use (0,0). size1, size2 describe how wide the frame shall be on each\n",
    "        # side relating to the belonging shape dimension of grads2.\n",
    "        grads_pad2 = np.swapaxes(grads_pad,1,0) # (N, Co, H', W')\n",
    "        dx = multi_channel_convolution2d(grads_pad2,w2) \n",
    "        \n",
    "        return dx\n",
    "    \n",
    "# Activation functions:\n",
    "class ReLU(Module):\n",
    "    \"\"\" NNumpy implementation of the Rectified Linear Unit. \"\"\"\n",
    "        \n",
    "    def compute_outputs(self, s):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        s : (N, K) ndarray\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        a : (N, K) ndarray\n",
    "        cache : ndarray or iterable of ndarrays\n",
    "        \"\"\"\n",
    "        \n",
    "        cache = s\n",
    "        \n",
    "        a = np.clip(s,0,None) # replace negative values with 0\n",
    "        \n",
    "        return a, cache\n",
    "    \n",
    "    def compute_grads(self, grads, cache):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        grads : (N, K) ndarray\n",
    "        cache : ndarray or iterable of ndarrays\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ds : (N, K) ndarrays\n",
    "        \"\"\"\n",
    "        \n",
    "        # grads = dL/da \n",
    "        # dL/ds = grads * da/ds\n",
    "        \n",
    "        s = cache\n",
    "        ds = np.clip(s,0,None)/s # d max(0,x)/d x = max(0,x)/x\n",
    "        \n",
    "        return ds\n",
    "\n",
    "class ELU(Module):\n",
    "    \"\"\" NNumpy implementation of the Exponential Linear Unit. \"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=1.):\n",
    "        super().__init__()\n",
    "        if alpha < 0:\n",
    "            raise ValueError(\"negative values for alpha are not allowed\")\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        \n",
    "        \n",
    "    def compute_outputs(self, s):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        s : (N, K) ndarray\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        a : (N, K) ndarray\n",
    "        cache : ndarray or iterable of ndarrays\n",
    "        \"\"\"\n",
    "        \n",
    "        cache = s\n",
    "        \n",
    "        a = np.copy(s)\n",
    "        a[np.where(s<=0)] = self.alpha * (np.exp(s[np.where(s<=0)])-1)\n",
    "        \n",
    "        return a, cache\n",
    "    \n",
    "    def compute_grads(self, grads, cache):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        grads : (N, K) ndarray\n",
    "        cache : ndarray or iterable of ndarrays\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ds : (N, K) ndarrays\n",
    "        \"\"\"\n",
    "        s = cache\n",
    "        \n",
    "        ds = np.copy(s)\n",
    "        ds[np.where(s<=0)] = self.alpha * np.exp(s[np.where(s<=0)])\n",
    "        ds[np.where(s>0)] = 1\n",
    "        \n",
    "        return ds\n",
    "\n",
    "# Pooling - reducing size:\n",
    "class MaxPool2d(Module):\n",
    "    \"\"\" Numpy DL implementation of a max-pooling layer. \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__()\n",
    "        self.kernel_size = tuple(kernel_size)\n",
    "\n",
    "    def compute_outputs(self, x):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : (N, C, H, W) ndarray\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        a : (N, C, H', W') ndarray\n",
    "        cache : ndarray or tuple of ndarrays\n",
    "        \"\"\"\n",
    "        # Getting the x indices and values suitable for the kernel, i.e. building\n",
    "        # up kernel bites:\n",
    "        input_indices = sig2col(np.arange(x.size).reshape(x.shape),self.kernel_size,stride=self.kernel_size)\n",
    "        converted_x = sig2col(x,self.kernel_size,stride=self.kernel_size)\n",
    " \n",
    "        # Draw the max value and its index out of each kernel bite:\n",
    "        x_re = converted_x.reshape((converted_x.shape[0],converted_x.shape[1],converted_x.shape[2]*converted_x.shape[3],converted_x.shape[4]*converted_x.shape[5]))\n",
    "        max_values_x = np.max(x_re, axis=3)\n",
    "        where_max_values_x = np.argmax(x_re, axis=3)\n",
    "        where_max_values_x = where_max_values_x.reshape((where_max_values_x.shape[0],where_max_values_x.shape[1],where_max_values_x.shape[2],1))\n",
    "        # bring where_max_values_x into same shape length like x by adding\n",
    "        # one dimension with size=1.\n",
    "        \n",
    "        # Bring found out array with max values into the wanted shape:\n",
    "        # with stride=self.kernel_size we get:\n",
    "        height_x_rows = int(x.shape[2]/self.kernel_size[0])\n",
    "        width_x_col = int(x.shape[3]/self.kernel_size[1])\n",
    "        a = max_values_x.reshape(max_values_x.shape[0],max_values_x.shape[1],height_x_rows,width_x_col)\n",
    "        \n",
    "        \n",
    "        cache = x, x_re,  where_max_values_x,input_indices\n",
    "        return a, cache\n",
    "        \n",
    "       \n",
    "    def compute_grads(self, grads, cache):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        grads : (N, C, H', W') ndarray\n",
    "        cache : ndarray or tuple of ndarrays\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dx : (N, C, H, W) ndarray\n",
    "        \"\"\"\n",
    "        \n",
    "        # General idea: maxpooling: we have an area from with we draw the max.\n",
    "        # For the derivative mark the max of the place as 1 because derivative of \n",
    "        # max(x) is 1. The other places as 0.\n",
    "        \n",
    "        self.zero_grad()\n",
    "        \n",
    "        x, x_re, where_max_values_x,input_indices = cache\n",
    "        \n",
    "        # Mark all indices places where x_re is maximal with 1:\n",
    "        np.put_along_axis(x_re, where_max_values_x, 1 , axis=3)\n",
    "        \n",
    "        # collect all indices where x_re has NO max value in not_max_val:\n",
    "        max_ind = x_re.shape[-1]\n",
    "        not_max_val = np.ones((x_re.shape[0],x_re.shape[1],x_re.shape[2],max_ind-1),dtype='int')\n",
    "        val_range = np.arange(max_ind).tolist()\n",
    "        for ind_i,i in enumerate(where_max_values_x):\n",
    "            for ind_j,j in enumerate(i):\n",
    "                for ind_c,c in enumerate(j):\n",
    "                    val_range_copy = np.copy(val_range).tolist()\n",
    "                    \n",
    "                    val_range_copy.remove(c[0])\n",
    "                    not_max_val[ind_i,ind_j,ind_c] = np.array(val_range_copy,dtype='int')\n",
    "        \n",
    "        # Mark all indices places where x_re has no max as 0:\n",
    "        np.put_along_axis(x_re, not_max_val, 0 , axis=3) \n",
    "        \n",
    "        # Bring grads into shape such that it is mulitplicable with x_re:\n",
    "        grads = grads.reshape((grads.shape[0],grads.shape[1],grads.shape[2]*grads.shape[3],1))\n",
    "        grads2 = np.repeat(grads, x_re.shape[3] ,axis= 3)\n",
    "        \n",
    "        dx = grads2 * x_re\n",
    "        \n",
    "        # Reshape dx considering the indices of x-values where each value has \n",
    "        # to be places after the original order:\n",
    "        j = np.arange(x.size).reshape(x.shape)\n",
    "        ii = np.argsort(input_indices.flatten()) # getting indices\n",
    "        # input indices has shape of x_re\n",
    "        dx = (dx.flatten()[ii]).reshape((x.shape)) # dx has originally shape of \n",
    "        # x_re --> Flatten it like the input_indices above and then use the \n",
    "        # found indices order\n",
    "        \n",
    "        return dx\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as 2:     \n",
    "# activation functions:\n",
    "class Identity(Module):\n",
    "    \"\"\" NNumpy implementation of the identity function. \"\"\"\n",
    "        \n",
    "    def compute_outputs(self, s):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        s : (N, K) ndarray\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        a : (N, K) ndarray\n",
    "        cache : ndarray or iterable of ndarrays\n",
    "        \"\"\"\n",
    "        \n",
    "        a = s # we have out_neuron = activation_func(pre_activation) = I(s) = s\n",
    "        \n",
    "        cache = s\n",
    "        \n",
    "        return a, cache\n",
    "    \n",
    "    \n",
    "    def compute_grads(self, grads, cache):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        grads : (N, K) ndarray\n",
    "        cache : ndarray or iterable of ndarrays\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ds : (N, K) ndarray\n",
    "        \"\"\"\n",
    "        \n",
    "        # we have input: grads = dLinear(a(s))/da(s)\n",
    "        # we want output: dL/ds = dLinear(a(s))/da(s) * da(s)/ds = grads * da(s)/ds\n",
    "        # for identity we get: da(s)/ds = 1 (np.ones)\n",
    "        return grads\n",
    "        \n",
    "        \n",
    "class Tanh(Module):\n",
    "    \"\"\" NNumpy implementation of the hyperbolic tangent function. \"\"\"\n",
    "        \n",
    "    def compute_outputs(self, s):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        s : (N, K) ndarray\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        a : (N, K) ndarray\n",
    "        cache : ndarray or iterable of ndarrays\n",
    "        \"\"\"\n",
    "\n",
    "        a = np.tanh(s)\n",
    "        cache = s\n",
    "        \n",
    "        return a, cache\n",
    "        \n",
    "    \n",
    "    def compute_grads(self, grads, cache): # I replaced a with cache by in description ache denoted\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        grads : (N, K) ndarray\n",
    "        cache : ndarray or iterable of ndarrays\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ds : (N, K) ndarray\n",
    "        \"\"\"\n",
    "        # grads = dL/da \n",
    "        # dL/ds = grads * da/ds\n",
    "        \n",
    "        tanh_ = np.tanh(cache)\n",
    "        dads = ( 1 - tanh_ ** 2)\n",
    "        ds = grads * dads\n",
    "        return ds\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(Module):\n",
    "    \"\"\" NNumpy module to convert multi-dimensional outputs to a single vector. \"\"\"\n",
    "    \n",
    "    def compute_outputs(self, x):\n",
    "        return x.reshape(len(x), -1), x.shape\n",
    "    \n",
    "    def compute_grads(self, grads, shape):\n",
    "        return grads.reshape(shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the remainder of this assignment will be to train a network on the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(x_train, y_train):\n",
    "    x_train = x_train.reshape(-1, 1, 28, 28)\n",
    "    y_train = to_one_hot(y_train)\n",
    "    \n",
    "    # NOTE: custom data processing is allowed\n",
    "    x_train = np.pad(x_train, ((0, 0), (0, 0), (2, 2), (2, 2)))\n",
    "    x_train = x_train - np.mean(x_train, axis=(1, 2, 3), keepdims=True)\n",
    "    x_train = x_train / np.std(x_train, axis=(1, 2, 3), keepdims=True)\n",
    "    \n",
    "    return x_train, y_train\n",
    "\n",
    "x_train, y_train = get_mnist_data()\n",
    "print(x_train.shape)\n",
    "x_train, y_train = process_data(x_train, y_train)\n",
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Evaluation and Update (3 Points)\n",
    "\n",
    "When using the optimisers to fit a neural network to a given set of data, we can effectively minimise the empirical error. However, we are actually interested in minimising the risk. Therefore, it is also useful to evaluate the network regularly on unseen data.\n",
    "\n",
    " > Implement the `evaluate`, and `update` functions so that they perform the training and evaluation computations, respectively, for one iteration (aka *epoch*) over the entire dataset. Make sure to return the loss values to get loss curves at the end of this assignment.\n",
    " \n",
    "**Hint:** you can use the `step` method of the optimiser to update the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(network, metric, data_loader):\n",
    "    \"\"\"\n",
    "    Evaluate a network by computing a metric for specific data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    network : Module\n",
    "        A module that implements the network.\n",
    "    metric : callable\n",
    "        A function that takes logits and labels \n",
    "        and returns a scalar numpy array.\n",
    "    data_loader : Dataloader\n",
    "        The data loader that provides the batches.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    values : ndarray\n",
    "        The computed metric values for each batch in the data loader.\n",
    "        computed metric values for each batch in the data loader.\n",
    "    \"\"\"\n",
    "    network.eval()\n",
    "    errors = np.array([])\n",
    "    for mini_batch_x, mini_batch_y in data_loader: \n",
    "\n",
    "        logits = network.forward(mini_batch_x)\n",
    "        error_one_batch = metric(logits, mini_batch_y)\n",
    "        errors = np.append(errors, error_one_batch)\n",
    "   \n",
    "    values = np.array([np.sum(errors)/len(errors)]) # we only want one error values in the end\n",
    "\n",
    "    return values\n",
    "\n",
    "    \n",
    "    \n",
    "def update(network, loss, data_loader, optimiser):\n",
    "    \"\"\"\n",
    "    Update a network by optimising the loss for the given data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    network : Module\n",
    "        A module that implements the network.\n",
    "    loss : Module\n",
    "        Loss function module.\n",
    "    data_loader : Dataloader\n",
    "        The data loader that provides the batches.\n",
    "    optimiser : Optimiser\n",
    "        Optimisation algorithm to use for the update.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    errors : ndarray\n",
    "        The computed loss for each batch in the data loader.\n",
    "    \"\"\"\n",
    "    network.train()\n",
    "    loss.train()\n",
    "    errors = np.array([])\n",
    "    \n",
    "    for mini_batch_x, mini_batch_y in data_loader: \n",
    "\n",
    "        logits = network.forward(mini_batch_x)\n",
    "        \n",
    "        network.zero_grad()\n",
    "        error_one_batch = loss.forward(logits, mini_batch_y)\n",
    "        errors = np.append(errors, error_one_batch)\n",
    "\n",
    "        dlogits, dtargets = loss.backward(np.array([1]))\n",
    "        dx = network.backward(dlogits) \n",
    "\n",
    "        optimiser.step() # no return value\n",
    "\n",
    "    errors = np.array([np.sum(errors)/len(errors)]) # we only want one error values in the end\n",
    "   \n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, valid_loader, network, loss, optimiser, epochs=1):\n",
    "    \"\"\"\n",
    "    Train a neural network with gradient descent.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    train_loader : Dataloader\n",
    "        Dataloader producing batches of input-target pairs.\n",
    "    valid_loader : Dataloader\n",
    "        Dataloader producing batches of input-target pairs.\n",
    "    network : Module\n",
    "        A module that implements the network.\n",
    "    loss : Module\n",
    "        Loss function module.\n",
    "    optimiser : Optimiser\n",
    "        Optimisation algorithm.\n",
    "    epochs : int, optional\n",
    "        Number of times to iterate the dataset.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    train_errors : (epochs + 1, n_batches) ndarray\n",
    "        Training error for each epoch and each batch.\n",
    "    valid_errors : (epochs + 1, 1) ndarray\n",
    "        Validation error for each epoch.\n",
    "    \"\"\"\n",
    "    # log performance before training (for reference)\n",
    "    train_errors = [evaluate(network, loss.eval(), train_loader)]\n",
    "    valid_errors = [evaluate(network, loss.eval(), valid_loader)]\n",
    "    \n",
    "    print('start epoch')\n",
    "    \n",
    "    # train for given number of epochs\n",
    "    for _ in range(epochs):\n",
    "        train_errors.append(update(network, loss, train_loader, optimiser))\n",
    "        print('training done')\n",
    "        valid_errors.append(evaluate(network, loss.eval(), valid_loader))\n",
    "        print('evaluation done')\n",
    "        \n",
    "        print('one epoch done')\n",
    "    print('my train + valid errors',train_errors,valid_errors)\n",
    "    return np.stack(train_errors), np.stack(valid_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Training Logistic Regression (5 points)\n",
    "\n",
    "To test your framework, it is best to start with a simple problem.\n",
    "Therefore, we jump back to assignment one, where we learned that logistic regression is actually a single-layer network.\n",
    "If your optimiser works correctly, the loss should go down when a sufficiently small learning rate was chosen.\n",
    "\n",
    " > Build a single-layer network from your own modules and train it to classify MNIST digits.\n",
    " > You can use the `train` function from the previous exercise.\n",
    " > Train your network for at least five epochs.\n",
    " > Use an adaptive optimiser to train your network with *stochastic* gradients.\n",
    " > The `split_data` function can be used to create a validation dataset.\n",
    " \n",
    "**Hint:** the `Flatten` module I gifted earlier, might be useful for turning images into vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and prepare dataset:\n",
    "\n",
    "# use previous code:\n",
    "#x_train, y_train = get_mnist_data() # but of course we don't use x_train as x_train but as whole \n",
    "# dataset representation\n",
    "#x_train, y_train = process_data(x_train, y_train)\n",
    "\n",
    "flatter = Flatten()\n",
    "x_vec, x_vec_shape = flatter.compute_outputs(x_train)\n",
    "\n",
    "train_data, valid_data = split_data(x_vec,y_train)\n",
    "train_data_x, train_data_y = train_data\n",
    "val_data_x, val_data_y = valid_data\n",
    "\n",
    "batch_size = 16*2*2*2*2 # 256\n",
    "train_loader = Dataloader(train_data_x, train_data_y, batch_size, shuffle=True, seed=0)\n",
    "val_loader = Dataloader(val_data_x, val_data_y, batch_size, shuffle=True, seed=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Network architecture\n",
    "net = Sequential(\n",
    "        Linear(in_features=x_vec.shape[1], out_features=val_data_y.shape[1]),\n",
    "        AlgebraicSigmoid()\n",
    "        )\n",
    "\n",
    "\n",
    "loss = LogitCrossEntropy(reduction='none')\n",
    "optimiser = Adamax(net.parameters(), lr = 1e-5, bias_correction = True)\n",
    "# at this place we connect the optimizer with the parameters of the logistic regression\n",
    "\n",
    "train_err, valid_err = train(train_loader, val_loader, net, loss, optimiser, epochs=20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(net, train_err, valid_err);"
   ]
  },
  {
   "attachments": {
    "download.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZcAAAEICAYAAACTVrmbAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAABAK0lEQVR4nO3deXxU1fn48c+TbRIyIUASkE1BQFllERVBXIsCLijighbr/tW6/rS1fKt1q/ar1lq1Ki3WXQoogkKLOyiKC1vZUUBECSCEACEJZJnM8/vj3gyTyUxImEmCmef9es0rM+ece8+5k8k8Oefce66oKsYYY0wsJTR2A4wxxjQ9FlyMMcbEnAUXY4wxMWfBxRhjTMxZcDHGGBNzFlyMMcbEnAUXE1dEZKOI/KIR6h0qIt82dL3GNJakxm6AMfFAVT8Djm7sdhjTUKznYkwMiEhiY7chWk3hGMyhw4KLiVsikiAi40XkOxHJF5E3RKRVUP6bIvKTiBSIyDwR6RWU97KITBCR2SJSDJzmDrn9RkSWu9tMFZFUt/ypIpIbtH3Esm7+XSKyVUS2iMi1IqIi0jXCcbQSkZfcsrtE5G03/UoR+TykbGA/YY7hN+7xJgaVv0BElh/o/RKRVBF53U3fLSILRaRNFL8e8zNnwcXEs1uA84FTgHbALuDZoPx3gW5Aa2AJMClk+8uAh4EMoPJL/GJgONAZOAa4sob6w5YVkeHAHcAvgK7AqQc4jteAZkAvt61/PUD5SMfwFFAMnB6S/y/3eU3v16+ATKAjkAXcAOyrQztME2PBxcSzG4C7VTVXVUuB+4ExIpIEoKovqmphUF5fEckM2v4dVZ2vqn5VLXHTnlbVLaq6E5gF9Kuh/khlLwZeUtVVqrrXrTssEWkLjABuUNVdqlquqp/W4T0IPYbJwFh33xnASDcNan6/ynGCSldVrVDVxaq6pw7tME2MBRcTz44AZrjDOLuBNUAF0EZEEkXkEXcIaA+w0d0mO2j7TWH2+VPQ872At4b6I5VtF7LvcPVU6gjsVNVdNZSpSei+/wWMFhEPMBpYoqo/uHkR3y+c3tP7wBR3eO4xEUk+yDaZJsCCi4lnm4ARqtoi6JGqqptxhoNG4QxNZQKd3G0kaPv6WlJ8K9Ah6HXHGspuAlqJSIswecU4w2UAiMhhYcpUOQZVXQ38gNMbCh4Sq6wr7Pvl9pgeUNWewGDgHOCKGtptmjgLLiae/R14WESOABCRHBEZ5eZlAKVAPs4X9J8asF1vAFeJSA8RaQb8IVJBVd2KMzf0nIi0FJFkETnZzV4G9BKRfu7JAvfXsv5/AbcBJwNvBqVHfL9E5DQR6eOeDLAHZ5jMX8v6TBNkwcXEs6eAmcAHIlIIfAWc4Oa9ivMf/GZgtZvXIFT1XeBpYC6wPqju0gibjMP5Mv8G2A7c7u5nLfAg8BGwjv0nHRzIZJxJ+zmquiMovab36zBgGk5gWQN8ijNUZuKU2M3CjDm0iUgPYCXgUVVfY7fHmNqwnosxhyD3+hKPiLQEHgVmWWAxPycWXIw5NP0PzhDXdzhnZN3YuM0xpm5sWMwYY0zMWc/FGGNMzNmqyK7s7Gzt1KlTYzfDGGN+VhYvXrxDVXNC0y24uDp16sSiRYsauxnGGPOzIiI/hEtv8sFFRNKB54Ay4BNVDV180BhjTIxFNeciIh1FZK6IrBaRVSJyW4RyLURkmoh8IyJrROTEKOp8UUS2i8jKkPThIvKtiKwXkfFBWaOBaap6HXDewdZrjDGm9qKd0PcBd7rrCQ0CbhKRnmHKPQW8p6rdgb44V/AGiEhrdwXW4LSw964AXsZZpjy4bCLO0t8jgJ7A2KB2dGD/4nwVtTwuY4wxUYhqWMxd12ir+7xQRNYA7XGWywDAXaL8ZNx7VahqGc4QVbBTgBtEZKSqlorIdTg9jhFh6pwnIp1Cko8H1qvqBrfOKTiLDq4GcnECzFLCBFMRORc4t2vXSLHMmPhVXl5Obm4uJSUlBy5smrTU1FQ6dOhAcnLtFruO2ZyL+4XfH/g6JKszkAe8JCJ9gcXAbapaXFlAVd8Ukc7AVBF5E7gaGFaH6ttTdenwXPaveTQdeEZEzsa5Z0YVqjoLmDVw4MDr6lCfMXEhNzeXjIwMOnXqhIgceAPTJKkq+fn55Obm0rlz51ptE5PrXETEC7wF3B7mBkFJwABggqr2x1kGfHxIGVT1MaAEmACcp6pFsWibqhar6lWqeqNN5htTNyUlJWRlZVlgiXMiQlZWVp16sFEHF/eGQG8Bk1R1epgiuUCuqlb2aKbhBJvQ/QwFegMzgPvq2IzNVL3nRQc3zRgTJQssBur+OYj2bDEBXgDWqOoT4cqo6k/AJhE52k06g6A5GXc//YGJOPMkVwFZIvJQHZqyEOgmIp1FJAW4FGdp8Ho399vtPDNnXUNUZYwxPxvR9lyG4NxL4nQRWeo+RgKIyGwRaeeWuwWYJCLLce4THnrjpWbAxar6nar6ce5gF/bCHBGZDHwJHC0iuSJyjbta7M04t1ldA7yhqquiPLZa+WL9Dv42Zz22RpsxxuwXVXBR1c9VVVT1GFXt5z5mu3kjVXWL+3ypqg50y50fer9vVZ2vqiuCXper6vMR6hyrqm1VNVlVO6jqC276bFU9SlW7qOrD0RxXXWR7PZT6/BSV2mroxsTa7t27ee655w5q25EjR7J79+46bfPkk0/y6quvHlR9dbF06VJOPPFEevXqxTHHHMPUqVMDeUOHDqVfv37069ePdu3acf7554fdx1133UWvXr3o0aMHt956K6pKYWFhYNt+/fqRnZ3N7bffDsDf/vY3evfuzciRIykrc07Y/fzzz/l//+//BfaZl5fH8OHDw1VXd6pqD1WOPfZYPRjTFm3SI373b92QV3RQ2xtzKFu9enWj1v/9999rr169wuaVl5fHtK7y8nLt06dPzPcbzrfffqtr165VVdXNmzfrYYcdprt27apWbvTo0frKK69US58/f74OHjxYfT6f+nw+HTRokM6dO7dauQEDBuinn36qqqonnHCCVlRU6B//+EedOXOm+v1+PfPMMzU/P7/KNldeeaV+/vnnYdsd7vMALNIw36lNfvmX+pad4QEgv6iUztnpjdwaY+rPA7NWsXpL6Mmg0enZrjn3ndsrYv748eP57rvv6NevH8OGDePss8/mD3/4Ay1btuSbb75h7dq1nH/++WzatImSkhJuu+02rr/+emD/eoFFRUWMGDGCk046iS+++IL27dvzzjvvkJaWVqWuOXPmMGDAAJKSnK/F559/nokTJ1JWVkbXrl157bXXaNasGdu2beOGG25gw4YNAEyYMIHBgwfz6quv8vjjjyMiHHPMMbz2WuS7PB911FGB5+3ataN169bk5eXRokWLQPqePXuYM2cOL730UrXtRYSSkhLKyspQVcrLy2nTpk2VMmvXrmX79u0MHToUIFBu7969JCcn8/rrrzNixAhatWpVZbvzzz+fSZMmMWTIkIjtrw1bcj9KWekpAOwoinR7c2PMwXrkkUfo0qULS5cu5c9//jMAS5Ys4amnnmLt2rUAvPjiiyxevJhFixbx9NNPk5+fX20/69at46abbmLVqlW0aNGCt956q1qZ+fPnc+yxxwZejx49moULF7Js2TJ69OjBCy+8AMCtt97KKaecwrJly1iyZAm9evVi1apVPPTQQ8yZM4dly5bx1FNPATBz5kzuvffeGo9xwYIFlJWV0aVLlyrpb7/9NmeccQbNmzevts2JJ57IaaedRtu2bWnbti1nnXUWPXr0qFJmypQpXHLJJYGzvG6++WYGDRrEjz/+yJAhQ3jppZe46aabqu174MCBfPbZZzW2uTas5xKlHLfnklcUuuiAMU1LTT2MhnT88cdXuZDv6aefZsaMGQBs2rSJdevWkZWVVWWbzp07069fPwCOPfZYNm7cWG2/W7durfIFvXLlSu655x52795NUVERZ511FuD0cCrnZRITE8nMzOTVV1/loosuIjs7GyDQGzjvvPM477zISxpu3bqVcePG8corr5CQUPV//cmTJ3PttdeG3W79+vWsWbOG3NxcAIYNG8Znn30W6KWAE1yCe0/jxo1j3LhxADz44IPceuutvPvuu7z66qt07NiRv/zlLyQkJNC6dWu2bNkSsc21ZT2XKLWq7LkUWs/FmIaQnr5/+PmTTz7ho48+4ssvv2TZsmX0798/7IV+Ho8n8DwxMRGfr/oJOGlpaVW2vfLKK3nmmWdYsWIF9913X8yXwNmzZw9nn302Dz/8MIMGDaqSt2PHDhYsWMDZZ58ddtsZM2YwaNAgvF4vXq+XESNG8OWXXwbyly1bhs/nq9ITq7RlyxYWLFjA+eefz1/+8hemTp1KixYt+PjjjwHnwtnQIcODYcElSsmJCbRslkx+sQUXY2ItIyODwsLCiPkFBQW0bNmSZs2a8c033/DVV18ddF09evRg/fr1gdeFhYW0bduW8vJyJk3av7jHGWecwYQJEwCoqKigoKCA008/nTfffDMwJLdz584a6yorK+OCCy7giiuuYMyYMdXyp02bxjnnnENqamrY7Q8//HA+/fRTfD4f5eXlfPrpp1V6XZMnT2bs2LFht/3DH/7Agw8+CMC+ffsQERISEti7dy/gzNX07t27xvbXhgWXGMjyethRaMNixsRaVlYWQ4YMoXfv3vz2t7+tlj98+HB8Ph89evRg/Pjx1XoAdTFixAjmzZsXeP3HP/6RE044gSFDhtC9e/dA+lNPPcXcuXPp06cPxx57LKtXr6ZXr17cfffdnHLKKfTt25c77rgDiDzn8sYbbzBv3jxefvnlwGnDS5cuDeRPmTKlWnBYtGhRYJhszJgxdOnShT59+tC3b1/69u3LueeeW2X/4YLLf//7XwAGDHAWSbnsssvo06cP8+fPD5yCPHfu3Ig9proQtYv/ABg4cKAe7J0oL534Jb4KZdqNg2PcKmMa15o1a6pNFDdlF1xwAY899hjdunVr7KY0mpNPPpl33nmHli1bVssL93kQkcWqOjC0rPVcYiDb67GzxYxpAh555BG2bt3a2M1oNHl5edxxxx1hA0td2dliMZDt9ZBvZ4sZ87N39NFHc/TRRx+4YBOVk5MTcUWAurKeSwxke1MoLPVRUm43ujTGGLDgEhPZXuc0RxsaM8YYhwWXGNgfXGxozBhjwIJLTASvL2aMMcaCS0zY+mLGHDq8Xi/gXIke7gJFgFNPPZVIlx6MGTMmsChlfZo0aRLHHHMMffr0YfDgwSxbtiyQ99e//pVevXrRu3dvxo4dG3Z1gL///e/06dOHfv36cdJJJ7F6dZV7MPLjjz/i9Xp5/PHHAedMsJNOOonevXvz9ttvB8qNGjWqynIvv/nNb5gzZ07Ux2fBJQYq1xezYTFjDh3t2rVj2rRpddpm1apVVFRUcOSRR9ZTq/br3Lkzn376KStWrOAPf/hDYDXnzZs38/TTT7No0SJWrlxJRUUFU6ZMqbb9ZZddxooVK1i6dCl33XVX4MLNSnfccQcjRowIvJ48eTI33HADCxYs4MknnwRg1qxZ9O/fn3bt2gXK3XLLLTzyyCNRH5+dihwDqcmJeD1J5Nn6YqaJu+QfX1ZLO+eYtow7sRP7yiq48qUF1fLHHNuBiwZ2ZGdxGTe+vrhK3tT/ObHG+saPH0/Hjh0Dq/fef//9eL1ebrjhBkaNGsWuXbsoLy/noYceYtSoUVW23bhxI+eccw4rV65k3759XHXVVSxbtozu3buzb9++sPVNmjSpyn5uvPFGFi5cyL59+xgzZgwPPPAAAAsXLuS2226juLgYj8fDxx9/TLNmzfjd737He++9R0JCAtdddx233HJLxGMbPHj/RdeDBg0KLEIJ4PP52LdvH8nJyezdu7fKl3+l4NWSi4uLq9zj/u2336Zz585V1mGr3FdpaWlgfbUnn3ySWbNmVdnvEUccQX5+Pj/99BOHHXZYxPYfSJMOLiKSDjwHlAGfqOqkA2xy0LK9KeQXW8/FmFi65JJLuP322wPB5Y033uD9998nNTWVGTNm0Lx5c3bs2MGgQYM477zzqnzBBpswYQLNmjVjzZo1LF++PLD8Saj58+dXWTbl4YcfplWrVlRUVHDGGWewfPlyunfvziWXXMLUqVM57rjj2LNnD2lpaUycOJGNGzeydOlSkpKSAuuL3XvvvQwcOLDG1ZFfeOGFQC+jffv2/OY3v+Hwww8nLS2NM888kzPPPDPsds8++yxPPPEEZWVlgaGsoqIiHn30UT788MPAkBg4PZ3LLruMiRMn8uijj/Lcc88xbtw4mjVrVm2/AwYMYP78+Vx44YUR23xA4e4gFvwAOgJzgdXAKuC2COU2AiuApQTdmSxS+sE8gBeB7cDKkPThwLfAemB8UPo44Fz3+dSa9n2wd6KsNPq5+XrpP76Mah/GHGoa+06Uqqrdu3fXzZs369KlS3Xw4MGqqlpWVqY33XST9unTR/v27aupqam6detWVVVNT09X1ap3sRw1apR+/PHHgX32799fFy5cWK2ubt26BfajqjphwgTt37+/9unTR7Ozs3Xy5Mm6fPnyQDuCjR49Wj/44IM6H9+cOXO0e/fuumPHDlVV3blzp5522mm6fft2LSsr01GjRulrr71W4z4mTZqkV1xxhaqq3nnnnTp16lRVVb3vvvv0z3/+c7XyO3fu1F/84hdaWFio1157rV544YX6xRdfBPJ///vf69NPP11tu1jfidIH3KmqS0QkA1gsIh+q6uowZU9T1R11SAdARFoD+1S1MCitq6quDyn6MvAM8GpQuUTgWWAYkAssFJGZbvs64AQ2gHq9wjHbm8KGvOL6rMKYuHTRRRcxbdo0fvrpJy655BLAGb7Ky8tj8eLFJCcn06lTp5gsiR+87P7333/P448/zsKFC2nZsiVXXnllzJfdX758Oddeey3vvvtu4B40H330EZ07dyYnJwdwblr2xRdf8Mtf/jLifi699FJuvPFGAL7++mumTZvGXXfdxe7du0lISCA1NZWbb745UP6Pf/wjd999N5MnT+akk05izJgxjB49mvfffx+IzbL7B5zQV9WtqrrEfV4IrAHaR1VrdacAb4uIB0BErgP+FqYt84DQtayPB9ar6gZVLQOmAJWDprk4AQYiHKuInCsiEwsKCqI6gGyvx4bFjKkHl1xyCVOmTGHatGlcdNFFgLPUfuvWrUlOTmbu3Ln88MMPNe7j5JNP5l//+hfg3ARs+fLlYcsFL7u/Z88e0tPTyczMZNu2bbz77ruAs0TM1q1bWbhwIeAsze/z+Rg2bBj/+Mc/AveKOdCy+z/++COjR4/mtddeq3Lb48MPP5yvvvqKvXv3oqp8/PHHYRcPXbduXeD5f/7zn8Bim5999hkbN25k48aN3H777fz+97+vEljWrVtHbm4up556Knv37iUhIQERqTIPFYtl9+t0tpiIdAL6A1+HyVbgAxFZLCLX1yJ9fwHVN4H3gakicjlwNXBRLZvVHtgU9DqX/cFvOnChiEwAZoVu6NY9S1Wvz8zMrGV14WV7PezaW4avwh/VfowxVfXq1YvCwkLat29P27ZtAbj88stZtGgRffr04dVXX62yJH44N954I0VFRfTo0YN777037E20AM4++2w++eQTAPr27Uv//v3p3r07l112WeCe8ikpKUydOpVbbrmFvn37MmzYMEpKSrj22ms5/PDDOeaYY+jbt28gmN17773MnDmzWl0PPvgg+fn5/PrXv6Zfv34MHOgsLHzCCScwZswYBgwYQJ8+ffD7/YEzyYL39cwzz9CrVy/69evHE088wSuvvFKr9/Puu+/m4YcfBmDs2LFMmDCB4447jttuuw2A8vJy1q9fH2jPQQs3VhbuAXiBxcDoCPnt3Z+tgWXAyTWlR9jHFGAPkFNDmU4EzbkAY4B/Br0eBzxT2+OqfEQ75/LqF9/rEb/7t24r2BfVfow5lBwKcy4Nae/evXrCCSeoz+dr7KY0munTp+s999wTNq8ucy616rmISDLwFjBJVadHCFKb3Z/bgRk4w1UR08PUMRTo7Za5rzbtcm3GOemgUgc3rUFVLgGTZxdSGvOzlZaWxgMPPMDmzQ3+FXLI8Pl83HnnnVHv54DBRZxz+14A1qjqExHKpLuT/ZWn/54JrIyUHmb7/sBEnLmSq4AsEXmolsewEOgmIp1FJAW4FKjeB61n+5eAsXkX07RonN1Q8KyzzuLwww9v7GY0mosuuogWLVpUS6/r56A2PZchOENNp4vIUvcxEkBEZotIO6AN8LmILAMWAP9R1fdqSA/VDLhYVb9TVT9wBVBthk5EJgNfAkeLSK6IXKOqPuBmnDmbNcAbqrqqTu9CDNgSMKYpSk1NJT8/P+4CjKlKVcnPzyc1NbXW2xzwVGRV/RwIe2WSqo4Metk3TP6GcOlhys0PeV0OPB+mXPWbQjvps4HZB6qnPmVn2LL7punp0KEDubm55OXlNXZTTCNLTU2lQ4cOBy7oatJX6DekDE8SKUkJtr6YaVKSk5Pp3LlzYzfD/AzZwpUxIiLkeD3WczHGGCy4xFSWN8V6LsYYgwWXqM34by5XvLgAVSXb62GHrYxsjDEWXKKVV1jKvLV5FJdVkO1NsWExY4zBgkvUKm8UlldYSrbXw87iMvx+O23TGBPfLLhEKcfrnPe9fU8JWV4PPr9SsK+8kVtljDGNy4JLlAI9l6JSsr12IaUxxoAFl6i1zvDQrbWXpATnVGSw9cWMMcYuooxSy/QUPrzjFADWbnPudWbrixlj4p31XGLI1hczxhiH9Vxi4H+nO3e1e/j8PiQmiAUXY0zcs+ASA9v2lLK9sISEBKFVego7Cm1YzBgT32xYLAZyvB7y3Cvzs70e8out52KMiW8WXGIgJ8PDjqIyKvxKtjeFPJvQN8bEOQsuMZDtTaHCr+zaW2brixljDBZcYqJLay9Du2VT5vOT7U0hv7jU7txnjIlrTX5CX0TSgeeAMuATVZ0U6zqGdsthaLccwJlzKSn3U1xWgdfT5N9eY4wJK6qei4h0FJG5IrJaRFaJyG0Rym0UkRUislREFkVZ54sisl1EVoakDxeRb0VkvYiMD8oaDUxT1euA86Kpuzay3Kv0bWjMGBPPoh0W8wF3qmpPYBBwk4j0jFD2NFXtp6oDQzNEpLWIZISkdY2wn5eB4SFlE4FngRFAT2BsUDs6AJvc5xUHPqS6KymvYOhjc3jx8+9tfTFjjCHK4KKqW1V1ifu8EFgDtD+IXZ0CvC0iHgARuQ74W4Q65wE7Q5KPB9ar6gZVLQOmAKPcvFycAANhjldEzhWRiQUFBQfRbEdqciL5RWVs2b2P7Mqei50xZoyJYzGb0BeRTkB/4Osw2Qp8ICKLReT6apmqbwLvA1NF5HLgauCiOlTfnv29E3ACSmWQmw5cKCITgFlh6p6lqtdnZmbWobrqcjI85BWVBlZJtp6LMSaexWTGWUS8wFvA7aq6J0yRk1R1s4i0Bj4UkW/cHkiAqj4mIlOACUAXVS2KRdtUtRi4Khb7qknlhZStbH0xY4yJvuciIsk4gWWSqk4PV0ZVN7s/twMzcIaxQvczFOjt5t9Xx2ZsBjoGve7gpjWYbDe4JCcm0KJZsgUXY0xci/ZsMQFeANao6hMRyqRXTta7pwWfCYSe6dUfmIgzT3IVkCUiD9WhKQuBbiLSWURSgEuBmXU9nmgM6ZbNSd2yAXcJGJtzMcbEsWiHxYYA44AVIrLUTfu9qs4WkdnAtUAqMMOJQyQB/1LV90L20wy4WFW/AxCRK4Arw1UoIpOBU4FsEckF7lPVF0TkZpx5m0TgRVVdFeWx1cm4QUcEnmd7U6znYoyJa1EFF1X9HJAIeSODXvY9wH7mh7wuB56PUHZshPTZwOya6qlvqoqqc63L6i3hpp6MMSY+2PIvMTJvbR5H3fMuKzYXkGPrixlj4pwFlxjJTEumvELJKywl25tCYamPkvJ6uWbTGGMOeRZcYqTy+pa8otLAhZT5xTapb4yJTxZcYiTLXfYlr7DU1hczxsQ9Cy4x4klKpEWz5MCwGNiFlMaY+GVrwsfQFSd2oltr7/5hMbvWxRgTpyy4xNAdw44CYF+ZM5GfZz0XY0ycsmGxGPL7lV3FZaSlJJKekmjDYsaYuGXBJYb+NHsNgx+Zg6qSneGxZfeNMXHLgksM5WR42FdeQXFZhbu+mPVcjDHxyYJLDAWudSksJSvd1hczxsQvCy4xFBxcbFjMGBPPLLjEUJXg4vWwa28Zvgp/I7fKGGMangWXGGrXIo07hh1F19ZecrwpqMLOvdZ7McbEH7vOJYaapyZz6xndAPguz7lL847CMlpnpDZms4wxpsFZcImx7YUllFdo4Cp9m9Q3xsQjCy4xdvXLC8nxevjDOT0ByC+24GKMiT825xJjOV6Ps+x+RuXKyDbnYoyJP006uIhIuoi8IiLPi8jlDVFnToaHvMJSMjxJpCQm2LCYMSYuHTC4iEhHEZkrIqtFZJWI3FZD2UQR+a+I/DsobaOIrBCRpSKyKJrGisiLIrJdRFaGpA8XkW9FZL2IjA/KGg1MU9XrgPOiqbu2ctzrW1Qh25tii1caY+JSbXouPuBOVe0JDAJuEpGeEcreBqwJk36aqvZT1YHhNhKR1iKSEZLWNUzRl4HhIeUSgWeBEUBPYGxQ+zoAm9znDXLP4Wyvhwq/smtvGdkZHlt23xgTlw4YXFR1q6oucZ8X4gSP9qHlRKQDcDbwz4NoxynA2yLicfd1HfC3MG2ZB+wMST4eWK+qG1S1DJgCjHLzcnECDEQ4VhE5V0QmFhQUHESzqxvaLZvHxhyDJzmRbK/HhsWMMXGpTnMuItIJ6A98HSb7SeAuIPSSdAU+EJHFInJ9uP2q6pvA+8BUd27kauCiWjarPft7J+AElMrgNx24UEQmALMi1D1LVa/PzMysZXU169o6g4sHdsTrSbL1xYwxcavWpyKLiBd4C7hdVfeE5J0DbFfVxSJyasimJ6nqZhFpDXwoIt+4PZAqVPUxEZkCTAC6qGpRHY+lGlUtBq6Kdj91UV7hZ9WWPbRp7gkMi/n9SkKCNGQzjDGmUdWq5yIiyTiBZZKqTg9TZAhwnohsxBmWOl1EXgdQ1c3uz+3ADJxhrHB1DAV6u2Xuq8MxbAY6Br3u4KY1ilKfn/Ofnc+sZVvI9nrw+ZU9JeWN1RxjjGkUtTlbTIAXgDWq+kS4Mqr6v6raQVU7AZcCc1T1l+6pwBnuftKBM4GVoduLSH9gIs5cyVVAlog8VMtjWAh0E5HOIpLi1j+zltvGXHpKImnJie7ilSmAXaVvjIk/tem5DAHG4fRGlrqPkQAiMltE2tWwbRvgcxFZBiwA/qOq74Up1wy4WFW/U1U/cAXwQ2ghEZkMfAkcLSK5InKNqvqAm3HmbNYAb6jqqlocV70QkcC1LpVLwOTZhZTGmDhzwDkXVf0cCDthoKojw6R9AnziPt8A9K1FHfNDXpcDz4cpNzbC9rOB2Qeqp6HkZLhX6dv6YsaYONWkr9BvLDleT5VhMbvdsTEm3tjClfXgf045klKfn5bNUkgQ7I6Uxpi4Y8GlHvQ/vGXgeat0u5DSGBN/bFisHmwvLOG9lT9RWFJOttcupDTGxB8LLvVg2aYCbnh9Md/vKA4sZGmMMfHEgks9yMmoPAW51NYXM8bEJQsu9SA4uFSuL6aqjdwqY4xpOBZc6kHlKch5hc4dKUvK/ewta5AV/40x5pBgwaUeeJISyUxLtgspjTFxy05FricvXXUcbZqnsnZbIeAElyOy0hu5VcYY0zAsuNSTAe61LruKnTPFbH0xY0w8seBST5b8uIv124s4uVsOAPnFNixmjIkfNudST/6zfCv3vbOKVunusvvWczHGxBELLvUkJ8PDvvIKyiv8ZKYl24S+MSauWHCpJzne4AspbQkYY0x8seBSTwIXUrqnI+fbEjDGmDhiwaWeVFkCJsOWgDHGxBc7W6yeHJmTzpw7T6FdizS+3pBPngUXY0wcsZ5LPfEkJXJkjpfU5ESyvR4KS3yUlNsSMMaY+NDkey4ikg48B5QBn6jqpIaqe8qCH2melky2O0S2s7iMdi3SGqp6Y4xpNFH1XESko4jMFZHVIrJKRG6roWyiiPxXRP4dZZ0vish2EVkZkj5cRL4VkfUiMj4oazQwTVWvA86Lpu66eu2rH5i2ONfWFzPGxJ1oh8V8wJ2q2hMYBNwkIj0jlL0NWBMuQ0Rai0hGSFrXCPt5GRgeUjYReBYYAfQExga1owOwyX3eoONSORkeZ9l9d5VkCy7GmHgRVXBR1a2qusR9XogTPNqHlhORDsDZwD8j7OoU4G0R8bjlrwP+FqHOecDOkOTjgfWqukFVy4ApwCg3LxcnwECY4xWRc0VkYkFBQcTjPFg5Xie4VF7zYlfpG2PiRcwm9EWkE9Af+DpM9pPAXYA/3Laq+ibwPjBVRC4HrgYuqkP17dnfOwEnoFQGuenAhSIyAZgVpu5Zqnp9ZmZmHaqrnRz3FORWzdyei60vZoyJEzGZ0BcRL/AWcLuq7gnJOwfYrqqLReTUSPtQ1cdEZAowAeiiqkWxaJuqFgNXxWJfdZWT4cHnV0or/KSnJFrPxRgTN6LuuYhIMk5gmaSq08MUGQKcJyIbcYarTheR18PsZyjQG5gB3FfHZmwGOga97uCmNapLjuvI6gfPolV6Clleu5DSGBM/oj1bTIAXgDWq+kS4Mqr6v6raQVU7AZcCc1T1lyH76Q9MxJknuQrIEpGH6tCUhUA3EeksIiluPTPrfEAx1iwliWYpTucw25tiy+4bY+JGtD2XIcA4nN7IUvcxEkBEZotIu1rupxlwsap+p6p+4Argh3AFRWQy8CVwtIjkisg1quoDbsaZt1kDvKGqq6I7tOgV7CvnoX+vZuHGnWR7PTYsZoyJG1HNuajq54BEyBsZJu0T4JMw6fNDXpcDz0fY79gI6bOB2Qdqc0NKEPjn59/TurmH7AwPi3/Y1dhNMsaYBmHLv9QjryeJ1OQEdhSVkZ2ews69Zfgqwp4wZ4wxTYoFl3okIoELKbMzPKjCrr3ljd0sY4ypdxZc6lnlhZS2BIwxJp5YcKlnORke9pb5LLgYY+JKk18VubFNuPxYEhKE7/Kca0ItuBhj4oH1XOpZQoJzMl1lz8Vud2yMiQcWXOrZoo07uelfSygtryAlMcHuSGmMiQsWXOrZzuIy/rN8Kz/tKSHbm2IXUhpj4oIFl3qWk7F/It/WFzPGxAsLLvWsMrg4pyPb+mLGmPhgwaWeVQ0utr6YMSY+WHCpZ56kRDpnpwOQ5fWQX1yKqjZyq4wxpn7ZdS4NYO5vTgXgn59toLxCKdhXTgv37pTGGNMUWc+lAe2f3LehMWNM02bBpQH887MN3DRpiS0BY4yJGxZcGsCW3SXM/XY7WV5nKMyCizGmqbPg0gCcxSsraJaSCMCOQgsuxpimzYJLA2jtzrX4KpQEgfxim3MxxjRtFlwaQOVE/s7iMlql21X6xpimr0mfiiwi6cBzQBnwiapOaox2tGuRSq92zVEg25tCnl1IaYxp4g7YcxGRjiIyV0RWi8gqEbktTJlUEVkgIsvcMg8E5W0UkRUislREFkXTWBF5UUS2i8jKkPThIvKtiKwXkfFBWaOBaap6HXBeNHVHo2vrDP5z61CO69SKbPdCSmOMacpqMyzmA+5U1Z7AIOAmEekZUqYUOF1V+wL9gOEiMigo/zRV7aeqA8NVICKtRSQjJK1rmKIvA8NDyiUCzwIjgJ7A2KD2dQA2uc8rajzKBpLtTbFhMWNMk3fA4KKqW1V1ifu8EFgDtA8po6pa5L5Mdh91WePkFOBtEfEAiMh1wN/CtGUesDMk+XhgvapuUNUyYAowys3LxQkwEOFYReRcEZlYUFBQh+bW3dUvL+TR976x9cWMMXGhThP6ItIJ6A98HSYvUUSWAtuBD1W1sowCH4jIYhG5Ptx+VfVN4H1gqohcDlwNXFTLZrVnf+8EnIBSGfymAxeKyARgVoS6Z6nq9ZmZmbWs7uD8VFDC2p8KyfJ62FdeQXGpr17rM8aYxlTrCX0R8QJvAber6p7QfFWtAPqJSAtghoj0VtWVwEmqullEWgMfisg3bg8kdPvHRGQKMAHoEtQTOmiqWgxcFe1+YiEnw0NekbPsPji3O073NOnzKYwxcaxWPRcRScYJLJNUdXpNZVV1NzAXd25EVTe7P7cDM3CGscLVMRTo7Za5r3bNB2Az0DHodQc37ZCSk+Fxlt2vXILf5l2MMU1Ybc4WE+AFYI2qPhGhTI7bY0FE0oBhwDcikl45Ue+eFnwmsDLM9v2BiThzJVcBWSLyUC2PYSHQTUQ6i0gKcCkws5bbNpicDOf6lqx0WwLGGNP01abnMgQYB5zunk68VERGAojIbBFpB7QF5orIcpwv+w9V9d9AG+BzEVkGLAD+o6rvhamjGXCxqn6nqn7gCuCH0EIiMhn4EjhaRHJF5BpV9QE348zZrAHeUNVVdXoXGkDPts05uVsOGanOUJgFF2NMU3bAQX9V/RyQCHkj3adbcCb6Q/M3AH1rUcf8kNflwPNhyo2NsP1sYPaB6mlM5/Ztx7l921Hm8wPOnIsxxjRVtvxLA0tJSiAzLdl6LsaYJs2CSwPJ3bWXE/70ETOXbbELKY0xTZ4FlwbSPC2ZbXtK2VZQQpZdSGmMaeIsuDSQDE8SnqQE8opKyfF62GHrixljmjALLg1ERJzTkQudCynthmHGmKbMgksD2n+Vvoc9JT5KfYfEWprGGBNztv5IAxrWsw1+v9Iq3blKP7+ojHYt0hq5VcYYE3sWXBrQr0917iLwwaqfAAsuxpimy4bFGpivwk+W15aAMcY0bRZcGtAbizbR7Z53EXfBA1u80hjTVFlwaUCZacmoQoU6S8BYz8UY01RZcGlArd3l9gtLfDRLSbT1xYwxTZYFlwaU4waXHYVlzu2OredijGmiLLg0oGzv/huFZdn6YsaYJsyCSwNKTU7k+pOPpE/7TLK9HhsWM8Y0WRZcGtjvR/bg5KNybFjMGNOk2UWUDazM56ewpJwcbwo7i8uo8CuJCWHvxWaMMT9b1nNpYHe8sZQLJ3xBlteDX2FnsQ2NGWOaHgsuDSwnw0NeYWlgcj/flt43xjRBFlwaWOuMVIrLKvB6EgHspmHGmCapyc+5iEg68BxQBnyiqpMasz2V17okuPMsNqlvjGmKouq5iEhHEZkrIqtFZJWI3BamTKqILBCRZW6ZB6Ks80UR2S4iK0PSh4vItyKyXkTGB2WNBqap6nXAedHUHQuVwcXvrABjwcUY0yRFOyzmA+5U1Z7AIOAmEekZUqYUOF1V+wL9gOEiMii4gIi0FpGMkLSuEep8GRgeUjYReBYYAfQExga1owOwyX3e6HfnOqqNl98N786ROemkJCaww651McY0QVEFF1XdqqpL3OeFwBqgfUgZVdUi92Wy+9CQXZ0CvC0iHgARuQ74W4Q65wE7Q5KPB9ar6gZVLQOmAKPcvFycAANhjldEzhWRiQUFBQc63Jhom5nGjad2oWOrZnaVvjGmyYrZhL6IdAL6A1+HyUsUkaXAduBDVa1SRlXfBN4HporI5cDVwEV1qL49+3sn4ASUyiA3HbhQRCYAs0I3VNVZqnp9ZmZmHaqLzqade9lasM8upDTGNFkxmdAXES/wFnC7qu4JzVfVCqCfiLQAZohIb1VdGVLmMRGZAkwAugT1dqKiqsXAVbHYV6xc8NwXDOvZ2nouxpgmK+qei4gk4wSWSao6vaayqrobmEvInIm7n6FAb2AGcF8dm7EZ6Bj0uoObdkgKvtbF1hczxjRF0Z4tJsALwBpVfSJCmRy3x4KIpAHDgG9CyvQHJuLMk1wFZInIQ3VoykKgm4h0FpEU4FJgZh0Pp8GEBhfV0CkoY4z5eYu25zIEGAecLiJL3cdIABGZLSLtgLbAXBFZjhMEPlTVf4fspxlwsap+p6p+4Argh3AVishk4EvgaBHJFZFrVNUH3Iwzb7MGeENVV0V5bPUmx1sZXFIoq/CzZ5+vsZtkjDExFdWci6p+DoRddVFVR7pPt+BM9Ne0n/khr8uB5yOUHRshfTYw+wBNPiTkZHjIKyqlTXPnmpcFG3cyrGebRm6VMcbETpO/Qv9QdHaftnQ/LIPTjm7NUW283PP2Co7v1IrMZsmN3TRjjIkJW1usEfTpkMn5/dvjTU3miYv7kV9Uxr0zVx54Q2OM+Zmw4NII9pVVsOD7newoKqV3+0xuOb0b7yzdwuwVWxu7acYYExMWXBrB5t37uPgfXzJ//Q4Afn1aF47pkMndM1awvbCkkVtnjDHRs+DSCCoXr8wrdC6gTE5M4ImL+1JcVsHvp6+wU5ONMT97FlwaQfPUJFKSEgLBBaBr6wzuOutoPlqznTcX5zZi64wxJnoWXBqBiASudQl29ZDOnNC5FQ/OWk3urr2N1DpjjImeBZdGUnmtS7CEBOHxi/qiqvz2zeX4/TY8Zoz5ebLg0kh+P7IHvz3r6GrpHVs1495ze/Llhnxe+XJjwzfMGGNiwIJLIzm+cyuO6dAibN7FAztyevfWPPLuN6zfHpPFoY0xpkFZcGkkm3buZeayLfgq/NXyRIRHRvchLSWRO99cFraMMcYcyiy4NJJP1+Zx6+T/kl8cfsn91s1Teej83izbtJsJn3zXwK0zxpjoWHBpJKHXuoRzzjHtOLdvO576eB0rNzfMbZiNMSYWLLg0ktoEF4A/jupFq/QU7nxjGaW+ioZomjHGRM2CSyPJ8brB5QC3OW7RLIVHLzyGb7cV8sSHaxuiacYYEzULLo2ktj0XgNO6t2bs8R2ZOG8DizburO+mGWNM1Cy4NJLU5ETeunEwlxzXsVbl7z67Jx1apnHnm8soLrU7VxpjDm0WXBrRsUe0JNsdHjsQryeJx8f05cede/m/d9fUc8uMMSY6Flwa0Wfr8nhn6eZalz/hyCyuGdKZ17/6kXlr8+qxZcYYEx0LLo1o6sJNPPnRujpt85uzjqZray93TVtOwd7yemqZMcZEp0kHFxFJF5FXROR5Ebm8sdsTKiej+srIB5KanMgTF/clr6iU+2etqqeWGWNMdJJqU0hEOgKvAm0ABSaq6lO1LSMiG4FCoALwqerAg22wiLwInANsV9XeQenDgaeAROCfqvoIMBqYpqqzRGQqMOlg660PORkeikp97CurIC0lsdbbHdOhBTef1pWnPl7Hx2u2kZggJCYICVL1p/OcQFogP0FIjJCeIJAoQc9D9us8x3meIE5ZwS0fXC50/86yNonuNhKc79aVIDXnVdtvhDyR/cceyEvA3XdIXuUxJOyvPyH4dUL1fRtjDqxWwQXwAXeq6hIRyQAWi8iHqrq6DmVOU9Ud4XYuIq2BfapaGJTWVVXXhyn+MvAMTiCrLJsIPAsMA3KBhSIyE+gArHCLHXJXIFZe63LL5CX881fHAXD+s/PZWrAv8AWfmCCc3C2HP57vxNGxE7+isLScBBHat0ijvMJPuxZp9GzXHL9f+XRtHj6/H58fQAAlO91D+5ZpVPj9rNyyB4Dgm102T02ieVoyvgplS8E+FECd/xBUldTkRDxJCZT7/RTu8wXS1S2TIIIAFX4/ZX43XZWmeseAsAEyJOiFD7r7A5YEB+nQwBgU3EK3PdA/ApXbO88Jeu7UlxjcvkDb9rcr3D8RwelJCfu3T0yoGpxD0yq3q/IPT2XZoDZU/kyq9k8SFsx/xmoVXFR1K7DVfV4oImuA9sDqupSpwSnADSIyUlVLReQ6nF7HiDBtmScinUKSjwfWq+oGABGZAozCCTQdgKUcgkOAw3q24dendqFdi7RA2uAuWeQXlVGhit+vVKhyRFazQH6b5h7SShKp8CvNU5Op8CsndcvmptO6AnDR37+g1Oenwu98ufv9yvDeh3HLGd0o8/k544lP8PvBr+qWUUb1a88tZ3Rj994yTvy/OfhVg/Lht2cdyU2ndWXTzr0MfWxuteO4/9yeXDmkM9/+VMhZT86rlv/ohX24oH8HFv+wk7HPf10t/5HRfTi9e2u++G4Ht09dVi3/Txf0ZsARLZm3dgd/ml39TLkHz+tFtzYZfPLtdv4xb0O1/HvO7kG7FmnM+WY70xbnIoC4X1wJArefcRSZzZL5fP0OPv02DxFIEADnC/pXJ3YiJSmBRRt3ssJdhkfcgCoCZ/dphwis3FzADzv3IuC+h5CgyqAjs6hQ+C6viPzKi2YVcANGtzYZ+P3K5t37KCrZf5q5oiSKkNPcg98PO4vLKPNVuMHd3YVAekoSFX5lb5kv8DutUsZ97vP7f3YBPzggJSUkBIJoYkICiZVpQYGtes+9elCrmhYaKAkbPEODdeg/EMH/DAb/E1E14FM9+Af+UaksH6ZMlX8Qqu6/ykhCxH88qh9/QwRwqev92t0v9nlAb1XdU5syIvI9sAvns/4PVZ0YZpu7gMHAm8DNwDBVDbvevLv/f1cOi4nIGGC4ql7rvh4HnAD8DqeXUwJ8rqrVhsVE5Fzg3K5du163bl3dJtfjQeXnQ0Tw+5V95RWBwOdX50s0PSWJtJREynx+theWoEqV4JST4SEzLZm9ZT425BUHvnj97n6OzPHSKj2FXcVlrNxSEAiKlfvof3hLcjI8bNm9j4Ubd1bZv6pzkWlOhof124uYv35H4Au2sp6Lju1AltfD0k27mfvNdlSdoF3hd47vxlO70KJZCp+uzePjNduqBGa/Kvef14t0TxLvLN3MB6u3Odv792//j3HHkpSYwMvzv+f9VduqBOcEEabdOBiAv3zwLR+4+RVuz8/rSWLWLScBMP6t5cz5Zvv+90aVw5qn8t7tJwNw9csL+WxdXqB9AN0PywjkX/DcfP774+4qv78Bh7dg+q+HAHDmXz9l7baqf1JDumTxjysGUuFXznpyHj8VlFTJP+WoHB44rxcVqpz/zHwKQ66xOvXoHH59alcq/MovX/iaipDodepROYwZ2IHS8gp+O22508sN6oWd2CWLod2yKSr18fy8DU7QFtzALfTtkMnRhzWnqKSc91dvI/hrUIAurb20aZ5KYUk5KzYXIAiKun12aNM8lYzUJIpLfWzZXRLodTufbafXnpyUQEl5BXuCe+Xuz+RE53/S8gqlvMIf6JEHftb85/OzIALPjxvIL3q2OcjtZXG4qY46BRcR8QKfAg+r6vTalhGR9qq62R3++hC4RVWr/Yvr9jhGAl1UNeK5trUNLqp6c22PbeDAgbpo0aLaFjemUVUOS/pVSXK/APeW+Siv0P2BWZ2eT5Y7/Lp59z5KyisCPeIKv9IsJYnO2ekALNu02/nnwe8GT1VyvB56t88E4INVP1Hq8wcCp8+vdMpK5/jOrQB49cuN+Cq0Sn6f9pmcfFQO5RV+/vrh2v09cr8zjDq0Ww6/6NmGolIf989cFWibz++UG9WvHcN7t2X7nhLufHNZYN+Vj+uGHsmIPm35Lq+IX7++JNA7q8y/5+wejOjTlv/+uItrXlm0/x8Pt56nL+3Pmb0OY+6327nqpYXV3ufXrzmBk7pl8+/lW7j5X/+tlj/914Pp16EFUxb+yO9nrKyW/8b/DKJTdjqTv/6Rv4Y5M/T1a44ny+vhX1//wGtf/Vgt//krBtIsJZHJC37k38u3Vst/+tL+IPDGwk3M/24HCW5vWwRSEhMYP6I7FX7lPyu2snrrnkA+QFpKIpced3hg9KJra2/Yz9qBRB1cRCQZ+Dfwvqo+EUWZ+4EiVX08JH0oMAFYDBTWFBjCBJcTgftV9Sz39f8CqOr/1ergsOBiTLwL9EhV8fuhQpXUpASSEp2eTWGJr0pw86vSpnkqqcmJ7N5bxtaCkkBvujJ492zbnLSURLbs3sf3O4qD9u/0fIZ2yyY1OZG12wpZs3WPu//9veYLBrTHk5QYGJIN3r9flRtO7kJCgvDR6m0s3bQ7kO73KwkJwv+O6AHAG4s28d8fd1XplaelJPLwBX2ift+iCi7iDMq9AuxU1dvrUkZE0oEEdx4mHafn8qCqvhdUpj/wL5yzwL7HOavrO1W9J0JdnagaXJKAtcAZwGZgIXCZqtb6XF0LLsYYU3eRgkttJ7mHAOOA00VkqfsY6e54toi0q6FMG+BzEVkGLAD+ExxYXM2Ai1X1O1X1A1cAP0Q4kMnAl8DRIpIrIteoqg9nnuZ9YA3wRl0CizHGmNiq84R+U2U9F2OMqbtoey7GGGNMrVlwMcYYE3MWXIwxxsScBRdjjDExZ8HFGGNMzFlwMcYYE3N2KrJLRPKIcG1NLWQDYVd8bmTWrrqxdtWNtavuDtW2RdOuI1Q1JzTRgksMiMiiaO5RU1+sXXVj7aoba1fdHaptq4922bCYMcaYmLPgYowxJuYsuMRGtfvTHCKsXXVj7aoba1fdHapti3m7bM7FGGNMzFnPxRhjTMxZcDHGGBNzFlzqQESGi8i3IrJeRMaHyfeIyFQ3/2v3pmb13aaOIjJXRFaLyCoRuS1MmVNFpCDoPjv31ne73Ho3isgKt85q9zMQx9Pu+7VcRAY0QJuODnoflorIHhG5PaRMg7xfIvKiiGwXkZVBaa1E5EMRWef+bBlh21+5ZdaJyK8aoF1/FpFv3N/TDBFpEWHbGn/n9dCu+0Vkc+h9psJsW+Pfbj20a2pQmzaKyNII29bn+xX2u6HBPmPOvbjtcaAHkAh8BxwJpADLgJ4hZX4N/N19fikwtQHa1RYY4D7PwLkjZ2i7TsW5c2dDv2cbgewa8kcC7wICDAK+boTf6U84F4E1+PsFnAwMAFYGpT0GjHefjwceDbNdK2CD+7Ol+7xlPbfrTCDJff5ouHbV5ndeD+26H/hNLX7PNf7txrpdIfl/Ae5thPcr7HdDQ33GrOdSe8cD61V1g6qWAVOAUSFlRuHc6hlgGnCGiEh9NkpVt6rqEvd5Ic6dONvXZ50xNAp4VR1fAS1EpG0D1n8Gzu20D3Zlhqio6jxgZ0hy8GfoFeD8MJueBXyoqjtVdRfOrcOH12e7VPUDde74CvAV0CFW9UXTrlqqzd9uvbTL/fu/GJgcq/pqq4bvhgb5jFlwqb32wKag17lU/xIPlHH/EAuArAZpHeAOw/UHvg6TfaKILBORd0WkVwM1SYEPRGSxiFwfJr8272l9upTIf/SN8X4BtFHVre7zn3BuEx6qsd+3q3F6nOEc6HdeH252h+tejDDE05jv11Bgm6qui5DfIO9XyHdDg3zGLLg0ESLiBd4CblfVPSHZS3CGfvoCfwPebqBmnaSqA4ARwE0icnID1XtAIpICnAe8GSa7sd6vKtQZnzikrhUQkbsBHzApQpGG/p1PALoA/YCtOENQh5Kx1Nxrqff3q6bvhvr8jFlwqb3NQMeg1x3ctLBlRCQJyATy67thIpKM8+GZpKrTQ/NVdY+qFrnPZwPJIpJd3+1S1c3uz+3ADJzhiWC1eU/rywhgiapuC81orPfLta1yaND9uT1MmUZ530TkSuAc4HL3S6maWvzOY0pVt6lqhar6gecj1NdY71cSMBqYGqlMfb9fEb4bGuQzZsGl9hYC3USks/tf76XAzJAyM4HKsyrGAHMi/RHGijum+wKwRlWfiFDmsMq5HxE5Huf3Xq9BT0TSRSSj8jnOhPDKkGIzgSvEMQgoCOqu17eI/1E2xvsVJPgz9CvgnTBl3gfOFJGW7jDQmW5avRGR4cBdwHmqujdCmdr8zmPdruA5ugsi1Febv9368AvgG1XNDZdZ3+9XDd8NDfMZq4+zFJrqA+fsprU4Z57c7aY9iPMHB5CKM8yyHlgAHNkAbToJp1u7HFjqPkYCNwA3uGVuBlbhnCXzFTC4Adp1pFvfMrfuyvcruF0CPOu+nyuAgQ30e0zHCRaZQWkN/n7hBLetQDnOmPY1OHN0HwPrgI+AVm7ZgcA/g7a92v2crQeuaoB2rccZg6/8jFWeFdkOmF3T77ye2/Wa+9lZjvOl2Ta0Xe7ran+79dkuN/3lys9UUNmGfL8ifTc0yGfMln8xxhgTczYsZowxJuYsuBhjjIk5Cy7GGGNizoKLMcaYmLPgYowxJuYsuBhjjIk5Cy7GGGNi7v8D94UHd4DevLAAAAAASUVORK5CYII="
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![download.png](attachment:download.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is quite good for considering that we only used logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: Training a Multi-layer Network (3 Points)\n",
    "\n",
    "It would have been silly to do all of this work to train logistic regression.\n",
    "Something you probably already would have been able to do before the start of this course.\n",
    "The real goal is to train *deep* networks with multiple layers.\n",
    "You probably can't wait to build a convolutional network with your framework!\n",
    "\n",
    "> Use one of the optimisers from above to train a multi-layer convolutional neural network on the MNIST dataset. \n",
    "> Feel free to also create new modules and try out new things! (make sure to include any new code in the notebook!).\n",
    "> Achieve a model with 80% accuracy to collect all points. \n",
    "> For reference: the sample solution (using the LeNet architecture illustrated below) takes &approx;15&nbsp;min to train for 10&nbsp;epochs.\n",
    "\n",
    "**Hint:** You can probably reuse a few things from the previous exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <figcaption style=\"width: 100%; text-align: center;\">LeNet-5 architecture</figcaption>\n",
    "    <img src=\"https://miro.medium.com/max/2154/1*1TI1aGBZ4dybR6__DI9dzA.png\" />\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Network architecture\n",
    "cnn = Sequential(\n",
    "    Conv2d(in_channels=x_train.shape[1], out_channels=6, kernel_size=(5,5), use_bias=True),\n",
    "    # output_size = input_size - kernel_size + 1 = 32 - 5 + 1 = 28\n",
    "    \n",
    "    ELU(), # as we implemented it in assignment 3\n",
    "    \n",
    "    MaxPool2d(kernel_size=(2,2)), # keep only half of the size; in_size=28x28 out_size=14x14\n",
    "    \n",
    "    Conv2d(in_channels=6, out_channels=16, kernel_size=(5,5), use_bias=True),\n",
    "    # in_size = 14 --> 14 + 1 - 5 = 10 = out-size --> kernel_size1=5.\n",
    "    # the in_channels are the out_channels from the previous layer\n",
    "    \n",
    "    ELU(),\n",
    "    \n",
    "    MaxPool2d(kernel_size=(2,2)), # keep only half of the size; in_size=10x10 out_size=5x5\n",
    "    \n",
    "    \n",
    "    # We have channels=16, in_size=5x5. Next we want to transform the layer such that we can use linear\n",
    "    # layer. Therefore, get 120 output units in total. Only Conv2D can change the channel\n",
    "    # number from the used modules.:\n",
    "    Conv2d(in_channels=16, out_channels=120, kernel_size=(5,5), use_bias=True),\n",
    "    ELU(),\n",
    "    # Bring the output units to one level such that they can be used for the Linear Layer which only \n",
    "    # assumes one channel:\n",
    "    Flatten(),\n",
    "    \n",
    "    Linear(in_features=120, out_features=84),\n",
    "    \n",
    "    AlgebraicSigmoid(), # used in logistic regression\n",
    "    \n",
    "    Linear(in_features=84, out_features=10)\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "     \n",
    "loss = LogitCrossEntropy(reduction='none')\n",
    "optimiser = GradientDescent(cnn.parameters(), lr = 1e-5)\n",
    "# at this place we connect the optimizer with the parameters of the CNN\n",
    "\n",
    "# This time x keeps its channel:\n",
    "train_data, valid_data = split_data(x_train,y_train)\n",
    "\n",
    "# limit dataset for computational weak devivces:\n",
    "#train_data = (train_data[0][:10000], train_data[1][:10000])\n",
    "#valid_data = (valid_data[0][:200], valid_data[1][:200])\n",
    "\n",
    "train_data_x_cnn, train_data_y_cnn = train_data\n",
    "val_data_x_cnn, val_data_y_cnn = valid_data\n",
    "\n",
    "\n",
    "batch_size = 32 # 4\n",
    "train_loader_cnn = Dataloader(train_data_x_cnn, train_data_y_cnn, batch_size, shuffle=True, seed=0)\n",
    "val_loader_cnn = Dataloader(val_data_x_cnn, val_data_y_cnn, batch_size, shuffle=True, seed=0)\n",
    "\n",
    "train_err, valid_err = train(train_loader_cnn, val_loader_cnn, cnn, loss, optimiser, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curves(cnn, train_err, valid_err);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unluckily, the accuracy is quite low here, however, I want to make you aware that my computational power on my device is quite limited. Therefore, I was only able to use a low epoch number (3) and only a part of the dataset for training. Also the batch size is quite limited (4). Else my computer would stop working at all.<br>\n",
    "For you I change my small code hyperparameters into the computational more heavy ones. However, because of my limitations I was not able to find out the perfect hyperparameter matches."
   ]
  },
  {
   "attachments": {
    "download.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![download.png](attachment:download.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
