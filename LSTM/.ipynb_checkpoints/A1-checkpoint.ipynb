{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Introduction to the Fully Recurrent Network\n",
    "\n",
    "\n",
    "## Exercise 1: Numerical stability of the binary cross-entropy loss function\n",
    "\n",
    "We will use the binary cross-entropy loss function to train our RNN, which is defined as \n",
    "$$\n",
    "L_{\\text{BCE}}(\\hat y, y) = -y \\log \\hat y - (1-y) \\log (1-\\hat y),\n",
    "$$\n",
    "where $y$ is the label and $\\hat y$ is a prediction, which comes from a model (e.g. an RNN) and is usually sigmoid-activated, i.e., we have\n",
    "$$\n",
    "\\hat y = \\sigma(z) = \\frac{1}{1+e^{-z}}.\n",
    "$$\n",
    "The argument $z$ is called *logit*. For reasons of numerical stability it is better to let the model emit the logit $z$ (instead of the prediction $\\hat y$) and incorporate the sigmoid activation into the loss function. Explain why this is the case and how we can gain numerical stability by combining the two functions $L_{\\text{BCE}}(\\hat y, y)$ and $\\sigma(z)$ into one function $L(z, y) = L_{\\text{BCE}}(\\sigma(z), y)$. \n",
    "\n",
    "*Hint: Prove that $\\log(1+e^{z}) = \\log (1+e^{-|z|}) + \\max(0, z)$ and argue why the right-hand side is numerically more stable. Finally, express $L(z,y)$ in terms of that form.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------- Solution -----------------------<br>\n",
    "sigmoids e problematic. e^x increases exponentially as soon as x becomes > 0.<br>\n",
    "cross entropy with y_pred = 1/1+e^(-z) needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "$$\n",
    "\\text{Prove that:} \\log(1+e^{z}) = \\log (1+e^{-|z|}) + \\max(0, z) \\\\\\\\\n",
    "\\text{Case z<0: } \\log(1+e^{-z}) = \\log(1+e^{-|z|}) + 0 = \\log(1+e^{-|z|}) + max(0,z)\\\\\n",
    "\\text{Case z=0: } \\log(1+e^{0}) = \\log(1+e^{-|0|}) + 0 = \\log(1+e^{-|0|}) + max(0,0)\\\\\n",
    "\\text{Case z>0: }  \\log(1+e^{z}) = \\log (1+e^{-|z|}) + \\max(0, z) \\\\\n",
    " e^{\\log(1+e^{z}) }= e^{\\log (1+e^{-|z|}) + \\max(0, z)} \\\\\n",
    " 1+e^{z} = e^{\\log (1+e^{-|z|})} \\cdot e^{ \\max(0, z)}\\\\\n",
    " 1+e^{z} = (1+e^{-z}) \\cdot e^{z}\\\\\n",
    " 1+e^{z} = e^{z} + e^{z-z}\\\\\n",
    " 1+e^{z} = e^{z} + 1\\\\\n",
    " \\Box\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "argue why the right-hand side is numerically more stable:<br>\n",
    "log is suited to handle small numbers (close to 0). However, we use sigmoid activation function here which has the majority input values between -5 and 5 which are not small numbers. Therefore, $\\log(1+e^z)$ gets very high values through $e^z$ whereas $log(1+1/e^{|z|})$ gets smaller inside values and can therefore produce more stable solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Side note:<br>\n",
    "Showing that the equation gives us the $\\hat{y}=\\sigma(z)$:<br>\n",
    "$$\n",
    "\\text{We have: } \\log(1+e^{z}) = \\log (1+e^{-|z|}) + \\max(0, z) \\\\\n",
    "\\max(0, z) = \\log(1+e^{z}) - \\log (1+e^{-|z|}) \\\\\n",
    "\\max(0, z) = \\log(1+e^{z}) + \\log ((1+e^{-|z|})^{-1})\\\\\n",
    "\\max(0, z) = \\log(1+e^{z}) + \\log \\left(\\frac{1}{1+e^{-|z|}}\\right)\\\\\n",
    "\\max(0, z) = \\log(1+e^{z}) + \\log(\\sigma(z))\\\\\n",
    " \\log(\\sigma(z)) = \\max(0, z) - \\log(1+e^{z}) \\\\\n",
    "\\sigma(z) = e^{\\max(0, z) - \\log(1+e^{z}) }\\\\\n",
    "\\sigma(z) = \\frac{e^{\\max(0, z)}} { e^{\\log(1+e^{z}) }}\\\\\n",
    "\\sigma(z) = \\frac{e^{\\max(0, z)}} { 1+e^{z}} ?= \\hat{y} = \\frac{1}{1+e^{-z}} = \\frac{1}{1+\\frac{1}{e^{z}}}\n",
    "=  \\frac{1}{\\frac{e^{z}+1}{e^z}} =  \\frac{e^z}{e^{z}+1}\\\\\n",
    "\\text{Given for z>=0}\n",
    "\\\\\n",
    "\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Finally, express  ùêø(ùëß,ùë¶)  in terms of putting $\\sigma(z)$ in it:<br>\n",
    "$$\n",
    "\\text{Put it into loss function: } L(z,y) = -y \\cdot \\log \\left(\\frac{e^{\\max(0, z)}} { 1+e^{z}}\\right) - (1-y) \\cdot \\log \\left(1-\\frac{e^{\\max(0, z)}} { 1+e^{z}}\\right)\\\\\n",
    "\\\\\n",
    "= -y \\left(  \\log(e^{\\max(0, z)}) - \\log \\left( 1+e^{z}\\right)  \\right) \n",
    "- \\log \\left(1-\\frac{e^{\\max(0, z)}} { 1+e^{z}}\\right)\n",
    "+ y \\cdot \\log \\left(1-\\frac{e^{\\max(0, z)}} { 1+e^{z}}\\right)\n",
    "\\\\\n",
    "= -y \\cdot max(0,z) + y \\cdot \\log \\left( 1+e^{z}\\right)\n",
    "- \\log \\left(1-\\frac{e^{\\max(0, z)}} { 1+e^{z}}\\right)\n",
    "+ y \\cdot \\log \\left(1-\\frac{e^{\\max(0, z)}} { 1+e^{z}}\\right)\n",
    "\\\\\n",
    "\\text{We have with z>=0: } 1-\\frac{e^{\\max(0, z)}} { 1+e^{z}} = \\frac{1+e^{z} -e^{\\max(0, z)}} { 1+e^{z}}\n",
    "=  \\frac{1} { 1+e^{z}}\\\\\n",
    "\\text{So we can use:} L(z,y) =  -y \\cdot max(0,z) + y \\cdot \\log \\left( 1+e^{z}\\right)\n",
    "+ \\log( 1+e^{z}) - y \\cdot \\log( 1+e^{z}) \\\\\n",
    " =  -y \\cdot max(0,z) + \\log( 1+e^{z})\\\\\n",
    "\\text{Put in forumla which we proved: } \\\\\n",
    "=  -y \\cdot max(0,z) + \\log( 1+e^{-|z|}) + max(0,z)\\\\\n",
    "\\text{When first putting in the $\\sigma(z)$ from $\\hat{y}$ we get: } \\\\\n",
    "=  -y \\cdot z + \\log( 1+e^{-|z|}) + max(0,z)\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Derivative of the loss\n",
    "\n",
    "Calculate the derivative of the binary cross-entropy loss function $L(z, y)$ with respect to the logit $z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "$$\n",
    "\\frac{\\partial L(z,y)}{\\partial z} = \\frac{\\partial -yz}{\\partial z} \n",
    "+ \\frac{\\partial \\log(1+e^{-|z|})}{\\partial z} \n",
    "+\\frac{max(0,z)}{\\partial z} \\\\ \n",
    "= -y + \\frac{1}{1+e^{-|z|}} \\cdot (- e^{-|z|}) + \\frac{max(0,z)}{z} \\text{    # max(0,z) derivative can be only 0 or 1}\\\\\n",
    "= -y - \\frac{1}{(1+e^{-|z|} )\\cdot  e^{|z|} }  + \\frac{max(0,z)}{z}\\\\\n",
    "= -y - \\frac{1}{e^{|z|} + 1 }  + \\frac{max(0,z)}{z}\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Initializing the network\n",
    "Consider the fully recurrent network\n",
    "$$\n",
    "s(t) = W x(t) + R a(t-1) \\\\\n",
    "a(t) = \\tanh(s(t)) \\\\\n",
    "z(t) = V a(t) \\\\\n",
    "\\hat y(t) = \\sigma(z(t))\n",
    "$$\n",
    "for $t \\in \\mathbb{N}, x(t) \\in \\mathbb{R}^{D}, s(t) \\in \\mathbb{R}^{I}, a(t) \\in \\mathbb{R}^{I}, z(t) \\in \\mathbb{R}^K, \\hat y(t) \\in \\mathbb{R}^K$ and $W, R, V$ are real matrices of appropriate sizes and $\\hat a(0) = 0$. \n",
    "\n",
    "*Compared to the lecture notes we choose $f(x) = \\tanh(x) = (e^x - e^{-x})(e^x + e^{-x})^{-1}$ and $\\varphi(x) = \\sigma(x) = (1+e^{-x})^{-1}$. Further, we introduced an auxiliary variable $z(t)$ and transposed the weight matrices.*\n",
    "\n",
    "Write a function `init` that takes a `model` and integers $D, I, K$ as arguments and stores the matrices $W, R, V$ as members `model.W`, `model.R`, `model.V`, respectively. The matrices should be `numpy` arrays of appropriate sizes and filled with random values that are uniformly distributed between -0.01 and 0.01. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from scipy.special import expit as sigmoid\n",
    "\n",
    "class Obj(object):\n",
    "    pass\n",
    "\n",
    "model = Obj()\n",
    "T, D, I, K = 10, 3, 5, 1\n",
    "\n",
    "def init(model, D, I, K):\n",
    "    ########## YOUR SOLUTION HERE ##########\n",
    "    \n",
    "    # W as input weight matrix\n",
    "    # R as recurrent weight matrix\n",
    "    # V as output weight matrix\n",
    "    \n",
    "    model.W = np.random.uniform(low=-.01,high=.010000000000000001,size=(I,D)) \n",
    "    # the high values is excluded. To reach 0.01 as possible value we have a look at the number of decimal \n",
    "    # places of the outputed number, it's 18. Therefore, mark the 18-th decimal place of the high value as one\n",
    "    # such that high=.010000000000000001 is excluded but .01000000000000000 is still included.\n",
    "    \n",
    "    # reason size: we have s(t) = W x(t) + ... where x(t) has shape Dx1 and s(t) with shape Ix1\n",
    "    \n",
    "    \n",
    "    model.R = np.random.uniform(low=-.01,high=.010000000000000001,size=(I,I))\n",
    "    # reason size: s(t) = ... + R a(t-1) where a(t-1) with shape Ix1 and s(t) with shape Ix1\n",
    "    \n",
    "    model.V = np.random.uniform(low=-.01,high=.010000000000000001,size=(K,I))\n",
    "    # reason size: z(t) = V a(t) where a(t) with shape Ix1 and z(t) with shape Kx1\n",
    "    \n",
    "    return\n",
    "\n",
    "Obj.init = init\n",
    "model.init(D, I, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: The forward pass\n",
    "Implement the forward pass for the fully recurrent network for sequence classification (many-to-one mapping). To this end, write a function `forward` that takes a `model`, a sequence of input vectors `x`, and a label `y` as arguments. The inputs will be represented as a `numpy` array of shape `(T, D)`. It should execute the behavior of the fully recurrent network and evaluate the (numerically stabilized) binary cross-entropy loss at the end of the sequence and return the resulting loss value. Store the sequence of hidden activations $(a(t))_{t=1}^T$ and the logit $z(T)$ into `model.a` and `model.z`, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6932047439720525"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def forward(model, x, y):\n",
    "    ########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "    a = np.zeros((model.R.shape[0],1)) # getting a(0)\n",
    "\n",
    "    model.a = a.T # model.a: one row shows one time moment t\n",
    "        \n",
    "    for t in x:\n",
    "        \n",
    "        t = t.reshape((len(t),1))\n",
    "        s_t = model.W @ t + model.R @ a\n",
    "        \n",
    "        a = np.tanh(s_t)\n",
    "        model.a = np.concatenate((model.a,a.T),axis=0)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    model.z = (model.V @ model.a[-1]) # searched for z(T), so we only need the last activation in time\n",
    "\n",
    "    # y_pred = sigmoid(model.z) # taking the last prediction of the last time step \n",
    "                                # because of many to one mapping\n",
    "\n",
    "    loss = - y * model.z + np.log(1 + np.exp(-abs(model.z))) + max(0,model.z)\n",
    "    return loss[0]\n",
    "    \n",
    "Obj.forward = forward\n",
    "model.forward(np.random.uniform(-1, 1, (T, D)), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: The computational graph\n",
    "\n",
    "Visualize the computational graph of the fully recurrent network unfolded in time. The graph should show the functional dependencies of the nodes $x(t), a(t), z(t), L(z(t), y(t))$ for $t \\in \\{1, 2, 3\\}$. Use the package `networkx` in combination with `matplotlib` to draw a directed graph with labelled nodes and edges. If you need help take a look at [this guide](https://networkx.guide/visualization/basics/). Make sure to arrange the nodes in a meaningful way. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "G=nx.DiGraph() # important to give here a Directed Graph. Else the edges are stored without any direction\n",
    "# information such that they partwise point into the flase direction later.\n",
    "\n",
    "G.add_node('a(0)',pos=(0,2))\n",
    "G.add_edge('a(0)','a(1)',name='R')\n",
    "\n",
    "for t in range(1,4): # 4 is execluded\n",
    "\n",
    "    G.add_node(f'x({t})',pos=(t,1))\n",
    "\n",
    "    G.add_node(f'a({t})',pos=(t,2))\n",
    "    G.add_node(f'z({t})',pos=(t,3))\n",
    "    G.add_node(f'L(z({t}),y({t}))',pos=(t,4))\n",
    "    \n",
    "    G.add_edge(f'x({t})', f'a({t})',name='W')\n",
    "    G.add_edge(f'a({t})', f'z({t})',name='V')\n",
    "    G.add_edge(f'z({t})', f'L(z({t}),y({t}))',name='Put in')\n",
    "    \n",
    "    G.add_edge(f'a({t})', f'a({t+1})',name='R')\n",
    "    \n",
    "    \n",
    "G.add_node('a(4)',pos=(4,2))\n",
    "    \n",
    "\n",
    "pos = nx.get_node_attributes(G,'pos')\n",
    "\n",
    "name_edge = nx.get_edge_attributes(G,'name')\n",
    "\n",
    "nx.draw(G,pos,with_labels = True,arrows=True,node_size=800,node_color='yellow',node_shape='.')\n",
    "                      \n",
    "nx.draw_networkx_edge_labels(G, pos, name_edge) # adding edge labels\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook done by Nina Braunmiller - k11923286"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
