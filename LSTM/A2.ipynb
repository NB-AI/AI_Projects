{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Training the Fully Recurrent Network\n",
    "\n",
    "\n",
    "\n",
    "## Exercise 1: Data generation\n",
    "\n",
    "There are two classes, both occurring with probability 0.5. There is one input unit. Only the first sequence element conveys relevant information about the class. Sequence elements at positions $t > 1$ stem from a Gaussian with mean zero and variance 0.2. The first sequence element is 1.0 (-1.0) for class 1 (2). Target at sequence end is 1.0 (0.0) for class 1 (2)\n",
    "\n",
    "Write a function `generate_data` that takes an integer `T` as argument which represents the sequence length. Seed the `numpy` random generator with the number `0xDEADBEEF`. Implement the [Python3 generator](https://docs.python.org/3/glossary.html#term-generator) pattern and produce data in the way described above. The input sequences should have the shape `(T, 1)` and the target values should have the shape `(1,)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from scipy.special import expit as sigmoid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class FullyRecurrentNetwork(object):\n",
    "    def __init__(self, D, I, K):\n",
    "        self.W = np.random.uniform(-0.01, 0.01, (I, D))\n",
    "        self.R = np.random.uniform(-0.01, 0.01, (I, I))\n",
    "        self.V = np.random.uniform(-0.01, 0.01, (K, I))\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        # helper function for numerically stable loss\n",
    "        def f(z):\n",
    "            return np.log1p(np.exp(-np.absolute(z))) + np.maximum(0, z)\n",
    "        \n",
    "        # infer dims\n",
    "        T, D = x.shape\n",
    "        K, I = self.V.shape\n",
    "\n",
    "        # init result arrays\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.a = np.zeros((T, I))\n",
    "\n",
    "        # iterate forward in time \n",
    "        # trick: access model.a[-1] in first iteration\n",
    "        for t in range(T):\n",
    "            self.a[t] = np.tanh(self.W @ x[t] + self.R @ self.a[t-1])\n",
    "    \n",
    "        self.z = self.V @ self.a[t] #model.V @ self.a[t]\n",
    "        \n",
    "        return y * f(-self.z) + (1-y) * f(self.z)\n",
    "\n",
    "T, D, I, K = 10, 3, 5, 1\n",
    "model = FullyRecurrentNetwork(D, I, K)\n",
    "model.forward(np.random.uniform(-1, 1, (T, D)), 1)\n",
    "\n",
    "def generate_data(T):\n",
    "    ########## YOUR SOLUTION HERE ##########\n",
    "    \n",
    "    \n",
    "    # setting seed:\n",
    "    np.random.seed(0xDEADBEEF)\n",
    "    \n",
    "    while True: # while-loop gives you different yield-values each time\n",
    "        \n",
    "        # generate the first element-s of the sequences, each with probability 0.5:\n",
    "        # We have Bernoulli distribution (with replacement):\n",
    "        first_ele = np.random.binomial(1, p=0.5, size=1).astype('float')\n",
    "        first_ele[np.where(first_ele==0)] = -1.0\n",
    "\n",
    "        # setting target:\n",
    "        if first_ele[0]==1:\n",
    "            target = np.array([1], dtype='float')\n",
    "        else:\n",
    "            target = np.array([0], dtype='float')\n",
    "\n",
    "\n",
    "        # generate remaining values which come from a gaussian distribution:\n",
    "        mu = 0\n",
    "        sigma = np.sqrt(0.2) # std\n",
    "\n",
    "        rest_ele = np.random.normal(mu,sigma,T-1)\n",
    "\n",
    "        # Combine first element with random generated elements:\n",
    "        input_sample = np.append(first_ele, rest_ele).reshape(T,1)\n",
    "\n",
    "\n",
    "        # generator = A function which returns a generator iterator. \n",
    "        # It looks like a normal function except that it contains yield expressions for \n",
    "        # producing a series of values usable in a for-loop or that can be retrieved \n",
    "        # one at a time with the next() function.\n",
    "        yield input_sample, target \n",
    "\n",
    "data = generate_data(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1.        ],\n",
       "        [-0.13333295]]),\n",
       " array([1.]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_data_sample = next(data) # getting the data produced by the function, call one yield with each\n",
    "# next().\n",
    "# When implementing a for-loop in which is yield in the function and several \n",
    "# next(data), we get lots of samples.\n",
    "one_data_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Gradients for the network parameters\n",
    "Compute gradients of the total loss \n",
    "$$\n",
    "L = \\sum_{t=1}^T L(t), \\quad \\text{where} \\quad L(t) = L(z(t), y(t))\n",
    "$$\n",
    "w.r.t. the weights of the fully recurrent network. To this end, find the derivative of the loss w.r.t. the logits and hidden pre-activations first, i.e., \n",
    "$$\n",
    "\\psi^\\top(t) = \\frac{\\partial L}{\\partial z(t)} \\quad \\text{and} \\quad \\delta^\\top(t) = \\frac{\\partial L}{\\partial s(t)}.\n",
    "$$\n",
    "With the help of these intermediate results you should be able to compute the gradients w.r.t. the weights, i.e., $\\nabla_W L, \\nabla_R L, \\nabla_V L$. \n",
    "\n",
    "*Hint: Take a look at the computational graph from the previous assignment to see the functional dependencies.*\n",
    "\n",
    "*Remark: Although we only have one label at the end of the sequence, we consider the more general case of evaluating a loss at every time step in this exercise (many-to-many mapping).*\n",
    "\n",
    "########## YOUR SOLUTION HERE ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{1. get } \\psi^\\top(t) = \\frac{\\partial L}{\\partial z(t)} \\quad\\\\\n",
    "\\text{From assig. 1 ex. 2 and the fact that we can ignore the sum as only part with t=t_relevant important, we get:}\\\\\n",
    "\\psi^\\top(t) = \\frac{\\partial L(z,y)}{\\partial z(t)} = -y(t) - \\frac{1}{e^{|z(t)|} + 1 }  + \\frac{max(0,z(t))}{z(t)}\\\\\n",
    "\\text{Solution:} e(t)=  \\sigma (z(t)-y) \\text{ because \\hat{y}= \\sigma(z(t))}\\\\\n",
    "\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{2. get }  \\delta^\\top(t) = \\frac{\\partial L}{\\partial s(t)}\\\\\n",
    "\\frac{\\partial L}{\\partial s(t)}\n",
    "= \\frac{\\partial L}{\\partial a(t)} \\cdot \\frac{\\partial a(t)}{\\partial s(t)}\n",
    "= \\left(\\frac{\\partial L(z(t),y(t))}{\\partial a(t)} \n",
    "+\n",
    "\\frac{\\partial L}{\\partial s(t+1)} \\frac{\\partial s(t+1)}{\\partial a(t)}\\right)\n",
    "\\cdot \n",
    "\\frac{\\partial a(t)}{\\partial s(t)}\n",
    "\\\\\n",
    "= \\left(\\frac{\\partial L(z(t),y(t))}{\\partial z(t)} \\frac{\\partial z(t)}{\\partial a(t)}\n",
    "+\n",
    "\\delta^\\top(t+1) R^\\top \\right)\n",
    "\\cdot\n",
    "diag(\\tanh'(s(t)))\n",
    "\\\\\n",
    "= \\left(\\psi^\\top(t) \n",
    "\\frac{\\partial V a(t)}{\\partial a(t)}\n",
    "+\n",
    "\\delta^\\top(t+1) R^\\top \\right)\n",
    "\\cdot\n",
    "diag(\\tanh'(s(t)))\n",
    "\\\\\n",
    "= \\left(\\psi^\\top(t) \n",
    "V^\\top\n",
    "+\n",
    "\\delta^\\top(t+1) R^\\top \\right)\n",
    "\\cdot\n",
    "diag(1 - \\tanh^2(s(t)))\n",
    "\\\\\n",
    "$$\n",
    "Solution: we use diag() because only on diagonal the terms are independent of each other off-values=0 in a matrix over time(?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{3. get }\\nabla_V L\\\\\n",
    " \\frac{\\partial L(z,y)}{\\partial z(t)} \\cdot \\frac{\\partial z(t)}{\\partial V} \n",
    "= \\frac{\\partial L(z,y)}{\\partial z(t)} \\cdot \\frac{\\partial V a(t)}{\\partial V}\n",
    "= \\frac{\\partial L(z,y)}{\\partial z(t)} \\cdot a(t)\n",
    "=  \\psi^\\top(t) \\cdot  \\tanh(s(t))\n",
    "\\\\\n",
    "\\text{Sum that up over time:}\\\\\n",
    "\\nabla_V L = \\sum_{t=1}^T  \\psi^\\top(t) \\cdot  a(t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{4. get }\\nabla_R L\\\\\n",
    "\\frac{\\partial L}{\\partial s(t)} \\cdot \\frac{\\partial s(t)}{\\partial R}\n",
    "= \\frac{\\partial L}{\\partial s(t)} \\cdot \\frac{\\partial W x(t) + R a(t-1)}{\\partial R}\n",
    "= \\frac{\\partial L}{\\partial s(t)} \\cdot  a(t-1)\n",
    "=  \\delta^\\top(t) \\cdot  a(t-1)\n",
    "\\\\\n",
    "\\text{Sum that up over time:}\\\\\n",
    "\\nabla_R L = \\sum_{t=1}^T   \\delta^\\top(t) \\cdot  a(t-1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{5. get }\\nabla_W L\\\\\n",
    "\\frac{\\partial L}{\\partial s(t)} \\cdot \\frac{\\partial s(t)}{\\partial W}\n",
    "= \\frac{\\partial L}{\\partial s(t)} \\cdot \\frac{\\partial W x(t) + R a(t-1)}{\\partial W}\n",
    "= \\frac{\\partial L}{\\partial s(t)} \\cdot x(t)\n",
    "=  \\delta^\\top(t) \\cdot  x(t)\n",
    "\\\\\n",
    "\\text{Sum that up over time:}\\\\\n",
    "\\nabla_W L = \\sum_{t=1}^T   \\delta^\\top(t) \\cdot  x(t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: The backward pass\n",
    "Write a function `backward` that takes a model `self` as argument. The function should compute the gradients of the loss with respect to all model parameters and store them to `self.dW`, `self.dR`, `self.dV`, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(self):\n",
    "    ########## YOUR SOLUTION HERE ##########\n",
    "    \n",
    "    # dz:\n",
    "    z_full = self.V @ self.a.T \n",
    "    dz = -self.y - 1/(np.exp(np.abs(z_full))+1) + np.clip(z_full,0,None)/z_full\n",
    "    # y and z are scalars --> dz is scalar\n",
    "\n",
    "    # ds:\n",
    "    # add 0 row to a because it starts earlier than the signal x:\n",
    "    a2 = np.concatenate((np.zeros(self.a.shape[1]).reshape(1,-1),self.a),axis=0)\n",
    "    s = self.W @ self.x.T + self.R @ a2[:-1].T\n",
    "    # and not only one\n",
    "    \n",
    "    ds = np.zeros((self.x.shape[0]+1,self.V.shape[1]))\n",
    "   \n",
    "    for ind,time_rev in enumerate(ds[1:]):\n",
    "\n",
    "        ds[1+ind] = (dz[:,-ind-1] @ self.V + ds[ind-1].T@self.R.T) @ np.diag(1-np.tanh(s.T[-ind-1])**2)\n",
    "\n",
    "    # ds: the last row is the first time step t=1\n",
    "    ds = np.flip(ds,axis=0)[:-1] # now the last row is the last time step t=T\n",
    "    \n",
    "    self.dz = dz\n",
    "    self.ds = ds\n",
    "\n",
    "    # dV: When we look at def forward() we have a many-to-one mapping, therfore\n",
    "    # now sum and only the last output needed:\n",
    "    dV = dz[-1].reshape(-1,1) @ a2[-1].reshape(-1,1).T\n",
    "    self.dV = np.sum(dV,axis=0).reshape(1,-1)\n",
    "    \n",
    "    # dR:\n",
    "    deri_R = np.ones_like(self.R)\n",
    "    dR = ds.T @ a2[:-1]\n",
    "    self.dR = dR\n",
    "\n",
    "    # dW:\n",
    "    deri_W = np.ones_like(self.W)\n",
    "    dW = ds.T @ self.x\n",
    "    self.dW = dW\n",
    "    \n",
    "\n",
    "    return\n",
    "\n",
    "FullyRecurrentNetwork.backward = backward\n",
    "model.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution:\n",
    "def backward(self):\n",
    "    ########## YOUR SOLUTION HERE ##########\n",
    "    T, D = self.x.shape\n",
    "    K, I = self.V.shape\n",
    "    \n",
    "    self.dW, self.dR, self.dV = np.zeros_like(self.W),np.zeros_like(self.R),np.zeros_like(self.V)\n",
    "    delta = np.zeros((T,I))\n",
    "    \n",
    "    psi_T = sigmoid(self.z) - self.y\n",
    "    self.dV = psi_T[:,None] @ self.a[T-1][None,:]\n",
    "    \n",
    "    for t in reversed(range(T)):\n",
    "        if t == T-1:\n",
    "            delta[t] = psi_T @ self.V @ np.diag(1-self.a[t]**2)\n",
    "        else:\n",
    "            delta[t] = delta[t+1] @ self.R @ np.diag(1-self.a[t]**2)\n",
    "            \n",
    "        self.dW += delta[t][:,None] @ self.x[t][None,:]\n",
    "        \n",
    "        if t != 0:\n",
    "            self.dR += delta[t][:,None] @ self.a[t-1][None,:]\n",
    "\n",
    "FullyRecurrentNetwork.backward = backward\n",
    "model.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Gradient checking\n",
    "Write a function `grad_check` that takes a model `self`, a float `eps` and another float `thresh` as arguments and computes the numerical gradients of the model parameters according to the approximation\n",
    "$$\n",
    "f'(x) \\approx \\frac{f(x + \\varepsilon) - f(x - \\varepsilon)}{2 \\varepsilon}.\n",
    "$$\n",
    "If any of the analytical gradients are farther than `thresh` away from the numerical gradients the function should throw an error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_check(self, eps, thresh):\n",
    "    ########## YOUR SOLUTION HERE ##########\n",
    "    \n",
    "\n",
    "    # numeric_part:\n",
    "    numeric_deri = lambda func: (func(eps)-func(-eps))/(2*eps)\n",
    "    numeric_deri_time = lambda func,time: (func(eps,time)-func(-eps,time))/(2*eps)\n",
    "    \n",
    "    loss = lambda ep: - self.y * (self.z+ep) + np.log1p(np.exp(-np.abs((self.z+ep)))) + max(0,(self.z+ep))\n",
    "    numeric_dz = numeric_deri(loss)\n",
    "    \n",
    "    z_full = self.V @ self.a.T\n",
    "    loss_time = lambda ep,time: - self.y * (z_full[:,time]+ep) + np.log1p(np.exp(-np.abs((z_full[:,time]+ep)))) + max(0,(z_full[:,time]+ep))\n",
    "    \n",
    "    a2 = np.concatenate((np.zeros(self.a.shape[1]).reshape(1,-1),self.a),axis=0)\n",
    "    #s = self.W @ self.x.T + self.R @ a2[:-1].T\n",
    "    #tanh_s = lambda ep: np.tanh((s+ep))\n",
    "    print(self.V.shape,a2.shape,'ffff')\n",
    "    \n",
    "    x2 = np.concatenate((self.x, np.zeros(self.x.shape[1]).reshape(1,-1)),axis=0)\n",
    "    z = lambda ep,time: self.V @ (self.a[time]+ep)\n",
    "    s = lambda ep,time: (self.W @ x2[time+1].T).T + (self.R @ (self.a[time].reshape(-1,1) +ep)).T\n",
    "    a = lambda ep,time: np.tanh(s(ep,time))\n",
    "    numeric_ds = np.zeros((self.x.shape[0]+1,self.V.shape[1]))\n",
    "    \n",
    "    for ind,time_rev in enumerate(numeric_ds[1:]):\n",
    "        numeric_ds[ind+1] = (numeric_deri_time(loss_time,-ind-1) * numeric_deri_time(z,-ind-1) + numeric_ds[ind-1] * numeric_deri_time(s,-ind+1-1)) * numeric_deri_time(a,-ind-1)\n",
    "\n",
    "        #numeric_ds[ind+1] = (numeric_dz * numeric_deri_time(z,-ind-1) + numeric_ds[ind-1] * numeric_deri_time(s,-ind+1-1)) * numeric_deri_time(a,-ind-1)\n",
    "\n",
    "        #numeric_ds[ind+1] = (numeric_dz * numeric_deri(z(eps,-ind-1)) + numeric_ds[ind-1] * numeric_deri(s(eps,-ind+1-1))) * numeric_deri(a(eps,-ind-1))\n",
    "        #ds[1+ind] = (dz * self.V + ds[ind-1].T@self.R.T) @ np.diag(1-np.tanh(s.T[-ind])**2)\n",
    "    # ds: the last row is the first time step t=1\n",
    "    numeric_ds = np.flip(numeric_ds,axis=0)[:-1] # now the last row is the last time step t=T\n",
    "    \n",
    "    #numeric_ds = numeric_dz * self.V @ numeric_deri(tanh_s)\n",
    "    print('my',self.V.shape,self.dV.shape)\n",
    "    \n",
    "    \n",
    "    fffff\n",
    "    #z_all_time = lambda ep: np.sum(self.V @ (self.a.T+ep),axis=)\n",
    "    \n",
    "    \n",
    "    V_rep = np.repeat(self.V,5,axis=0)\n",
    "    V_a = lambda ep: (self.V+ep) @ a2[:-1].T\n",
    "    numeric_dV = numeric_dz * numeric_deri(V_a) \n",
    "    # dV = dL/dz * dz/dV --> Put in separately L and z into numeric differentiation\n",
    "    # formula such that dV = num_formula(L) * num_formula(z)\n",
    "  \n",
    "    \n",
    "    s2 = lambda ep:  self.W @ self.x.T + (self.R+ep) @ a2[:-1].T\n",
    "    numeric_dR = numeric_ds * numeric_deri(s2)\n",
    "   \n",
    "    s3 = lambda ep:  (self.W+ep) @ self.x.T + self.R @ a2[:-1].T\n",
    "    numeric_dW = numeric_ds * numeric_deri(s3)\n",
    "    print('!!!!!!!1',numeric_dW.shape,numeric_ds.shape,numeric_deri(s3).shape)\n",
    "    # analytic part:\n",
    "    analytic_dV = self.dV#.T\n",
    "    analytic_dR = self.dR\n",
    "    analytic_dW = self.dW\n",
    "    \n",
    "    # comparison:\n",
    "    o=np.allclose(numeric_dV,self.dV,atol=thresh) # abs\n",
    "    print(o)\n",
    "    print('dv',numeric_dV.shape,self.dV.shape)\n",
    "    print(self.a.shape)\n",
    "    \n",
    "    print('dr',numeric_dR.shape, analytic_dR.shape)\n",
    "    print('dw',numeric_dW.shape, analytic_dW.shape)\n",
    "   # print(numeric_dR)\n",
    "    #print(analytic_dR)\n",
    "    print('x shape',self.x.shape)\n",
    "    print(numeric_ds.shape)\n",
    "    print('##################')\n",
    "    print(self.ds.shape)\n",
    "    print('selfs',self.ss.shape)\n",
    "    print(numeric_dV)\n",
    "    print('########')\n",
    "    print(self.dV)\n",
    "    \n",
    "    check_dV = np.allclose(numeric_grad, analytic_dV, atol=thresh) \n",
    "    # atol: compare the absolute difference between two values.\n",
    "    check_dR = np.allclose(numeric_grad, analytic_dR, atol=thresh) \n",
    "    check_dW = np.allclose(numeric_grad, analytic_dW, atol=thresh) \n",
    "    \n",
    "    if not check_dV:\n",
    "        raise Exception('Numeric and analytic dV differ too strongly!')\n",
    "\n",
    "    if not check_dR:\n",
    "        raise Exception('Numeric and analytic dR differ too strongly!')\n",
    "    \n",
    "    if not check_dW:\n",
    "        raise Exception('Numeric and analytic dW differ too strongly!')\n",
    "    \n",
    "\n",
    "FullyRecurrentNetwork.grad_check = grad_check\n",
    "model.grad_check(1e-7, 1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution:\n",
    "def my_numeric_grad(self,weight_matrix, eps):\n",
    "\n",
    "    numeric_dWRV = np.zeros_like(weight_matrix) # the numeric (and also \n",
    "    # the analytic one) has to have the same shape as the belonging weight_matrix\n",
    "        \n",
    "    for row_ind,row in enumerate(weight_matrix):\n",
    "        for col_ind,col in enumerate(row):\n",
    "            \n",
    "            # first part numeric formula:\n",
    "            weight_matrix[row_ind,col_ind] += eps\n",
    "            loss_f_numeric_one = self.forward(self.x,self.y)\n",
    "\n",
    "            # second part numeric formula:\n",
    "            weight_matrix[row_ind,col_ind] -= 2*eps\n",
    "            loss_f_numeric_two = self.forward(self.x,self.y)\n",
    "            \n",
    "            # bring weight_matrix back into its original being:\n",
    "            weight_matrix[row_ind,col_ind] += eps\n",
    "\n",
    "            numeric_dWRV[row_ind,col_ind] = (loss_f_numeric_one-loss_f_numeric_two) / (2*eps)\n",
    "\n",
    "    return numeric_dWRV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution:\n",
    "def grad_check(self, eps, thresh):\n",
    "    ########## YOUR SOLUTION HERE ##########\n",
    "    \n",
    "\n",
    "    # numeric_part:\n",
    "    numeric_dV = self.my_numeric_grad(self.V, eps)\n",
    "    numeric_dW = self.my_numeric_grad(self.W, eps)\n",
    "    numeric_dR = self.my_numeric_grad(self.R, eps)\n",
    "    \n",
    "    \n",
    "    # analytic part:\n",
    "    analytic_dV = self.dV\n",
    "    analytic_dR = self.dR\n",
    "    analytic_dW = self.dW\n",
    "    \n",
    "    # comparison:   \n",
    "    check_dV = np.allclose(numeric_dV, analytic_dV, atol=thresh) \n",
    "    # atol: compare the absolute difference between two values.\n",
    "    check_dR = np.allclose(numeric_dR, analytic_dR, atol=thresh) \n",
    "    check_dW =  np.allclose(numeric_dW, analytic_dW, atol=thresh) \n",
    "    \n",
    "    if not check_dV:\n",
    "        raise Exception('Numeric and analytic dV differ too strongly!')\n",
    "\n",
    "    if not check_dR:\n",
    "        raise Exception('Numeric and analytic dR differ too strongly!')\n",
    "    \n",
    "    if not check_dW:\n",
    "        raise Exception('Numeric and analytic dW differ too strongly!')\n",
    "    \n",
    "FullyRecurrentNetwork.my_numeric_grad = my_numeric_grad\n",
    "FullyRecurrentNetwork.grad_check = grad_check\n",
    "model.grad_check(1e-7, 1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Parameter update\n",
    "\n",
    "Write a function `update` that takes a model `self` and a float argument `eta`, which represents the learning rate. The method should implement the gradient descent update rule $\\theta \\gets \\theta - \\eta \\nabla_{\\theta}L$ for all model parameters $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(self, eta):\n",
    "    ########## YOUR SOLUTION HERE ##########\n",
    "\n",
    "    self.V = self.V - eta * self.dV # self.V -= eta * self.dV \n",
    "    self.R = self.R - eta * self.dR\n",
    "    self.W = self.W - eta * self.dW\n",
    "\n",
    "    return\n",
    "\n",
    "FullyRecurrentNetwork.update = update\n",
    "model.update(0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6: Network training\n",
    "\n",
    "Train the fully recurrent network with 32 hidden units. Start with input sequences of length one and tune the learning rate and the number of update steps. Then increase the sequence length by one and tune the hyperparameters again. What is the maximal sequence length for which the fully recurrent network can achieve a performance that is better than random? Visualize your results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "FullyRecurrentNetwork.backward = backward\n",
    "FullyRecurrentNetwork.update = update\n",
    "\n",
    "T_big,epoch_size = 5,10\n",
    "best_lr,best_updating,best_loss = np.zeros((epoch_size,T_big)),np.zeros(T_big,),np.ones((epoch_size,T_big))*float('inf')\n",
    "I, K =  32, 1 # x with NxD is 1D # I as number of hidden units\n",
    "\n",
    "for ind_T,T in enumerate(range(1,T_big+1)):\n",
    "\n",
    "    loss_old = float('inf')\n",
    "    one_data_sample = next(generate_data(T)) # trough seed all the old samples\n",
    "    # from loop before stay the same and one is added\n",
    "    x, y = one_data_sample\n",
    "\n",
    "\n",
    "    for update_number in [1,2,3]: # update every/every second/every third time\n",
    "\n",
    "        for lr in [0.0005,0.01,0.05,0.1]:\n",
    "            \n",
    "            D = 1\n",
    "            model2 = FullyRecurrentNetwork(D, I, K)\n",
    "\n",
    "            for epoch in range(0,epoch_size):\n",
    "\n",
    "                #for t in range(T):\n",
    "                loss_f = model2.forward(x, y[0])\n",
    "                \n",
    "                if epoch % update_number == 0:\n",
    "                    model2.backward() # compute gradients to update\n",
    "                    model2.update(lr) # update gradients\n",
    "\n",
    "\n",
    "                if best_loss[epoch,ind_T] > loss_f[0]:\n",
    "                    best_lr[epoch,ind_T] = lr\n",
    "                    best_loss[epoch,ind_T] = loss_f[0]\n",
    "                    \n",
    "        if best_loss[epoch,ind_T] == loss_f[0]:\n",
    "            best_updating[ind_T] = update_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_range = [epoch for epoch in range(0,epoch_size)]\n",
    "legend_list = []\n",
    "\n",
    "for ind,col in enumerate(best_loss.T):\n",
    "    plt.plot(epoch_range,col)\n",
    "    legend_list.append(f'T={ind+1}')\n",
    "    \n",
    "plt.legend(legend_list)\n",
    "plt.title('best loss scores per epoch for different T')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.show;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind,col in enumerate(best_lr.T):\n",
    "    plt.plot(epoch_range,col)\n",
    "\n",
    "plt.legend(legend_list)\n",
    "plt.title('best learning rate values per epoch for different T')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('learning rate')\n",
    "plt.show;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legend_list = []\n",
    "best_updating = best_updating.astype('int')\n",
    "plt.scatter([t for t in range(1,T+1)],best_updating)\n",
    "\n",
    "plt.legend(legend_list)\n",
    "plt.title('best values for how often to update weights for different T')\n",
    "plt.xlabel('T')\n",
    "plt.ylabel('after every _. epoch update')\n",
    "plt.show;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7: The Vanishing Gradient Problem\n",
    "\n",
    "Analyze why the network is incapable of learning long-term dependencies. Show that $\\|\\frac{\\partial a(T)}{\\partial a(1)}\\|_2 \\leq \\|R\\|_2^{T-1}$ , where $\\|\\cdot\\|_2$ is the spectral norm, and discuss how that affects the propagation of error signals through the time dimension of the network. \n",
    "\n",
    "*Hint: Use the fact that the spectral norm is submultiplicative for square matrices, i.e. $\\|AB\\|_2 \\leq \\|A\\|_2\\|B\\|_2$ if $A$ and $B$ are both square.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "########## YOUR SOLUTION HERE ##########\n",
    "$$\n",
    "\\frac{\\partial a(t)}{\\partial a(t-1)} = \\frac{\\partial \\tanh(s(t))}{\\partial a(t-1) }\n",
    "=\\frac{\\partial \\tanh(s(t)) }{\\partial s(t)} \\frac{\\partial s(t)}{\\partial a(t-1)}\n",
    "= (1 - \\tanh^2(s(t))) R\n",
    "= \\frac{\\partial s(t+1) }{\\partial s(t+1-1)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{It follows:}\\\\\n",
    "\\frac{\\partial a(T)}{\\partial a(T-(T-1))} \n",
    "= \\frac{\\partial a(T) }{\\partial a(1)}\n",
    "= \\frac{\\partial a(T)}{\\partial a(T-1)} \\frac{\\partial a(T-1)}{\\partial a(T-2)} ...\\\\\n",
    "= \\prod_{t'=0}^{(T-1)-1} \\frac{\\partial a(T-t')}{\\partial a(T-1-t')}\n",
    "= R^{(T-1)} \\prod_{t'=0}^{(T-1)-1} (1 - \\tanh^2(s(T-t')))\\\\\n",
    "= R^{(T-1)} \\tau\n",
    "$$\n",
    "Side note: |Derivative_of_tanh|<1, i.e. the derivative becomes very small with many multiplications like in the product above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution start <br>\n",
    "We can't use the product for matrices but for scalars. The solution would be to apply the norm rules to the single derivatives in the derivate multiplications above. <br>\n",
    "Solution end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spectral norm: Knowledge derived from https://de.wikipedia.org/wiki/Spektralnorm . <br>\n",
    "The spectral norm describes the maximal eigenvalue squared of its contained matrix. I.e.:\n",
    "$$\n",
    "\\text{We have: } ||A||_2\\\\\n",
    "det(\\lambda I - A^\\top A) = 0 \\text{ ->} \\lambda_{1/2/...}\\\\\n",
    "||A||_2 = \\sqrt{\\max\\lambda_{1/2/...}\\\\}\n",
    "$$\n",
    "\n",
    "Besides that, from https://de.wikipedia.org/wiki/Spektralnorm  we get the following rule:\n",
    "$$\n",
    "|| A x||_2 \\leq ||A||_2 ||x||| \\text{ with A.shape=MxN, x.shape=N,}\\\\\n",
    "\\text{This accounts for our case:}\\\\\n",
    "\\left|\\left|\\frac{\\partial a(T) }{\\partial a(1)}\\right|\\right|_2\n",
    "= || R^{(T-1)} \\tau ||_2 \\text{ with R.shape=IxI,} \\tau \\text{.shape=I,}\\\\\n",
    "\\leq ||R^{(T-1)}||_2 ||\\tau||_2\n",
    "$$\n",
    "\n",
    "Furthermore, for square matrices we get:\n",
    "$$\n",
    "||A B||_2 \\leq ||A||_2 ||B||_2\\\\\n",
    "\\text{We get:} ||R^{(T-1)}||_2 \\leq ||R||_2 ||R||_2 ... = ||R||_2^{(T-1)}\n",
    "$$\n",
    "\n",
    "It follows:\n",
    "$$\n",
    "\\left|\\left|\\frac{\\partial a(T) }{\\partial a(1)}\\right|\\right|_2 \\leq ||R||_2^{(T-1)} ||\\tau||_2 \n",
    "\\leq ||R||_2^{(T-1)}  \\\\\n",
    "\\text{ because } \\tau \\text{ only contains very small values and therefore small eigenvalues}\\\\\n",
    "\\square\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vanishing gradients: When T is high and $||K||_2$ has small eigenvalues through small K-values then $||R||_2^{(T-1)}$ has even smaller values through small-value-multiplications. From the shown equation it follows that $\\left|\\left|\\frac{\\partial a(T) }{\\partial a(1)}\\right|\\right|_2$ has even smaller values and therefore also the values from $\\frac{\\partial a(T) }{\\partial a(1)}$ have to be very small which shows the vanishing gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook done by Nina Braunmiller k11923286"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
